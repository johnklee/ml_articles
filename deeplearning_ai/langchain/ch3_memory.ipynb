{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2460d53-59a1-4b56-a407-7bd2377b7de7",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://learn.deeplearning.ai/courses/langchain/lesson/3/memory)) <b><font size='3ptx'>From this chapter, we are going to see how we could use memory to let chatbot remember the chatting history.</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddcf67-7688-4806-824a-34f464142834",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "### <b><font color='darkgreen'>Outline</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1'>ConversationBufferMemory</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2'>ConversationBufferWindowMemory</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect3'>ConversationTokenBufferMemory</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect4'>ConversationSummaryMemory</a></font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0764ddb3-d5e5-4d82-8119-cd1ca6080e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "a = load_dotenv(find_dotenv(os.path.expanduser('~/.env'))) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38574797-4ba1-4194-9ecb-2a10dbcde58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878186b3-966a-49ed-bda6-e3b37602fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3be4e4-edc6-49bb-ac8b-0e080dfcde7b",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <b><font color='darkblue'>ConversationBufferMemory</font></b>\n",
    "<b><font size='3ptx'>This memory [ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html#langchain-memory-buffer-conversationbuffermemory) allows for storing messages and then extracts the messages in a variable.</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e49d7a0-2be8-4300-bdae-0fc1a4f01f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0e8036-0541-4d60-b9ea-d2d73743901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ac2488-36d9-4660-a149-d7bc26e92b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d74147-6aa9-448c-af35-fb010f507d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8030511e-1137-4495-9a77-c1498f021bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew. Is there anything else you would like to know or discuss?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ea9f6-4b28-4920-9de2-22f4ee8b3f13",
   "metadata": {},
   "source": [
    "From the `Current converssation` section, the chatting history is kept and therefore LLM knows your name from previous discussion.\n",
    "\n",
    "Let's take a look at how memory works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb9d55c-3cdd-4bc8-b5b5-00b7e803f4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew. Is there anything else you would like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "530b004a-d9e9-435a-8d4e-a171e86eab8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Andrew. Is there anything else you would like to know or discuss?\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_memory_variables: Return history buffer.\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83330ce-cd58-499a-ace8-3874dfe50e7c",
   "metadata": {},
   "source": [
    "Or you could feed in the chatting history this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8a3ba87-bcd7-4037-b734-5b81890743df",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44f7c5cd-9d25-4649-89b1-93ac9913df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save context from this conversation to buffer.\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08d07c03-001b-4360-978b-40b415f5cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beff4afb-299b-4f0f-9654-fd7f90ff412e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec8ffacd-41fb-4ba8-a56a-cc243f41f1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one more chatting record:\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b34cee6-fc1a-4a78-88bc-d18d63dcff17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d36dea-389c-4bd6-a40c-1e24304118cc",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <b><font color='darkblue'>ConversationBufferWindowMemory</font></b>\n",
    "<b><font size='3ptx'>[ConversationBufferWindowMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html) keeps a list of the interactions of the conversation over time. It only uses the last `K` interactions.</font></b>\n",
    "\n",
    "<b>This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large.</b> Let's first explore the basic functionality of this type of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80e781cb-cae1-451d-8244-a7a88d8a6eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13162b-d179-47ea-81f6-f1902e32646b",
   "metadata": {},
   "source": [
    "For `k=1`, it only remember the last chatting record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aee47ca6-52d9-401f-a90a-ed87498f944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chatting record\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "# Second chatting record\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd9f493f-e237-4207-9847-073672fac750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: Cool\n"
     ]
    }
   ],
   "source": [
    "# The first chatting record will be dropped\n",
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f951b49-c471-4a88-a10d-7cb53cd7abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a18f303c-a183-49a7-a4ec-fc6342218930",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = conversation.predict(input=\"Hi, my name is Andrew\")  # Chatting record 1\n",
    "_ = conversation.predict(input=\"What is 1+1?\")           # chatting record 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec84c921-f68b-4fd0-aad2-88b09f0e3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4733cfc-3d9f-4360-8d07-0c02b1ad8697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because the chatting record 1 is flushed, LLM won't know the answer.\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bf8fe-ee2a-4655-b067-a00fcedbe59d",
   "metadata": {},
   "source": [
    "<a id='sect3'></a> \n",
    "## <b><font color='darkblue'>ConversationTokenBufferMemory</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>[ConversationTokenBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html) keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to determine when to flush interactions.</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bdba326-1298-4f67-971b-d195d1da02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebc899-fe14-418f-ac7e-3d1aa10106c7",
   "metadata": {},
   "source": [
    "Because different LLM will have different way to measure the length of tokens, so we have to feed in the LLM object in this kind of memory. Let's prepare some chatting history for learning how it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b509468-3877-4daf-93ee-656a5208cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92cec82e-d70c-4d2c-880c-20ef9eef043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "Human: Chatbots are what?\n",
      "AI: Charming!\n"
     ]
    }
   ],
   "source": [
    "chat_history = memory.load_memory_variables({})['history']\n",
    "print(len(chat_history))\n",
    "print(f'{chat_history}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcce3b3c-6b6f-47ea-a1f8-4ea4255365f0",
   "metadata": {},
   "source": [
    "Let's enlarge the `max_token_limit=100`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23af6617-e029-4ad8-b5bc-a9a5c5820265",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e1d71fa-15fc-4670-865b-248cf3a2e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c14d8c40-3b3c-4f6a-94ca-5edf1018ac3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n",
      "Human: AI is what?!\n",
      "AI: Amazing!\n",
      "Human: Backpropagation is what?\n",
      "AI: Beautiful!\n",
      "Human: Chatbots are what?\n",
      "AI: Charming!\n"
     ]
    }
   ],
   "source": [
    "chat_history = memory.load_memory_variables({})['history']\n",
    "print(len(chat_history))\n",
    "print(f'{chat_history}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337d0ba-b3e7-42f5-b37c-399d934d4b29",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <b><font color='darkblue'>ConversationSummaryMemory</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>[ConversationSummaryMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.summary.ConversationSummaryMemory.html) creates a summary of the conversation over time.</font></b>\n",
    "\n",
    "<b>This can be useful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory</b>. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "74ee20a2-09c3-479b-a123-affc5c254d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258d01b-d348-42d0-bca9-1dcd86b8edac",
   "metadata": {},
   "source": [
    "Let's create some chatting history for testing this kind of memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa921803-df98-4187-af05-23cca7ad7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "554b613b-b919-446a-914e-45da893a9a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human and AI exchange greetings and discuss the day's schedule. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3c8bc49-4990-48e5-9ae0-fc7cbbc4d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c44ef785-12c9-4d83-bf39-96448457f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the day's schedule. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the morning meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. This could include a live demonstration of the language processing capabilities, data encryption features, and any new integrations that have been implemented. Additionally, for the lunch meeting with the customer interested in AI, a demo highlighting the AI capabilities of our products and services would be impressive. This could involve showcasing machine learning algorithms, natural language processing tools, and any personalized recommendations based on user data. Let me know if you need help preparing any specific demos!'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7f54283-82bc-4f43-b9ac-c5a9b42327c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human and AI exchange greetings and discuss the day's schedule. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's activities, suggesting demos for the meetings that showcase the latest features and updates on the LangChain project, as well as the AI capabilities of their products and services. The AI offers assistance in preparing specific demos if needed.\n"
     ]
    }
   ],
   "source": [
    "print(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a751cbd-30a7-4212-8d93-1a7981afd761",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Deeplearning.ai - Langchain Ch1: Models, prompts and parsers](https://learn.deeplearning.ai/courses/langchain/lesson/2/models%2C-prompts-and-parsers)\n",
    "* [Deeplearning.ai - Langchain Ch4: Chain](https://learn.deeplearning.ai/courses/langchain/lesson/4/chains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
