{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50e9db9-c135-4eed-9ddf-b6d58891b002",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://learn.deeplearning.ai/courses/langchain/lesson/4/chains)) <b><font size='3ptx'>Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL. ([more](https://python.langchain.com/v0.1/docs/modules/chains/))</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0777dc61-0029-4d5b-b677-18e5d0fe99d0",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "### <b><font color='darkgreen'>Outline</font></b>\n",
    "* <font size='3ptx'><a href='#sect1'>**LLMChain**</a></font> (deprecated in LangChain 0.1.17)\n",
    "* <font size='3ptx'><a href='#sect2'>**Sequential Chains**</a></font>\n",
    "    - SimpleSequentialChain\n",
    "    - SequentialChain\n",
    "* <font size='3ptx'><a href='#sect3'>**Router Chain**</a></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cd974c-7255-4fe2-8e9d-65038f296473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TEST_DATA = pd.DataFrame({'index': ['row1', 'row2'], 'review': ['review1', 'review2']})\n",
    "\n",
    "a = load_dotenv(find_dotenv(os.path.expanduser('~/.env'))) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bed72fa-3b6b-46b9-ad3b-0a808ce66bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cc035-75d5-41c5-b639-c18b415e8dd5",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <b><font color='darkblue'>LLMChain</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f933ab9-ba89-4695-a598-43c976c71a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3679d1b-b9e3-4463-9d38-af94ef374b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# `LLMChain` was deprecated in LangChain 0.1.17\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebee6564-8a53-4f87-ba1b-8ba855fe7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "resp = chain.invoke(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dafb680-6cdb-4417-9185-e189c9420935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Royal Comfort Linens\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04a8cd-ea09-4c55-86f2-1716af9c0fd9",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <b><font color='darkblue'>SimpleSequentialChain</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>[SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html) is a structure for combining multiple LLMChains, allowing for sequential processing of different language tasks.</font></b>\n",
    "\n",
    "<b>It is simpler than [SequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SequentialChain.html) because it directly passes the output of one LLMChain as input to the next, without additional intermediate variables or complex data flow management</b>. This design makes [**SimpleSequentialChain**](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html) ideal for simple sequential tasks, such as first generating a company name and then creating a description for that name, as in this example. It simplifies the implementation of sequential processing steps and is suitable for scenarios that require sequential processing but do not need complex data management.\n",
    "\n",
    "In a simple scenario with only one input and one output, two LLMChains can be concatenated using [**SimpleSequentialChain**](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html):\n",
    "\n",
    "![simple chain](images/ch4_img1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "106b4193-0712-4c16-ae96-f9dd6aabc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5eff9b4-888b-4977-9f75-c7416cebd5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Assume you are good at math. Please answer below question\"\n",
    "    \"Question: {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71cf79e0-6a3a-4873-aa16-b5a7c324c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Base on result as {result}, is it even?\")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9c266fa-132c-4ca0-a981-8bdad762825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9510221a-6338-4a61-927b-fcd3edc282ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mThe answer is 3.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mNo, the number 3 is not an even number. It is an odd number.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 1 + 2?',\n",
       " 'output': 'No, the number 3 is not an even number. It is an odd number.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.invoke({'input': 'What is 1 + 2?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea7941-f358-4e26-bf5e-712233e34767",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>SequentialChain</font></b>\n",
    "<b><font size='3ptx'>[SequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SequentialChain.html) is a structure used to connect multiple LLMChains (<font color='brown'>language model chains</font>) in sequence.</font></b>\n",
    "\n",
    "In this case, each LLMChain represents a specific processing step (<font color='brown'>such as translation, summarization, etc.</font>), and [**SequentialChain**](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SequentialChain.html) ensures that these steps are executed in order. <b>This structure allows complex data processing flows to be broken down into smaller, more manageable units and executed sequentially, thereby effectively handling complex language tasks</b>.\n",
    "![sequential chain](images/ch4_img2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25e0070f-a47c-416a-9afd-834d1bb8921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Count the total money earned by below information: \"\n",
    "    \"{Background}\"\n",
    ")\n",
    "# chain 1: input= Background and output= Salary_sum\n",
    "chain_one = LLMChain(\n",
    "    llm=llm, prompt=first_prompt, output_key=\"Salary_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eacee5b-0f37-4194-a38a-c9e52cd8a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    'Double the value of \"{Salary_sum}\"')\n",
    "\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(\n",
    "    llm=llm, prompt=second_prompt,  output_key=\"Final_salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac8303db-c2aa-4001-9d76-da1a9028ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Count the people mentioned in below information:\\n\\n{Background}\"\n",
    ")\n",
    "# chain 3: input= Background and output= Num_people\n",
    "chain_three = LLMChain(\n",
    "    llm=llm, prompt=third_prompt, output_key=\"Num_people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0070f4b-e889-44e5-82b5-93a7961fdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    'Divide the Final salary as \"{Final_salary}\" '\n",
    "    'by number of people as \"{Num_people}\" and return the result.')\n",
    "\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(\n",
    "    llm=llm, prompt=fourth_prompt, output_key=\"Final_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0046959-0fd8-45cb-b300-8138747ab7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Background \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Background\"],\n",
    "    output_variables=[\"Final_salary\", \"Num_people\", \"Final_result\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcc02232-524d-4d7f-bc2d-1bb8d4c66dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = 'John earned $100; Peter earned $150; Mary earned $200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66318f5c-7532-49b6-a0ef-b624ebe36162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Background': 'John earned $100; Peter earned $150; Mary earned $200',\n",
       " 'Final_salary': 'The revised statement would be: The total money earned is $900. \\n\\n$200 (John) + $300 (Peter) + $400 (Mary) = $900',\n",
       " 'Num_people': 'There are three people mentioned: John, Peter, and Mary.',\n",
       " 'Final_result': 'The average salary per person is $300.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain.invoke({'Background': background})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c862e5-cf75-4037-82d1-5fe7a74ad2cf",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <b><font color='darkblue'>Router Chain</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>The purpose of the Router Chain ([LLMRouterChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html)) is to automatically determine which category of template is most suitable for answering a user's question when it is received</font></b>.\n",
    "\n",
    "For example, if the question is about physics, it will select the physics template to generate the answer. This greatly improves the quality and relevance of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da5a23-c2db-4c44-9beb-eb31e23d7159",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Demo of Router Chain</font></b>\n",
    "Designed to guide language models to answer questions as an expert in a specific field. This enables the language model to focus more on knowledge in a particular domain and provide answers based on the expertise of that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc3ed78a-4f21-4c7b-9278-ded310ee9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d46ada93-e14d-468e-b8f7-0bb3916ab4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4bf67e3b-9c44-4895-bb68-55a44f2f6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "470be1cb-8bb1-45ac-99e1-d0d8be28bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"Physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Default\",\n",
    "        \"description\": \"If you don't find a better option, select this option.\",\n",
    "        \"prompt_template\": \"{input}\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75ca26b3-353f-4b9e-9576-249de1ecd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5fdcbaa-b54f-4084-8d7d-e8e03bb00478",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e44c68b3-507a-4ddc-a85a-9f8c3aad1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising \\\n",
    "it will ultimately lead to a better response from the language model. \\\n",
    "If you don't know which prompt is better, select name as \"Default\" always.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "63402435-29a0-4812-9dfd-fa9a7a40fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a06cd475-8eaf-4418-a487-1c019f756210",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d54e73ab-7f74-4742-8ca0-663cb5def802",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain, \n",
    "    destination_chains=destination_chains, \n",
    "    default_chain=default_chain,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4757182e-51da-4058-85dd-6e02d8d7b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is black body radiation?',\n",
       " 'text': \"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation and emits radiation at all frequencies. The radiation emitted by a black body is characterized by a continuous spectrum of wavelengths and intensities that depend only on the temperature of the body. This phenomenon is described by Planck's law of black body radiation, which states that the intensity of radiation emitted by a black body at a given wavelength is proportional to the temperature of the body and follows a specific distribution known as the Planck distribution. Black body radiation is important in various fields of physics, including thermodynamics, quantum mechanics, and astrophysics.\"}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"What is black body radiation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e72239e0-398e-4cc1-8188-edaceb9d0ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "Math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 2 + 2', 'text': 'The answer to 2 + 2 is 4.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"what is 2 + 2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "49ad73af-1013-4395-a4a4-2e85f2c0406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'What day is today?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What day is today?', 'text': 'Today is Thursday.'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'input': \"What day is today?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b83ce-50e3-42f0-91b4-b78c32ddfeb9",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Deeplearning.ai - Langchain Ch2: Model, prompt and parser](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch2_model_prompt_and_parser.ipynb)\n",
    "* [Deeplearning.ai - Langchain Ch3: Memory](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch3_memory.ipynb)\n",
    "* [Deeplearning.ai - Langchain Ch4: Chain](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch4_chains.ipynb)\n",
    "* [Deeplearning.ai - Langchain Ch5: Question and answer](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch5_question_and_answer.ipynb)\n",
    "* [Deeplearning.ai - Langchain Ch6: Evaluation](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch6_evaluation.ipynb)\n",
    "* [Deeplearning.ai - Langchain Ch7: Agents](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch7_agents.ipynb)\n",
    "* [Medium - LangChain — Sequential LLM Calls](https://medium.com/@princekrampah/langchain-sequential-6912164ca568)\n",
    "* [[LangChain for LLM Application Development] 課程筆記- Chains](https://hackmd.io/@YungHuiHsu/SJJvZ-ya2?utm_source=preview-mode&utm_medium=rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
