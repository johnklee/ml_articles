{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1760e11-624f-4ab4-a735-482a24291bfa",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([course link](https://learn.deeplearning.ai/courses/functions-tools-agents-langchain/lesson/2/openai-function-calling)) <b><font size='3ptx'>\"Let's explore how OpenAI can extract arguments from messages and select the right function. .</font> Let's get started and unlock the potential of this powerful tool..</b>\n",
    "\n",
    "Tool calling allows a model to respond to a given prompt by generating output that matches a user-defined schema. While the name implies that the model is performing some action, this is actually not the case! <b>The model is coming up with the arguments to a tool, and actually running the tool</b> (or not) <b>is up to the user - for example, if you want to extract output matching some schema from unstructured text, you could give the model an \"extraction\" tool that takes parameters matching the desired schema, then treat the generated output as your final result</b> ([more](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/)).\n",
    "\n",
    "<b><font color='orange'>Note</font></b>\n",
    "* LLM's don't always produce the same results. The results you see in this notebook may differ from the results you see in the video.\n",
    "* Notebooks results are temporary. Download the notebooks to your local machine if you wish to save your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6eb594-dd3b-4326-b7ad-684637b4cbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.2.6\n",
      "langchain-anthropic==0.1.15\n",
      "langchain-community==0.2.6\n",
      "langchain-core==0.2.10\n",
      "langchain-experimental==0.0.62\n",
      "langchain-google-genai==1.0.6\n",
      "langchain-groq==0.1.3\n",
      "langchain-openai==0.1.9\n",
      "langchain-text-splitters==0.2.0\n",
      "langchainhub==0.1.14\n",
      "openai==1.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -P '(openai|langchain)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94ac3ca-7189-4386-8844-32039504d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import httpx\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "a = load_dotenv(find_dotenv(os.path.expanduser('~/.env'))) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78867b8a-15cb-490b-803c-3306096c6df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b87eb89-3244-45bd-bdda-c17089dcb573",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Define your customized functions as tools</font></b>\n",
    "First, let's create a mock function `get_current_weather` to retrieve a location's temperature.  The OpenAI chatbot will use this function to provide the current weather when asked by the user.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d4b361-0f40-41c9-bd66-adbaa487ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c903d6-171d-424b-b859-745e09241455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a976b-ca6e-4783-9d7b-a7ee23655ddf",
   "metadata": {},
   "source": [
    "Next, let's prepare a message of user to ask about the weather:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bc4394-3d2c-4728-a75c-76290906a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Boston?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a00ed3-7c5a-4604-9af5-a3c10446e42f",
   "metadata": {},
   "source": [
    "Pass the message into chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10b3098-81f5-44a3-964e-9166b7ba946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b6a8042-712f-4520-882f-7c71636451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    ")\n",
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1183da89-ee10-4552-a2bc-ea09c3f3751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9kMlTsQtmT06hRpaFYd1EN0gv2qPB', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather'), tool_calls=None))], created=1720837623, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=15, prompt_tokens=82, total_tokens=97))\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bef06dd-9c00-4a9d-bb5b-1c9fbb2f7322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather'), tool_calls=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0597ac90-152b-492c-879a-a758e1305805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(arguments='{\"location\":\"Boston\"}', name='get_current_weather')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b0fda71-c752-42b0-949b-7cb9a9adaa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\":\"Boston\"}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6197b184-5609-4724-a649-447341fab134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'location': 'Boston'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = json.loads(chat_completion.choices[0].message.function_call.arguments)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f0d55-ec04-4cb9-b37d-edf08a91324b",
   "metadata": {},
   "source": [
    "Let's test the chatbot's behavior by calling the function and checking if it works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f5ec05e-98a9-4740-8a36-69102a9f81fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"location\": \"Boston\", \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_weather(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fb0df-afc0-456a-b14f-34d628e4aefa",
   "metadata": {},
   "source": [
    "How does the chatbot handle non-weather-related questions? Let's find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d59743-e532-4be0-a314-a1e41428db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdfdf68e-7473-4a48-94d6-153910200d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9kMxqYnfIUVHEoKBAeParCRzn8ADj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1720838390, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=76, total_tokens=86))\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    ")\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda4d042-f0f9-4988-8c66-094ac5b3700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07cee54-9369-4d61-816b-6b0e3b9f2dcf",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Function call approach</font></b>\n",
    "There are options you can decide how the function calls are decided:\n",
    "* **`auto`**:\n",
    "* **`none`**:\n",
    "* **`force`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9633761b-2625-47e0-9005-ee200ea73c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module openai.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'Iterable[ChatCompletionMessageParam]', model: 'Union[str, ChatModel]', frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'completion_create_params.FunctionCall | NotGiven' = NOT_GIVEN, functions: 'Iterable[completion_create_params.Function] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, logprobs: 'Optional[bool] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, response_format: 'completion_create_params.ResponseFormat | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, stop: 'Union[Optional[str], List[str]] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, stream_options: 'Optional[ChatCompletionStreamOptionsParam] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'ChatCompletionToolChoiceOptionParam | NotGiven' = NOT_GIVEN, tools: 'Iterable[ChatCompletionToolParam] | NotGiven' = NOT_GIVEN, top_logprobs: 'Optional[int] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'str | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of openai.resources.chat.completions.Completions instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b60d7-82e9-4c43-b9cd-46818fd5c6e7",
   "metadata": {},
   "source": [
    "#### <b>`function_call=\"auto\"`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "288d2b7c-78fe-422d-954e-3b90d2a6ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9kN0U4GxghkfAKwKAKgtppoR5er5W', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1720838554, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=76, total_tokens=86))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f41409f-8de8-47f7-a1d2-6cca0fc40778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't need to call function\n",
    "response.choices[0].message.function_call is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69315851-71d1-402f-8713-ff7b2b481ef2",
   "metadata": {},
   "source": [
    "#### <b>`function_call=\"none\"`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "714e9cb4-0f56-46be-9cd3-2eee074b36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9kN5b7EINNXmTgshob0C2KnMvLBnW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, I'll check the current weather in Taiwan for you. One moment please.\", role='assistant', function_call=None, tool_calls=None))], created=1720838871, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=83, total_tokens=100))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in Taiwan?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "response_none = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call=\"none\",\n",
    ")\n",
    "print(response_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee597e33-e931-4673-ba45-3337d3f5902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we set function_call=\"none\", it still won't call function\n",
    "response.choices[0].message.function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187b729-e802-4426-9087-32da2deefef0",
   "metadata": {},
   "source": [
    "#### <b>`function_call={\"name\": \"get_current_weather\"}`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f869040-1ac3-4969-9a74-378448cce625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9kN6rHuiyZYhMeNUjNejHWOdke5I6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"San Francisco, CA\",\"unit\":\"celsius\"}', name='get_current_weather'), tool_calls=None))], created=1720838949, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=86, total_tokens=99))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi!\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=llm_model,\n",
    "    messages=messages,\n",
    "    functions=functions,\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b2ef039-1bb5-4b5d-9e7f-f78a63be48fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(arguments='{\"location\":\"San Francisco, CA\",\"unit\":\"celsius\"}', name='get_current_weather')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can force the chatbot to call an unnecessary function, which may result in the generation of mock arguments:\n",
    "response.choices[0].message.function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4bf6ae-0929-4e0d-a3d7-a08947e332c6",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Deeplearning.ai - Functions, Tools and Agents with LangChain ch3: LangChain Expression Language (LCEL)](https://learn.deeplearning.ai/courses/functions-tools-agents-langchain/lesson/3/langchain-expression-language-(lcel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
