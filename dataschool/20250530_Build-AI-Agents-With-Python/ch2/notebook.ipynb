{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd9ff43-16d5-4b20-b6de-0342fb78d5e6",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([Source](https://courses.dataschool.io/view/courses/build-ai-agents-with-python/3021201-chapter-2-instructing-the-app/9838472-reviewing-the-workflow-and-chatbot)) Reviewing the workflow and chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b035b7e4-7e45-4dd8-8190-ab9c53f916c9",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Section 1: Reviewing the workflow and chatbot</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0cb5d72-28d9-4789-99a9-65dba6bc83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24115a60-ba5b-43d9-a075-f9b43784a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98326cb3-f9db-4c24-a080-57cd83da9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbf44aee-e0c7-4f49-83e0-9fdcce45b8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1 + 2 = 3', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--89a90f41-5315-45ea-9d9f-30786c8edf91-0', usage_metadata={'input_tokens': 7, 'output_tokens': 8, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = model.invoke('What is 1+2?')\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33cc1b9f-2a1b-4c48-9302-dc1873c5c87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 2 = 3'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f30a39-50bb-4492-b89e-1eacdd3f807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    updated_messages = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": updated_messages}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"model_node\", call_model)\n",
    "workflow.add_edge(START, \"model_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83fc7f82-d8c3-40d3-9a84-3803109def9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce720473-6d45-48ec-aef8-81151647ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(chat_id: int):\n",
    "    config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User:\")\n",
    "\n",
    "        if user_input in [\"exit\", \"quit\"]:\n",
    "            print(\"AI: See you later!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"AI: \", end=\"\")\n",
    "            for chunk, metadata in app.stream({\"messages\": user_input}, config, stream_mode=\"messages\"):\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c85bd9-5db4-4005-8194-1a40f0b0fb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is 1+1?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 1 + 1 = 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: See you later!\n"
     ]
    }
   ],
   "source": [
    "chatbot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a58d9-c60f-4e26-9d69-b0cb5ce15aa3",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Section 2: Adding a system prompt</font></b>\n",
    "([Source](https://courses.dataschool.io/view/courses/build-ai-agents-with-python/3021201-chapter-2-instructing-the-app/9850941-adding-a-system-prompt)) [**LangChain Messages**](https://python.langchain.com/docs/concepts/messages/#langchain-messages): LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "\n",
    "LangChain messages are Python objects that subclass from a [**BaseMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html). The five main message types are:\n",
    "* [**SystemMessage**](https://python.langchain.com/docs/concepts/messages/#systemmessage): corresponds to system role\n",
    "* [**HumanMessage**](https://python.langchain.com/docs/concepts/messages/#humanmessage): corresponds to user role\n",
    "* [**AIMessage**](https://python.langchain.com/docs/concepts/messages/#aimessage): corresponds to assistant role\n",
    "* [**AIMessageChunk**](https://python.langchain.com/docs/concepts/messages/#aimessagechunk): corresponds to assistant role, used for streaming responses\n",
    "* [**ToolMessage**](https://python.langchain.com/docs/concepts/messages/#toolmessage): corresponds to tool role\n",
    "\n",
    "Below we will leverage [**ChatPromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#chatprompttemplate) to compose system message with human message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11a89883-386f-497e-883c-af5ba7b8e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    ('system', 'Limit all of your responses to two sentences.'),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b4280d2-1af8-4b4f-aef8-aa515be243d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {'messages': ['What is the history of AI?']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7434daa-bd4d-41f9-ab43-bc3f78e7d990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Limit all of your responses to two sentences.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the history of AI?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc681e3-ce3c-4e5c-8fab-aee6cb783ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The history of AI began in the mid-20th century with pioneers exploring symbolic reasoning and early neural networks. Over the decades, AI has experienced periods of excitement and setbacks, leading to the current era of machine learning and deep learning.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--90fe9675-1b64-4f1c-99d0-cced4f2c17d2-0', usage_metadata={'input_tokens': 16, 'output_tokens': 49, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = model.invoke(prompt.invoke(state))\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b22e4a21-af5a-42f7-998a-066763f69474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The history of AI began in the mid-20th century with pioneers exploring symbolic reasoning and early neural networks. Over the decades, AI has experienced periods of excitement and setbacks, leading to the current era of machine learning and deep learning.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8002c9f-b772-42fe-b182-4509f76cfa90",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Section 3: Chaining multiple actions</font></b>\n",
    "([source](https://courses.dataschool.io/view/courses/build-ai-agents-with-python/3021201-chapter-2-instructing-the-app/9851093-chaining-multiple-actions)) One point about [**LangChain Expression Language**](https://python.langchain.com/docs/concepts/lcel/) is that any two [**Runable**](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html)s can be \"chained\" together into sequences.\n",
    "\n",
    "The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. This can be done using the pipe operator (`|`), or the more explicit `.pipe()` method, which does the same thing ([more](https://python.langchain.com/docs/how_to/sequence/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2c962ca-599d-45d4-a0ea-e44923c60be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bfff04f-3ed5-4865-9809-e85feceb66d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLMs evolved from earlier natural language processing models, gaining prominence with the introduction of transformers in 2017. These models have since grown in size and capability, leading to significant advancements in various language-based tasks.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {'messages': ['What is the history of LLM?']}\n",
    "chain.invoke(state).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560233bd-5728-444f-97b8-0aed494a7415",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Section 4: Calling the model</font></b>\n",
    "Let's apply what we learn from previous section to rewrite our chat app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8e72e38-8434-4ba7-96de-e95467a8c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def call_model(state: MessagesState, prompt, model):\n",
    "    chain = prompt | model\n",
    "    updated_messages = chain.invoke(state)\n",
    "    return {\"messages\": updated_messages}\n",
    "\n",
    "\n",
    "short_chinese_prompt = ChatPromptTemplate([\n",
    "    ('system', 'Limit all of your responses to two sentences and use Chinese.'),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "short_chinese_call_model = partial(call_model, prompt=short_chinese_prompt, model=model)\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"model_node\", short_chinese_call_model)\n",
    "workflow.add_edge(START, \"model_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "chinese_app = workflow.compile(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2efc768a-ccc7-4d81-af15-2c1d5527c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(chat_id: int, app):\n",
    "    config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User:\")\n",
    "\n",
    "        if user_input in [\"exit\", \"quit\"]:\n",
    "            print(\"AI: See you later!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"AI: \", end=\"\")\n",
    "            for chunk, metadata in app.stream({\"messages\": user_input}, config, stream_mode=\"messages\"):\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41ddedee-7d32-43b0-829e-bd8476709c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is the history of machine learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: 机器学习起源于20世纪50年代，并在随后的几十年里经历了多次发展和变革。它从最初的符号主义方法逐渐发展到统计学习和深度学习等更复杂的技术。\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: See you later!\n"
     ]
    }
   ],
   "source": [
    "chatbot(0, chinese_app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fba00e-410b-4d0e-9426-002fe5fd57b7",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Section 5: Reviewing the updated workflow</font></b>\n",
    "([source](https://courses.dataschool.io/view/courses/build-ai-agents-with-python/3021201-chapter-2-instructing-the-app/9966020-reviewing-the-updated-workflow)) Let's pull things together in one cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69e2800f-8cb7-4bb4-ad67-966bdf3a589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "short_chinese_prompt = ChatPromptTemplate([\n",
    "    ('system', 'Limit all of your responses to two sentences and use Traditional Chinese.'),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "\n",
    "chain = short_chinese_prompt | model\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState, chain=chain):\n",
    "    updated_messages = chain.invoke(state)\n",
    "    return {\"messages\": updated_messages}\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"model_node\", call_model)\n",
    "workflow.add_edge(START, \"model_node\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "chinese_app = workflow.compile(memory)\n",
    "\n",
    "\n",
    "def chatbot(chat_id: int, app=chinese_app):\n",
    "    config = {\"configurable\": {\"thread_id\": chat_id}}\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"User:\")\n",
    "\n",
    "        if user_input in [\"exit\", \"quit\"]:\n",
    "            print(\"AI: See you later!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"AI: \", end=\"\")\n",
    "            for chunk, metadata in app.stream({\"messages\": user_input}, config, stream_mode=\"messages\"):\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a626b6c-33c9-4e42-bbd6-f6d9f00e8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: What is the history of RAG?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: RAG (檢索增強生成) 的歷史相對較新，它源於對大型語言模型在知識密集型任務中局限性的認識。這種方法旨在通過從外部知識源檢索相關信息來增強模型的生成能力，從而提高其準確性和可靠性。\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: See you later!\n"
     ]
    }
   ],
   "source": [
    "chatbot(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
