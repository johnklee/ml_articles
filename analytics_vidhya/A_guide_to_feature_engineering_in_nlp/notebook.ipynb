{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3d680f",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://www.analyticsvidhya.com/blog/2021/04/a-guide-to-feature-engineering-in-nlp/)) <font size='3ptx'><b>\"If 80 percent of our work is data preparation, then ensuring data quality is the important work of a machine learning team.” - – Andrew Ng</b> Feature engineering is one of the most important steps in machine learning. It is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Think machine learning algorithm as a learning child the more accurate information you provide the more they will be able to interpret the information well. Focusing first on our data will give us better results than focusing only on models. Feature engineering helps us to create better data which helps the model understand it well and provide reasonable results.</font>\n",
    "\n",
    "<b>NLP is a subfield of artificial intelligence where we understand human interaction with machines using natural languages</b>. To understand a natural language, you need to understand how we write a sentence, how we express our thoughts using different words, signs, special characters, etc basically we should understand the context of the sentence to interpret its meaning.\n",
    "\n",
    "<b>If we can use these contexts as features and feed them to our model then the model will be able to understand the sentence better</b>. Some of the common features that we can extract from a sentence are the number of words, number of capital words, number of punctuation, number of unique words, number of stopwords, average sentence length, etc. We can define these features based on our data set we are using. In this blog, we will use a Twitter data set so we can add some others features like the number of hashtags, number of mentions, etc. We will discuss them in detail in the coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ca056",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "### <font color='darkgreen'>Table of Content</font>\n",
    "* <font size='3ptx'><b><a href='#sect1'>NLP task overview</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect2'>List of features with code</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect3'>Implementation</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4'>Results comparison with and without doing feature engineering</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect5'>Conclusion</a></b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8840748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f856407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a281528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42b680fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "RF_N_ESTIMATORS = 500\n",
    "RF_MIN_SAMPLES_SPLIT = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c67d9f",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>NLP task overview</font>\n",
    "<font size='3ptx'><b>To understand the feature engineering task in NLP, we will be implementing it on a Twitter dataset</b>. We will be using COVID-19 Fake News Dataset. The task is to classify the tweet as Fake or Real.</font>\n",
    "\n",
    "You can download the dataset from [here](https://www.kaggle.com/datasets/arashnic/covid19-fake-news?resource=download) (or [here](https://github.com/MiHarsh/Public_stuffs)). The dataset is divided into train, validation, and test set.  Below is the distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f94722",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('https://raw.githubusercontent.com/MiHarsh/Public_stuffs/master/Constraint_English_Train%20-%20Sheet1.csv')\n",
    "test_df = pd.read_csv('https://raw.githubusercontent.com/MiHarsh/Public_stuffs/master/Constraint_English_Val%20-%20Sheet1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd8342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>4769</td>\n",
       "      <td>Arizona's cases are declining and although tes...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6141</th>\n",
       "      <td>6142</td>\n",
       "      <td>BREAKING: Boris Johnson set to announce pubs b...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>2985</td>\n",
       "      <td>Non-#COVID19 patients continue to require cont...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label\n",
       "4768  4769  Arizona's cases are declining and although tes...  real\n",
       "6141  6142  BREAKING: Boris Johnson set to announce pubs b...  real\n",
       "2984  2985  Non-#COVID19 patients continue to require cont...  real"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d4fb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>1736</td>\n",
       "      <td>Commonly used drugs hydroxychloroquine and chl...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>1087</td>\n",
       "      <td>First case of coronavirus traced to Marine put...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>317</td>\n",
       "      <td>???Covid-19 means ???certificate of identifica...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label\n",
       "1735  1736  Commonly used drugs hydroxychloroquine and chl...  fake\n",
       "1086  1087  First case of coronavirus traced to Marine put...  fake\n",
       "316    317  ???Covid-19 means ???certificate of identifica...  fake"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f0b6a3",
   "metadata": {},
   "source": [
    "Below is the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c248038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fd43f7b6da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVoklEQVR4nO3df7DldX3f8efL5Yc2GFnKDYO7m0LiqkE7Qb0iYqZVVFho2pWGKNSR1ZqurVDjJOMI6XSIWhqT0dAxVeKmbISWiEiwbiyBbJDoGEW5KgILMtygZHeDcnERo0yw4Lt/nM+Op+vucoH7PR/uvc/HzJn7Pe/P5/s97ztcXvPdz/me70lVIUmavKf0bkCSlisDWJI6MYAlqRMDWJI6MYAlqRMDWJI6GSyAkzw1yZeSfC3JtiTvavWPJPlGkpva49hWT5IPJJlNcnOSF44da0OSO9tjw1A9S9IkHTDgsR8CTqyq7yc5EPhckj9vY++oqiv3mH8KsLY9XgJcBLwkyWHA+cA0UMCXk2ypqvv39cLr1q2ra665ZoF/HUl63LK34mBnwDXy/fb0wPbY36c+1gOXtv1uAA5NciRwMrC1qna10N0KrNvfa993331P/BeQpIENugacZEWSm4B7GYXoF9vQBW2Z4cIkB7faKmD72O47Wm1f9T1fa2OSmSQzc3NzC/2rSNKCGzSAq+qRqjoWWA0cl+T5wHnAc4EXA4cB71yg19pUVdNVNT01NbUQh5SkQU3kKoiq+i5wPbCuqu5pywwPAX8MHNem7QTWjO22utX2VZekRW3IqyCmkhzatp8GvBr4elvXJUmA1wC3tl22AGe1qyGOBx6oqnuAa4GTkqxMshI4qdUkaVEb8iqII4FLkqxgFPRXVNWnknw6yRSjdwVvAv59m381cCowCzwIvAmgqnYleQ9wY5v37qraNWDfkjQRWYq3o5yenq6ZmZnebUjSbpO9DE2StH8GsCR1YgBLUicGsCR1YgBLUicGsCR1MuR1wIvOqjU/y9/t2P7oE7VsPHP1GnZu/9vebWiJMoDH/N2O7bzuw5/v3YaeRD72lhN6t6AlzCUISerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTg7o3YD0pPaUA0jSuws9STxz9Rp2bv/bBTueASztz48e5nUf/nzvLvQk8bG3nLCgx3MJQpI6GSyAkzw1yZeSfC3JtiTvavWjk3wxyWySjyU5qNUPbs9n2/hRY8c6r9XvSHLyUD1L0iQNeQb8EHBiVf0icCywLsnxwO8CF1bVs4D7gTe3+W8G7m/1C9s8khwDnAE8D1gHfCjJigH7lqSJGCyAa+T77emB7VHAicCVrX4J8Jq2vb49p42/MqN3P9YDl1fVQ1X1DWAWOG6oviVpUgZdA06yIslNwL3AVuBvgO9W1cNtyg5gVdteBWwHaOMPAP94vL6XfcZfa2OSmSQzc3NzA/w2krSwBg3gqnqkqo4FVjM6a33ugK+1qaqmq2p6ampqqJeRpAUzkasgquq7wPXAS4FDk+y+/G01sLNt7wTWALTxZwDfGa/vZR9JWrSGvApiKsmhbftpwKuB2xkF8elt2gbgk217S3tOG/90VVWrn9GukjgaWAt8aai+JWlShvwgxpHAJe2KhacAV1TVp5LcBlye5L8AXwUubvMvBv5nkllgF6MrH6iqbUmuAG4DHgbOrqpHBuxbkiZisACuqpuBF+ylfhd7uYqhqv4B+NV9HOsC4IKF7lGSevKTcJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0YwJLUiQEsSZ0MFsBJ1iS5PsltSbYl+fVW/+0kO5Pc1B6nju1zXpLZJHckOXmsvq7VZpOcO1TPkjRJBwx47IeB36yqryR5OvDlJFvb2IVV9b7xyUmOAc4Angc8E/jLJM9uwx8EXg3sAG5MsqWqbhuwd0ka3GABXFX3APe07b9Pcjuwaj+7rAcur6qHgG8kmQWOa2OzVXUXQJLL21wDWNKiNpE14CRHAS8AvthK5yS5OcnmJCtbbRWwfWy3Ha22r/qer7ExyUySmbm5uYX+FSRpwQ0ewEkOAf4UeHtVfQ+4CPh54FhGZ8jvX4jXqapNVTVdVdNTU1MLcUhJGtSQa8AkOZBR+F5WVVcBVNW3x8b/CPhUe7oTWDO2++pWYz91SVq0hrwKIsDFwO1V9ftj9SPHpp0G3Nq2twBnJDk4ydHAWuBLwI3A2iRHJzmI0Rt1W4bqW5ImZcgz4JcBbwBuSXJTq/0WcGaSY4ECvgm8BaCqtiW5gtGbaw8DZ1fVIwBJzgGuBVYAm6tq24B9S9JEDHkVxOeA7GXo6v3scwFwwV7qV+9vP0lajPwknCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUicGsCR1YgBLUieDBXCSNUmuT3Jbkm1Jfr3VD0uyNcmd7efKVk+SDySZTXJzkheOHWtDm39nkg1D9SxJkzTkGfDDwG9W1THA8cDZSY4BzgWuq6q1wHXtOcApwNr22AhcBKPABs4HXgIcB5y/O7QlaTEbLICr6p6q+krb/nvgdmAVsB64pE27BHhN214PXFojNwCHJjkSOBnYWlW7qup+YCuwbqi+JWlSJrIGnOQo4AXAF4EjquqeNvQt4Ii2vQrYPrbbjlbbV33P19iYZCbJzNzc3ML+ApI0gMEDOMkhwJ8Cb6+q742PVVUBtRCvU1Wbqmq6qqanpqYW4pCSNKhBAzjJgYzC97KquqqVv92WFmg/7231ncCasd1Xt9q+6pK0qA15FUSAi4Hbq+r3x4a2ALuvZNgAfHKsfla7GuJ44IG2VHEtcFKSle3Nt5NaTZIWtQMGPPbLgDcAtyS5qdV+C3gvcEWSNwN3A69tY1cDpwKzwIPAmwCqaleS9wA3tnnvrqpdA/YtSRMxWABX1eeA7GP4lXuZX8DZ+zjWZmDzwnUnSf35SThJ6sQAlqRO5hXASV42n5okaf7mewb8B/OsSZLmab9vwiV5KXACMJXkN8aGfhpYMWRjkrTUPdpVEAcBh7R5Tx+rfw84faimJGk52G8AV9VngM8k+UhV3T2hniRpWZjvdcAHJ9kEHDW+T1WdOERTkrQczDeAPw78IfA/gEeGa0eSlo/5BvDDVXXRoJ1I0jIz38vQ/izJW5Mc2b5S6LD2TRWSpMdpvmfAu+9e9o6xWgE/t7DtSNLyMa8Arqqjh25EkpabeQVwkrP2Vq+qSxe2HUlaPua7BPHise2nMrqd5FcAA1iSHqf5LkH8x/HnSQ4FLh+iIUlaLh7v7Sh/ALguLElPwHzXgP+MH3978QrgF4ArhmpKkpaD+a4Bv29s+2Hg7qraMUA/krRszGsJot2U5+uM7oi2EvjhkE1J0nIw32/EeC3wJeBXGX2L8ReTeDtKSXoC5rsE8Z+AF1fVvQBJpoC/BK4cqjFJWurmexXEU3aHb/Odx7CvJGkv5nsGfE2Sa4GPtuevA64epiVJWh4e7TvhngUcUVXvSPKvgV9qQ18ALhu6OUlayh7tDPi/AecBVNVVwFUASf5pG/uXA/YmSUvao63jHlFVt+xZbLWjBulIkpaJRwvgQ/cz9rQF7EOSlp1HC+CZJP9uz2KSXwO+PExLkrQ8PNoa8NuBTyR5PT8O3GngIOC0AfuSpCVvvwFcVd8GTkjyCuD5rfx/qurTg3cmSUvcfO8HfD1w/cC9SNKy4qfZJKkTA1iSOjGAJamTwQI4yeYk9ya5daz220l2JrmpPU4dGzsvyWySO5KcPFZf12qzSc4dql9JmrQhz4A/AqzbS/3Cqjq2Pa4GSHIMcAbwvLbPh5KsSLIC+CBwCnAMcGabK0mL3nzvhvaYVdVnkxw1z+nrgcur6iHgG0lmgePa2GxV3QWQ5PI297aF7leSJq3HGvA5SW5uSxQrW20VsH1szo5W21f9JyTZmGQmyczc3NwQfUvSgpp0AF8E/DxwLHAP8P6FOnBVbaqq6aqanpqaWqjDStJgBluC2Jv2yToAkvwR8Kn2dCewZmzq6lZjP3VJWtQmegac5Mixp6cBu6+Q2AKckeTgJEcDaxl9CeiNwNokRyc5iNEbdVsm2bMkDWWwM+AkHwVeDhyeZAdwPvDyJMcCBXwTeAtAVW1LcgWjN9ceBs6uqkfacc4BrgVWAJurattQPUvSJA15FcSZeylfvJ/5FwAX7KV+NX7/nKQlyE/CSVInBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdTJYACfZnOTeJLeO1Q5LsjXJne3nylZPkg8kmU1yc5IXju2zoc2/M8mGofqVpEkb8gz4I8C6PWrnAtdV1VrguvYc4BRgbXtsBC6CUWAD5wMvAY4Dzt8d2pK02A0WwFX1WWDXHuX1wCVt+xLgNWP1S2vkBuDQJEcCJwNbq2pXVd0PbOUnQ12SFqVJrwEfUVX3tO1vAUe07VXA9rF5O1ptX/WfkGRjkpkkM3NzcwvbtSQNoNubcFVVQC3g8TZV1XRVTU9NTS3UYSVpMJMO4G+3pQXaz3tbfSewZmze6lbbV12SFr1JB/AWYPeVDBuAT47Vz2pXQxwPPNCWKq4FTkqysr35dlKrSdKid8BQB07yUeDlwOFJdjC6muG9wBVJ3gzcDby2Tb8aOBWYBR4E3gRQVbuSvAe4sc17d1Xt+caeJC1KgwVwVZ25j6FX7mVuAWfv4zibgc0L2JokPSn4SThJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqRODGBJ6sQAlqROugRwkm8muSXJTUlmWu2wJFuT3Nl+rmz1JPlAktkkNyd5YY+eJWmh9TwDfkVVHVtV0+35ucB1VbUWuK49BzgFWNseG4GLJt6pJA3gybQEsR64pG1fArxmrH5pjdwAHJrkyA79SdKC6hXABfxFki8n2dhqR1TVPW37W8ARbXsVsH1s3x2t9v9JsjHJTJKZubm5ofqWpAVzQKfX/aWq2pnkZ4CtSb4+PlhVlaQeywGrahOwCWB6evox7StJPXQ5A66qne3nvcAngOOAb+9eWmg/723TdwJrxnZf3WqStKhNPICT/FSSp+/eBk4CbgW2ABvatA3AJ9v2FuCsdjXE8cADY0sVkrRo9ViCOAL4RJLdr/8nVXVNkhuBK5K8GbgbeG2bfzVwKjALPAi8afItS9LCm3gAV9VdwC/upf4d4JV7qRdw9gRak6SJejJdhiZJy4oBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1MmiCeAk65LckWQ2ybm9+5GkJ2pRBHCSFcAHgVOAY4AzkxzTtytJemIWRQADxwGzVXVXVf0QuBxY37knSXpCUlW9e3hUSU4H1lXVr7XnbwBeUlXnjM3ZCGxsT58D3DHxRpeOw4H7ejehJxX/Jp6Y+6pq3Z7FA3p0MoSq2gRs6t3HUpBkpqqme/ehJw//JoaxWJYgdgJrxp6vbjVJWrQWSwDfCKxNcnSSg4AzgC2de5KkJ2RRLEFU1cNJzgGuBVYAm6tqW+e2ljKXcrQn/yYGsCjehJOkpWixLEFI0pJjAEtSJwawHrMk30xyeO8+tDCSvC3J7Uku28f4G5P890n3tRwsijfhNJwkYfRewI9696Ju3gq8qqp29G5kufEMeBlKclS7sdGlwK3Af05yY5Kbk7xrbN7/TvLlJNvaJw21xCT5Q+DngD9P8s4kX0jy1SSfT/Kcvcz/F23O4UlOattfSfLxJIdM/jdY3LwKYhlKchRwF3AC8NPA6cBbgDC6vvr3quqzSQ6rql1JnsboWux/XlXfSfJNYLqq/GjqErD7vyfwQ+DBdtnnq4D/UFW/kuSNbfw64DeAf8XoctCrgFOq6gdJ3gkcXFXv7vE7LFYuQSxfd1fVDUneB5wEfLXVDwHWAp8F3pbktFZf0+rfmXinmpRnAJckWQsUcODY2ImMQvikqvpekl9mdGfCvx6tYnEQ8IUJ97voGcDL1w/azwC/U1UfHh9M8nLgVcBLq+rBJH8FPHWSDWri3gNcX1WntX8l/dXY2N8wWqp4NjDD6O9ma1WdOekmlxLXgHUt8G93r98lWZXkZxidDd3fwve5wPE9m9REPIMf32PljXuM3Q38CnBpkucBNwAvS/IsgCQ/leTZk2p0qTCAl7mq+gvgT4AvJLkFuBJ4OnANcECS24H3MvofTkvb7wG/k+Sr7OVfx1X1deD1wMcZvXfwRuCjSW5mtPzw3Mm1ujT4JpwkdeIZsCR1YgBLUicGsCR1YgBLUicGsCR1YgBrWUny/UcZPyrJrY/xmB9p39wtPSYGsCR1YgBrWUpySJLr2p28bkmyfmz4gCSXtXvkXpnkH7V9XpTkM+0OcdcmObJT+1oiDGAtV/8AnFZVLwReAby/3RsZ4DnAh6rqF4DvAW9NciDwB8DpVfUiYDNwQYe+tYR4Mx4tVwH+a5J/BvwIWAUc0ca2V9Vft+3/BbyN0Ueznw9sbTm9Arhnoh1ryTGAtVy9HpgCXlRV/7fdE3f33d72/Hx+MQrsbVX10sm1qKXOJQgtV88A7m3h+wrgn4yN/WyS3UH7b4DPAXcAU7vrSQ5sdwWTHjcDWMvVZcB0uwPcWcDXx8buAM5ud4JbCVxUVT9k9M0hv5vka8BNjL5RRHrcvBuaJHXiGbAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdfL/AMlTW+D4qE9eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training data\n",
    "sns.displot(train_df, x=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22653ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fd43f7b7310>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing data\n",
    "sns.displot(test_df, x=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34629a87",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>List of features</font> ([back](#sect0))\n",
    "I will be listing out a total of 15 features that we can use for the above dataset, number of features totally depends upon the type of dataset you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e00b07",
   "metadata": {},
   "source": [
    "#### 1. Number of Characters\n",
    "Count the number of characters present in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f7b0a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232b327f",
   "metadata": {},
   "source": [
    "#### 2. Number of words\n",
    "Count the number of words present in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2dc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bce17f",
   "metadata": {},
   "source": [
    "#### 3. Number of capital characters\n",
    "Count the number of capital characters present in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f04164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_chars(text):\n",
    "    count=0\n",
    "    for i in text:\n",
    "        if i.isupper():\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b156a4",
   "metadata": {},
   "source": [
    "#### 4. Number of capital words\n",
    "Count the number of capital words present in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20119ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a002d7",
   "metadata": {},
   "source": [
    "#### 5. Count the number of punctuations\n",
    "In this function, we return a dictionary of 32 punctuation with the counts, which can be used as separate features, which I will discuss in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe3805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuations(text):\n",
    "    punctuations='!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~'\n",
    "    d=dict()\n",
    "    for i in punctuations:\n",
    "        d[str(i)+' count']=text.count(i)\n",
    "    return d "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0819c",
   "metadata": {},
   "source": [
    "#### 6. Number of words in quotes\n",
    "The number of words in the single quotation and double quotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d462847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_in_quotes(text):\n",
    "    x = re.findall(\"'.'|\\\".\\\"\", text)\n",
    "    count=0\n",
    "    if x is None:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in x:\n",
    "            t=i[1:-1]\n",
    "            count+=count_words(t)\n",
    "        return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4cf36",
   "metadata": {},
   "source": [
    "#### 7. Number of sentences\n",
    "Count the number of sentences in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d55ce",
   "metadata": {},
   "source": [
    "#### 8. Count the number of unique words\n",
    "Count the number of unique words in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e148ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918ea7c",
   "metadata": {},
   "source": [
    "#### 9. Count of hashtags\n",
    "Since we are using the Twitter dataset we can count the number of times users used the hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da43bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_htags(text):\n",
    "    x = re.findall(r'(#w[A-Za-z0-9]*)', text)\n",
    "    return len(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2084f9",
   "metadata": {},
   "source": [
    "#### 10. Count of mentions\n",
    "On Twitter, most of the time people reply or mention someone in their tweet, counting the number of mentions can also be treated as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b5661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mentions(text):\n",
    "    x = re.findall(r'(@w[A-Za-z0-9]*)', text)\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c47f74",
   "metadata": {},
   "source": [
    "#### 11. Count of stopwords\n",
    "Here we will count the number of stopwords used in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f24a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  \n",
    "    word_tokens = word_tokenize(text)\n",
    "    stopwords_x = [w for w in word_tokens if w in stop_words]\n",
    "    return len(stopwords_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0490b",
   "metadata": {},
   "source": [
    "#### 12. Calculating average word length\n",
    "This can be calculated by dividing the counts of characters by counts of words.\n",
    "```python\n",
    "train_df['avg_wordlength'] = train_df['char_count']/train_df['word_count']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893600f7",
   "metadata": {},
   "source": [
    "#### 13. Calculating average sentence length\n",
    "This can be calculated by dividing the counts of words by the counts of sentences.\n",
    "```python\n",
    "train_df['avg_sentlength'] = train_df['word_count']/train_df['sent_count']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb67cbe",
   "metadata": {},
   "source": [
    "#### 14. unique words vs word count feature\n",
    "This feature is basically the ratio of unique words to a total number of words.\n",
    "```python\n",
    "df['unique_vs_words'] = df['unique_word_count']/df['word_count']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c402d1",
   "metadata": {},
   "source": [
    "#### 15. Stopwords count vs words counts feature\n",
    "This feature is also the ratio of counts of stopwords to the total number of words.\n",
    "```python\n",
    "df['stopwords_vs_words'] = df['stopword_count']/df['word_count']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f539f6b9",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Implementation</font> ([back](#sect0))\n",
    "We will focus more on feature engineering, for this we will keep the approach simple, by using TF-IDF and simple pre-processing. All the code will be available on my GitHub repository https://github.com/ahmadkhan242/Feature-Engineering-in-NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bf849",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Applying the above-defined feature extraction on train and test set</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "490b281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(df):\n",
    "    df['char_count'] = df[\"tweet\"].apply(lambda x:count_chars(x))\n",
    "    df['word_count'] = df[\"tweet\"].apply(lambda x:count_words(x))\n",
    "    df['sent_count'] = df[\"tweet\"].apply(lambda x:count_sent(x))\n",
    "    df['capital_char_count'] = df[\"tweet\"].apply(lambda x:count_capital_chars(x))\n",
    "    df['capital_word_count'] = df[\"tweet\"].apply(lambda x:count_capital_words(x))\n",
    "    df['quoted_word_count'] = df[\"tweet\"].apply(lambda x:count_words_in_quotes(x))\n",
    "    df['stopword_count'] = df[\"tweet\"].apply(lambda x:count_stopwords(x))\n",
    "    df['unique_word_count'] = df[\"tweet\"].apply(lambda x:count_unique_words(x))\n",
    "    df['htag_count'] = df[\"tweet\"].apply(lambda x:count_htags(x))\n",
    "    df['mention_count'] = df[\"tweet\"].apply(lambda x:count_mentions(x))\n",
    "    df['punct_count'] = df[\"tweet\"].apply(lambda x:count_punctuations(x))\n",
    "    df['avg_wordlength'] = df['char_count']/df['word_count']\n",
    "    df['avg_sentlength'] = df['word_count']/df['sent_count']\n",
    "    df['unique_vs_words'] = df['unique_word_count']/df['word_count']\n",
    "    df['stopwords_vs_words'] = df['stopword_count']/df['word_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2ad7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = gen_features(train_df)\n",
    "test_df = gen_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d240b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>quoted_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>punct_count</th>\n",
       "      <th>avg_wordlength</th>\n",
       "      <th>avg_sentlength</th>\n",
       "      <th>unique_vs_words</th>\n",
       "      <th>stopwords_vs_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>3579</td>\n",
       "      <td>Jacob Rees-Mogg has told people to stop the \"e...</td>\n",
       "      <td>real</td>\n",
       "      <td>183</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 4, '# count': 1, '$ ...</td>\n",
       "      <td>7.320000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6278</th>\n",
       "      <td>6279</td>\n",
       "      <td>Over the last 5 days we have processed more th...</td>\n",
       "      <td>real</td>\n",
       "      <td>262</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 0, '$ ...</td>\n",
       "      <td>5.574468</td>\n",
       "      <td>15.666667</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.361702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>3902</td>\n",
       "      <td>Two West Virginia Brothers Can't Afford Covid-...</td>\n",
       "      <td>fake</td>\n",
       "      <td>138</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'! count': 0, '\" count': 0, '# count': 1, '$ ...</td>\n",
       "      <td>7.263158</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label  \\\n",
       "3578  3579  Jacob Rees-Mogg has told people to stop the \"e...  real   \n",
       "6278  6279  Over the last 5 days we have processed more th...  real   \n",
       "3901  3902  Two West Virginia Brothers Can't Afford Covid-...  fake   \n",
       "\n",
       "      char_count  word_count  sent_count  capital_char_count  \\\n",
       "3578         183          25           1                   5   \n",
       "6278         262          47           3                   2   \n",
       "3901         138          19           1                  21   \n",
       "\n",
       "      capital_word_count  quoted_word_count  stopword_count  \\\n",
       "3578                   0                  0               9   \n",
       "6278                   0                  0              17   \n",
       "3901                   0                  0               1   \n",
       "\n",
       "      unique_word_count  htag_count  mention_count  \\\n",
       "3578                 24           0              0   \n",
       "6278                 42           0              0   \n",
       "3901                 18           0              0   \n",
       "\n",
       "                                            punct_count  avg_wordlength  \\\n",
       "3578  {'! count': 0, '\" count': 4, '# count': 1, '$ ...        7.320000   \n",
       "6278  {'! count': 0, '\" count': 0, '# count': 0, '$ ...        5.574468   \n",
       "3901  {'! count': 0, '\" count': 0, '# count': 1, '$ ...        7.263158   \n",
       "\n",
       "      avg_sentlength  unique_vs_words  stopwords_vs_words  \n",
       "3578       25.000000         0.960000            0.360000  \n",
       "6278       15.666667         0.893617            0.361702  \n",
       "3901       19.000000         0.947368            0.052632  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c0001",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Adding some extra features using punctuation count</font>\n",
    "We will create a DataFrame from the dictionary returned by the “punct_count” function and then merge it with the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a81b446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>! count</th>\n",
       "      <th>\" count</th>\n",
       "      <th># count</th>\n",
       "      <th>$ count</th>\n",
       "      <th>% count</th>\n",
       "      <th>&amp; count</th>\n",
       "      <th>' count</th>\n",
       "      <th>( count</th>\n",
       "      <th>) count</th>\n",
       "      <th>* count</th>\n",
       "      <th>...</th>\n",
       "      <th>[ count</th>\n",
       "      <th>\\ count</th>\n",
       "      <th>] count</th>\n",
       "      <th>^ count</th>\n",
       "      <th>_ count</th>\n",
       "      <th>` count</th>\n",
       "      <th>{ count</th>\n",
       "      <th>| count</th>\n",
       "      <th>} count</th>\n",
       "      <th>~ count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ! count  \" count  # count  $ count  % count  & count  ' count  ( count  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        2        0        0        0        0        1   \n",
       "\n",
       "   ) count  * count  ...  [ count  \\ count  ] count  ^ count  _ count  \\\n",
       "0        0        0  ...        0        0        0        0        0   \n",
       "1        0        0  ...        0        0        0        0        0   \n",
       "2        1        0  ...        0        0        0        0        0   \n",
       "\n",
       "   ` count  { count  | count  } count  ~ count  \n",
       "0        0        0        0        0        0  \n",
       "1        0        0        0        0        0  \n",
       "2        0        0        0        0        0  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_punct_df = pd.DataFrame(list(test_df.punct_count))\n",
    "train_punct_df = pd.DataFrame(list(train_df.punct_count))\n",
    "train_punct_df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74b39b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>quoted_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>[ count</th>\n",
       "      <th>\\ count</th>\n",
       "      <th>] count</th>\n",
       "      <th>^ count</th>\n",
       "      <th>_ count</th>\n",
       "      <th>` count</th>\n",
       "      <th>{ count</th>\n",
       "      <th>| count</th>\n",
       "      <th>} count</th>\n",
       "      <th>~ count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6109</th>\n",
       "      <td>6110</td>\n",
       "      <td>567 positive cases and 39 deaths from COVID-19...</td>\n",
       "      <td>fake</td>\n",
       "      <td>86</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>2531</td>\n",
       "      <td>The latest CDC #COVIDView report shows that th...</td>\n",
       "      <td>real</td>\n",
       "      <td>277</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>1115</td>\n",
       "      <td>Text in Lithuanian excerpted from a letter cla...</td>\n",
       "      <td>fake</td>\n",
       "      <td>102</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label  \\\n",
       "6109  6110  567 positive cases and 39 deaths from COVID-19...  fake   \n",
       "2530  2531  The latest CDC #COVIDView report shows that th...  real   \n",
       "1114  1115  Text in Lithuanian excerpted from a letter cla...  fake   \n",
       "\n",
       "      char_count  word_count  sent_count  capital_char_count  \\\n",
       "6109          86          16           1                   7   \n",
       "2530         277          34           2                  27   \n",
       "1114         102          17           1                   5   \n",
       "\n",
       "      capital_word_count  quoted_word_count  stopword_count  ...  [ count  \\\n",
       "6109                   1                  0               5  ...        0   \n",
       "2530                   2                  0              11  ...        0   \n",
       "1114                   1                  0               7  ...        0   \n",
       "\n",
       "      \\ count  ] count ^ count  _ count  ` count  { count  | count  } count  \\\n",
       "6109        0        0       0        0        0        0        0        0   \n",
       "2530        0        0       0        0        0        0        0        0   \n",
       "1114        0        0       0        0        0        0        0        0   \n",
       "\n",
       "      ~ count  \n",
       "6109        0  \n",
       "2530        0  \n",
       "1114        0  \n",
       "\n",
       "[3 rows x 50 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging pnctuation DataFrame with main DataFrame\n",
    "train_df = pd.merge(train_df, train_punct_df, left_index=True, right_index=True)\n",
    "test_df = pd.merge(test_df, test_punct_df, left_index=True, right_index=True)\n",
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48aa1c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet', 'label', 'char_count', 'word_count', 'sent_count',\n",
       "       'capital_char_count', 'capital_word_count', 'quoted_word_count',\n",
       "       'stopword_count', 'unique_word_count', 'htag_count', 'mention_count',\n",
       "       'avg_wordlength', 'avg_sentlength', 'unique_vs_words',\n",
       "       'stopwords_vs_words', '! count', '\" count', '# count', '$ count',\n",
       "       '% count', '& count', '' count', '( count', ') count', '* count',\n",
       "       '+ count', ', count', '- count', '. count', '/ count', ': count',\n",
       "       '; count', '< count', '= count', '> count', '? count', '@ count',\n",
       "       '[ count', '\\ count', '] count', '^ count', '_ count', '` count',\n",
       "       '{ count', '| count', '} count', '~ count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can drop \"punct_count\" column from both df and test DataFrame\n",
    "train_df.drop(columns=['punct_count'], inplace=True)\n",
    "test_df.drop(columns=['punct_count'], inplace=True)\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39493d7b",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Re-processing</font>\n",
    "We performed a simple pre-processing step, like removing links, removing user name, numbers, double space, punctuation, lower casing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4383344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'httpS+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RTs@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\]^_`{|}~•@'\n",
    "    \n",
    "def preprocess(sent):\n",
    "    sent = remove_users(sent)\n",
    "    sent = remove_links(sent)\n",
    "    sent = sent.lower() # lower case\n",
    "    sent = re.sub('['+my_punctuation + ']+', ' ', sent) # strip punctuation\n",
    "    sent = re.sub('s+', ' ', sent) #remove double spacing\n",
    "    sent = re.sub('([0-9]+)', '', sent) # remove numbers\n",
    "    sent_token_list = [word for word in sent.split(' ')]\n",
    "    sent = ' '.join(sent_token_list)\n",
    "    return sent\n",
    "\n",
    "train_df['tweet']   = train_df['tweet'].apply(lambda x: preprocess(x))\n",
    "test_df['tweet'] = test_df['tweet'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d444641f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'tweet', 'label', 'char_count', 'word_count', 'sent_count',\n",
       "       'capital_char_count', 'capital_word_count', 'quoted_word_count',\n",
       "       'stopword_count', 'unique_word_count', 'htag_count', 'mention_count',\n",
       "       'avg_wordlength', 'avg_sentlength', 'unique_vs_words',\n",
       "       'stopwords_vs_words', '! count', '\" count', '# count', '$ count',\n",
       "       '% count', '& count', '' count', '( count', ') count', '* count',\n",
       "       '+ count', ', count', '- count', '. count', '/ count', ': count',\n",
       "       '; count', '< count', '= count', '> count', '? count', '@ count',\n",
       "       '[ count', '\\ count', '] count', '^ count', '_ count', '` count',\n",
       "       '{ count', '| count', '} count', '~ count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d173f34",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Encoding text</font>\n",
    "We will encode our text data using [**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). We first fit transform on our train and test set’s tweet column and then merge it with all features columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d8808a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is a a sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another another example example example</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text\n",
       "0                               this is a a sample\n",
       "1  this is another another example example example"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF example\n",
    "sample_doc_df = pd.DataFrame([\n",
    "    ['this is a a sample'],\n",
    "    ['this is another another example example example'],\n",
    "], columns=['text'])\n",
    "sample_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39dee6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>example</th>\n",
       "      <th>is</th>\n",
       "      <th>sample</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501549</td>\n",
       "      <td>0.704909</td>\n",
       "      <td>0.501549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.534284</td>\n",
       "      <td>0.801426</td>\n",
       "      <td>0.190074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    another   example        is    sample      this\n",
       "0  0.000000  0.000000  0.501549  0.704909  0.501549\n",
       "1  0.534284  0.801426  0.190074  0.000000  0.190074"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "vectorizer =  TfidfVectorizer()\n",
    "vectorizer.fit(sample_doc_df['text'])\n",
    "tfidf_df = pd.DataFrame(\n",
    "    vectorizer.transform(sample_doc_df['text']).toarray(),\n",
    "    columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d1035074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 s, sys: 1.18 s, total: 4.31 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer            =  TfidfVectorizer()\n",
    "train_tf_idf_features =  vectorizer.fit_transform(train_df['tweet']).toarray()\n",
    "test_tf_idf_features  =  vectorizer.transform(test_df['tweet']).toarray()\n",
    "\n",
    "# Converting above list to DataFrame\n",
    "train_tf_idf_df          = pd.DataFrame(train_tf_idf_features)\n",
    "test_tf_idf_df           = pd.DataFrame(test_tf_idf_features)\n",
    "\n",
    "# Saparating train and test labels from all features\n",
    "y_train               = train_df['label']\n",
    "y_test                = test_df['label']\n",
    "\n",
    "#Listing all features\n",
    "features = ['char_count', 'word_count', 'sent_count',\n",
    "       'capital_char_count', 'capital_word_count', 'quoted_word_count',\n",
    "       'stopword_count', 'unique_word_count', 'htag_count', 'mention_count',\n",
    "       'avg_wordlength', 'avg_sentlength', 'unique_vs_words',\n",
    "       'stopwords_vs_words', '! count', '\" count', '# count', '$ count',\n",
    "       '% count', '& count', '\\' count', '( count', ') count', '* count',\n",
    "       '+ count', ', count', '- count', '. count', '/ count', ': count',\n",
    "       '; count', '< count', '= count', '> count', '? count', '@ count',\n",
    "       '[ count', '\\ count', '] count', '^ count', '_ count', '` count',\n",
    "       '{ count', '| count', '} count', '~ count']\n",
    "\n",
    "# Finally merging all features with above TF-IDF. \n",
    "X_train = pd.merge(train_tf_idf_df, train_df[features], left_index=True, right_index=True)\n",
    "X_test  = pd.merge(test_tf_idf_df, test_df[features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "910aa79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>[ count</th>\n",
       "      <th>\\ count</th>\n",
       "      <th>] count</th>\n",
       "      <th>^ count</th>\n",
       "      <th>_ count</th>\n",
       "      <th>` count</th>\n",
       "      <th>{ count</th>\n",
       "      <th>| count</th>\n",
       "      <th>} count</th>\n",
       "      <th>~ count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 13918 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  [ count  \\ count  \\\n",
       "1941  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "5635  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "3880  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "\n",
       "      ] count  ^ count  _ count  ` count  { count  | count  } count  ~ count  \n",
       "1941        0        0        0        0        0        0        0        0  \n",
       "5635        0        0        0        0        0        0        0        0  \n",
       "3880        0        0        0        0        0        0        0        0  \n",
       "\n",
       "[3 rows x 13918 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6896c25",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Training</font>\n",
    "For training, we will be using the Random forest algorithm ([**RandomForestClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)) from the sci-kit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eb41dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 4s, sys: 653 ms, total: 2min 5s\n",
      "Wall time: 2min 5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(min_samples_split=15, n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(min_samples_split=15, n_estimators=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(min_samples_split=15, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_model = RandomForestClassifier(\n",
    "    n_estimators = 1000, min_samples_split = 15, random_state = RANDOM_STATE)\n",
    "clf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "610090c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b7472cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'real', 'fake', 'fake', 'real', 'real', 'real', 'real',\n",
       "       'fake', 'fake'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5492abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.6%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.97      0.89      0.93      1020\n",
      "        real       0.91      0.97      0.94      1120\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.93      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_test, y_pred):.01%}')\n",
    "print()\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713f83c",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>Result comparison</font> ([back](#sect0))\n",
    "* <font size='3ptx'><b><a href='#sect4_1'>Without using Feature Engineering techniques</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_2'>Using Feature Engineering techniques</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_3'>Replace tf-idf with word-vector</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_4'>SVC + word2vec + feature engineering</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_5'>SVC (Grid search) + word2vec + feature engineering</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_6'>SVC + tf-idf + feature engineering</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_7'>SVC (Grid search) + tf-idf + feature engineering</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_8'>SVC + tf-idf + feature engineering + 2gram</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_9'>SVC + word2vec + PCA(tf-idf & feature engineering,100)</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4_10'>SVC + word2vec + PCA(tf-idf & feature engineering,100) + 2gram</a></b></font>\n",
    "<br/>\n",
    "\n",
    "<font size='3ptx'><b>For comparison, we first trained our model on the above dataset by using features engineering techniques and then without using feature engineering techniques</b></font>.\n",
    "\n",
    "In both approaches, we pre-processed the dataset using the same method as described above and TF-IDF was used in both approaches for encoding the text data. You can use whatever encoding techniques you want to use like word2vec, glove, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7b7b8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bb58a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_perf_data(y_pred, name, feature_size, train_time):\n",
    "    y_test_binary = [1 if label=='fake' else 0 for label in y_test]\n",
    "    y_pred_binary = [1 if label=='fake' else 0 for label in y_pred]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    perf_data.append((\n",
    "        name,\n",
    "        feature_size,\n",
    "        accuracy,\n",
    "        recall_score(y_test_binary, y_pred_binary),\n",
    "        precision_score(y_test_binary, y_pred_binary),\n",
    "        f1_score(y_test_binary, y_pred_binary),\n",
    "        train_time))\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.01%}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return perf_data[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12474b58",
   "metadata": {},
   "source": [
    "<a id='sect4_1'></a>\n",
    "### <font color='darkgreen'>Without using Feature Engineering techniques</font> ([back](#sect4))\n",
    "Here we only use features tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e1c26e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 0 ns, total: 3min 3s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = RandomForestClassifier(\n",
    "    n_estimators = RF_N_ESTIMATORS, min_samples_split = RF_MIN_SAMPLES_SPLIT, random_state = RANDOM_STATE)\n",
    "clf_model.fit(train_tf_idf_features, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e3ca1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(test_tf_idf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "680ca4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.3%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.87      0.91      1020\n",
      "        real       0.89      0.95      0.92      1120\n",
      "\n",
      "    accuracy                           0.91      2140\n",
      "   macro avg       0.92      0.91      0.91      2140\n",
      "weighted avg       0.91      0.91      0.91      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('randomforest + tf-idf',\n",
       " 13872,\n",
       " 0.9130841121495327,\n",
       " 0.8745098039215686,\n",
       " 0.9389473684210526,\n",
       " 0.9055837563451777,\n",
       " datetime.timedelta(seconds=183, microseconds=325149))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'randomforest + tf-idf', train_tf_idf_features.shape[1],\n",
    "    train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('randomforest + tf-idf',\n",
    "     13872,\n",
    "     0.9121495327102803,\n",
    "     0.8745098039215686,\n",
    "     0.9369747899159664,\n",
    "     0.9046653144016227,\n",
    "     datetime.timedelta(seconds=152)))\n",
    "'''\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9726b",
   "metadata": {},
   "source": [
    "<a id='sect4_2'></a>\n",
    "### <font color='darkgreen'>Using Feature Engineering techniques</font> ([back](#sect4))\n",
    "Here we used up all features we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "67f300b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 0 ns, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = RandomForestClassifier(\n",
    "    n_estimators = RF_N_ESTIMATORS, min_samples_split = RF_MIN_SAMPLES_SPLIT, random_state = RANDOM_STATE)\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6cd1a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "470afe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.2%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.96      0.89      0.93      1020\n",
      "        real       0.91      0.97      0.94      1120\n",
      "\n",
      "    accuracy                           0.93      2140\n",
      "   macro avg       0.94      0.93      0.93      2140\n",
      "weighted avg       0.93      0.93      0.93      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('randomforest + tf-idf + feature engineering',\n",
       " 13918,\n",
       " 0.9317757009345794,\n",
       " 0.8892156862745098,\n",
       " 0.9648936170212766,\n",
       " 0.9255102040816325,\n",
       " datetime.timedelta(seconds=62, microseconds=310574))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'randomforest + tf-idf + feature engineering',\n",
    "    X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d36ade8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nperf_data.append(\\n    ('randomforest + tf-idf + feature engineering',\\n     13918,\\n     0.9336448598130841,\\n     0.8911764705882353,\\n     0.9670212765957447,\\n     0.9275510204081633,\\n     timedelta(minutes=1, seconds=3)))\\n\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('randomforest + tf-idf + feature engineering',\n",
    "     13918,\n",
    "     0.9336448598130841,\n",
    "     0.8911764705882353,\n",
    "     0.9670212765957447,\n",
    "     0.9275510204081633,\n",
    "     timedelta(minutes=1, seconds=3)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbdc80",
   "metadata": {},
   "source": [
    "<a id='sect4_3'></a>\n",
    "### <font color='darkgreen'>Replace tf-idf with word-vector</font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29774108",
   "metadata": {},
   "source": [
    "Here we use [**word2vec**](https://en.wikipedia.org/wiki/Word2vec) to replace tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a1e9d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gensim\n",
    "test_tf_idf_features = train_tf_idf_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "daea3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5ddda68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_feature_names = [f'w{i}' for i in range(300)]\n",
    "\n",
    "def text2w2v(df, column='tweet', only_w2v=False):\n",
    "    \"\"\"Turns text into word2vect\"\"\"\n",
    "    model = api.load(\"word2vec-google-news-300\") \n",
    "    wv_list = []\n",
    "    for ri, row in df.iterrows():\n",
    "        vector = np.zeros(300)\n",
    "        for w in row.get(column).split():\n",
    "            try:\n",
    "                vector += model[w]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "        wv_list.append(vector)\n",
    "        \n",
    "    w2v_df = pd.DataFrame(wv_list, columns=w2v_feature_names)\n",
    "    if only_w2v:\n",
    "        return w2v_df\n",
    "    \n",
    "    df = pd.concat([df, w2v_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3cbf1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 s, sys: 5.93 s, total: 57.8 s\n",
      "Wall time: 58.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = text2w2v(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4478f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.7 s, sys: 2.85 s, total: 52.6 s\n",
      "Wall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = text2w2v(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "de464c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 949)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>quoted_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>...</th>\n",
       "      <th>w290</th>\n",
       "      <th>w291</th>\n",
       "      <th>w292</th>\n",
       "      <th>w293</th>\n",
       "      <th>w294</th>\n",
       "      <th>w295</th>\n",
       "      <th>w296</th>\n",
       "      <th>w297</th>\n",
       "      <th>w298</th>\n",
       "      <th>w299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>5646</td>\n",
       "      <td>our total number of confirmed ca e  of covid  ...</td>\n",
       "      <td>real</td>\n",
       "      <td>238</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.942078</td>\n",
       "      <td>1.282257</td>\n",
       "      <td>-2.360809</td>\n",
       "      <td>-0.252247</td>\n",
       "      <td>0.365173</td>\n",
       "      <td>-0.709702</td>\n",
       "      <td>-0.553467</td>\n",
       "      <td>-1.146214</td>\n",
       "      <td>-0.391052</td>\n",
       "      <td>-0.863144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>4361</td>\n",
       "      <td>bloke who e mi o ha  to  top him from a puttin...</td>\n",
       "      <td>fake</td>\n",
       "      <td>150</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039013</td>\n",
       "      <td>1.947937</td>\n",
       "      <td>-1.239754</td>\n",
       "      <td>1.653015</td>\n",
       "      <td>-0.979614</td>\n",
       "      <td>-1.617981</td>\n",
       "      <td>-0.126404</td>\n",
       "      <td>-0.890869</td>\n",
       "      <td>-1.087830</td>\n",
       "      <td>1.596802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6185</th>\n",
       "      <td>6186</td>\n",
       "      <td>the end of the world i  nigh http  t co xilglu...</td>\n",
       "      <td>fake</td>\n",
       "      <td>102</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005615</td>\n",
       "      <td>0.537781</td>\n",
       "      <td>-0.464722</td>\n",
       "      <td>1.222900</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>-0.854614</td>\n",
       "      <td>-0.741211</td>\n",
       "      <td>-0.064758</td>\n",
       "      <td>-0.262695</td>\n",
       "      <td>0.596497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 949 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet label  \\\n",
       "5645  5646  our total number of confirmed ca e  of covid  ...  real   \n",
       "4360  4361  bloke who e mi o ha  to  top him from a puttin...  fake   \n",
       "6185  6186  the end of the world i  nigh http  t co xilglu...  fake   \n",
       "\n",
       "      char_count  word_count  sent_count  capital_char_count  \\\n",
       "5645         238          40           1                  10   \n",
       "4360         150          22           1                  33   \n",
       "6185         102          13           1                  11   \n",
       "\n",
       "      capital_word_count  quoted_word_count  stopword_count  ...      w290  \\\n",
       "5645                   1                  0              15  ... -0.942078   \n",
       "4360                   3                  0               0  ...  0.039013   \n",
       "6185                   0                  0               0  ... -0.005615   \n",
       "\n",
       "          w291      w292      w293      w294      w295      w296      w297  \\\n",
       "5645  1.282257 -2.360809 -0.252247  0.365173 -0.709702 -0.553467 -1.146214   \n",
       "4360  1.947937 -1.239754  1.653015 -0.979614 -1.617981 -0.126404 -0.890869   \n",
       "6185  0.537781 -0.464722  1.222900 -0.014404 -0.854614 -0.741211 -0.064758   \n",
       "\n",
       "          w298      w299  \n",
       "5645 -0.391052 -0.863144  \n",
       "4360 -1.087830  1.596802  \n",
       "6185 -0.262695  0.596497  \n",
       "\n",
       "[3 rows x 949 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "97c217ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[features + w2v_feature_names]\n",
    "X_test  = test_df[features + w2v_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9c64ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 33s, sys: 0 ns, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = RandomForestClassifier(\n",
    "    n_estimators = RF_N_ESTIMATORS, min_samples_split = RF_MIN_SAMPLES_SPLIT,\n",
    "    random_state = RANDOM_STATE)\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dcc4e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "48172e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.0%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.86      0.88      1020\n",
      "        real       0.88      0.92      0.90      1120\n",
      "\n",
      "    accuracy                           0.89      2140\n",
      "   macro avg       0.89      0.89      0.89      2140\n",
      "weighted avg       0.89      0.89      0.89      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('randomforest + word2vec + feature engineering',\n",
       " 946,\n",
       " 0.8901869158878505,\n",
       " 0.8607843137254902,\n",
       " 0.9042224510813595,\n",
       " 0.8819688598694123,\n",
       " datetime.timedelta(seconds=93, microseconds=162035))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'randomforest + word2vec + feature engineering',\n",
    "    X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61e90e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('randomforest + word2vec + feature engineering',\n",
    "     346,\n",
    "     0.908411214953271,\n",
    "     0.8725490196078431,\n",
    "     0.9309623430962343,\n",
    "     0.9008097165991903)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f80b47",
   "metadata": {},
   "source": [
    "<a id='sect4_4'></a>\n",
    "### <font color='darkgreen'>SVC + word2vec + feature engineering</font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "42716ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 0 ns, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = make_pipeline(StandardScaler(), SVC())\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c3100720",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4f9528cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.92      0.92      0.92      1020\n",
      "        real       0.93      0.93      0.93      1120\n",
      "\n",
      "    accuracy                           0.92      2140\n",
      "   macro avg       0.92      0.92      0.92      2140\n",
      "weighted avg       0.92      0.92      0.92      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SVC + word2vec + feature engineering',\n",
       " 946,\n",
       " 0.9247663551401869,\n",
       " 0.9205882352941176,\n",
       " 0.9214916584887144,\n",
       " 0.9210397253555664,\n",
       " datetime.timedelta(seconds=13, microseconds=514017))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'SVC + word2vec + feature engineering',\n",
    "    X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea693275",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('SVC + word2vec + feature engineering',\n",
    "     346,\n",
    "     0.9294392523364486,\n",
    "     0.9215686274509803,\n",
    "     0.9297725024727992,\n",
    "     0.9256523879862137)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe37b7",
   "metadata": {},
   "source": [
    "<a id='sect4_5'></a>\n",
    "### <font color='darkgreen'>SVC(Grid search) + word2vec + feature engineering</font> ([back](#sect4))\n",
    "Please refer to [\"Searching for optimal parameters with successive halving\"](https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9200c5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "cb3fe9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "CPU times: user 13.9 s, sys: 1.02 s, total: 15 s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01 ],'kernel': ['rbf', 'poly']}\n",
    "\n",
    "st = datetime.now()\n",
    "grid = GridSearchCV(\n",
    "    SVC(), param_grid, refit=True, verbose=2, n_jobs=-1)\n",
    "X_train_scaled = std_scaler.transform(X_train)\n",
    "grid.fit(X_train_scaled, y_train)                      \n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "da5f445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = std_scaler.transform(X_test)\n",
    "y_pred = grid.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fa6c9e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.0%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.92      0.91      0.92      1020\n",
      "        real       0.92      0.93      0.92      1120\n",
      "\n",
      "    accuracy                           0.92      2140\n",
      "   macro avg       0.92      0.92      0.92      2140\n",
      "weighted avg       0.92      0.92      0.92      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Grid search SVC + word2vec + feature engineering',\n",
       " 946,\n",
       " 0.9200934579439253,\n",
       " 0.907843137254902,\n",
       " 0.9232303090727817,\n",
       " 0.9154720711814137,\n",
       " datetime.timedelta(seconds=277, microseconds=684995))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'Grid search SVC + word2vec + feature engineering',\n",
    "    X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25896d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('Grid search SVC + word2vec + feature engineering',\n",
    "     346,\n",
    "     0.9205607476635514,\n",
    "     0.9362745098039216,\n",
    "     0.9009433962264151,\n",
    "     0.9182692307692307)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c89629",
   "metadata": {},
   "source": [
    "<a id='sect4_6'></a>\n",
    "### <font color='darkgreen'>SVC + tf-idf + feature engineering</font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "90647bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Feature engineering\n",
    "X_train = pd.merge(pd.DataFrame(vectorizer.transform(train_df['tweet']).toarray()),\n",
    "                   train_df[features], left_index=True, right_index=True)\n",
    "X_test  = pd.merge(pd.DataFrame(vectorizer.transform(test_df['tweet']).toarray()),\n",
    "                   test_df[features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "9e17fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 13918)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>[ count</th>\n",
       "      <th>\\ count</th>\n",
       "      <th>] count</th>\n",
       "      <th>^ count</th>\n",
       "      <th>_ count</th>\n",
       "      <th>` count</th>\n",
       "      <th>{ count</th>\n",
       "      <th>| count</th>\n",
       "      <th>} count</th>\n",
       "      <th>~ count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 13918 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  [ count  \\ count  \\\n",
       "185   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "2282  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "4941  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...        0        0   \n",
       "\n",
       "      ] count  ^ count  _ count  ` count  { count  | count  } count  ~ count  \n",
       "185         0        0        0        0        0        0        0        0  \n",
       "2282        0        0        0        0        0        0        0        0  \n",
       "4941        0        0        0        0        0        0        0        0  \n",
       "\n",
       "[3 rows x 13918 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dc2bfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = X_test.columns = [f'f-{i}' for i in range(X_train.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "87dec530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  25.7s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  32.6s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.2min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  27.2s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=  24.8s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  22.4s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  26.2s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  29.3s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=  27.3s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  22.7s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=  31.7s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  22.2s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  24.6s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  22.1s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  59.2s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  27.3s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  55.4s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  26.2s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  27.6s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=  25.8s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  24.8s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  58.9s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=  24.9s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  24.1s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  26.9s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  25.6s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  56.1s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  24.5s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  55.9s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=  57.7s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  27.2s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  25.6s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=  26.1s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  23.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  30.9s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=  25.9s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=  24.2s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.0min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=  23.8s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  25.2s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  25.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  51.1s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time= 1.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time= 1.1min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  46.9s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  26.1s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  24.2s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  55.7s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  25.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  59.7s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=  23.4s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  46.6s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  27.1s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=  24.7s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  56.5s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=  23.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  58.7s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=  27.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  43.6s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  28.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=  57.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=  26.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=  55.2s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=  25.4s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=  26.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=  43.3s\n",
      "CPU times: user 47min 44s, sys: 1h 4min 52s, total: 1h 52min 37s\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = make_pipeline(StandardScaler(), SVC())\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f7fa3bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54min 30s, sys: 1h 35min 55s, total: 2h 30min 26s\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7c385fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.6%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.91      0.92      1020\n",
      "        real       0.92      0.94      0.93      1120\n",
      "\n",
      "    accuracy                           0.93      2140\n",
      "   macro avg       0.93      0.93      0.93      2140\n",
      "weighted avg       0.93      0.93      0.93      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SVC + tf-idf + feature engineering',\n",
       " 13918,\n",
       " 0.9257009345794392,\n",
       " 0.9137254901960784,\n",
       " 0.9292123629112662,\n",
       " 0.921403855659911,\n",
       " datetime.timedelta(seconds=434, microseconds=36797))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, 'SVC + tf-idf + feature engineering',\n",
    "    X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "996255a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('SVC + tf-idf + feature engineering',\n",
    "     13918,\n",
    "     0.9257009345794392,\n",
    "     0.9137254901960784,\n",
    "     0.9292123629112662,\n",
    "     0.921403855659911)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28f667",
   "metadata": {},
   "source": [
    "<a id='sect4_7'></a>\n",
    "### <font color='darkgreen'>SVC(Grid search) + tf-idf + feature engineering</font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "16919afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=14.6min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 8.3min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=10.4min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=11.0min\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time= 7.0min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 8.9min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 6.3min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=13.0min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=10.8min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 7.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=11.6min\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time= 7.0min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 7.0min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time= 7.9min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=11.2min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=10.4min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 7.3min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=12.2min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 7.3min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 6.3min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time= 7.8min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=14.4min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 9.5min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=11.1min\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time= 5.6min\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time= 6.7min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 8.6min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 7.1min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 8.6min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=14.1min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 7.4min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=12.3min\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time= 8.4min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 5.4min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 7.5min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time= 6.3min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=12.9min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 5.0min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 6.4min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=12.7min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 7.6min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 4.9min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 6.6min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time= 5.8min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=14.5min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 5.4min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 7.2min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=12.2min\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time= 7.6min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 8.5min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 7.4min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 7.1min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=14.7min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 7.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=11.8min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 9.4min\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time= 8.0min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 6.8min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time= 6.4min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=13.2min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 7.3min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=11.8min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 9.7min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 7.5min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 6.3min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time= 5.7min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=14.7min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 8.0min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 8.4min\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=11.9min\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time= 6.4min\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time= 9.4min\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time= 8.3min\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=13.4min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 5.3min\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time= 6.9min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=11.6min\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time= 5.3min\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time= 7.1min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 6.6min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time= 5.1min\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time= 7.7min\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=12.7min\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time= 6.5min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=11.8min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 5.3min\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time= 7.0min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 6.0min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time= 5.3min\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time= 6.3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01],\n",
       "                         'kernel': ['rbf', 'poly']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [1, 0.1, 0.01],'kernel': ['rbf', 'poly']}\n",
    "st = datetime.now()\n",
    "grid = GridSearchCV(\n",
    "    SVC(), param_grid, refit=True, verbose=2, n_jobs=-1)\n",
    "\n",
    "grid.fit(X_train, y_train) \n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9325ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30fb31a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.3%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.95      0.94      1020\n",
      "        real       0.96      0.93      0.94      1120\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.94      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Gridsearch SVC + tf-idf + feature engineering',\n",
       " 13918,\n",
       " 0.9425233644859813,\n",
       " 0.9519607843137254,\n",
       " 0.9291866028708134,\n",
       " 0.9404358353510895)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(y_pred, 'Gridsearch SVC + tf-idf + feature engineering',\n",
    "                 X_train.shape[1], train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "614a4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "perf_data.append(\n",
    "    ('Gridsearch SVC + tf-idf + feature engineering',\n",
    "     13918,\n",
    "     0.9425233644859813,\n",
    "     0.9519607843137254,\n",
    "     0.9291866028708134,\n",
    "     0.9404358353510895,\n",
    "     timedelta())\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24dbbc",
   "metadata": {},
   "source": [
    "<a id='sect4_8'></a>\n",
    "### <font color='darkgreen'>SVC + tf-idf + feature engineering + 2gram</font> ([back](#sect4))\n",
    "Here we add 100 more features of 2 grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7a272730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoGramFeatures:\n",
    "    \"\"\"Class to collect 2gram features.\n",
    "    \n",
    "    Attributes:\n",
    "        top_n: Number of 2gram to collect.\n",
    "        min_doc_count: The limitation of minimum doc which 2gram should exist\n",
    "            in order to be collected.\n",
    "        text_column_name: The column name of dataframe as text to search for 2gram.\n",
    "        label_column_name: The column name of dataframe as labeling result.\n",
    "        fit_history: dict object with key as collected 2gram and value as its meta data\n",
    "            such as score and doc count.\n",
    "        fit_gram_list: The collrected 2gram list after fit.\n",
    "    \"\"\"\n",
    "    HEAD_TOKEN_PLACEHOLDER = '__HEAD_TOKEN__'\n",
    "\n",
    "    def __init__(self, text_column_name, label_column_name, top_n=100, min_doc_count=10):\n",
    "        self.top_n = top_n\n",
    "        self.min_doc_count = min_doc_count\n",
    "        self.text_column_name = text_column_name\n",
    "        self.label_column_name = label_column_name\n",
    "        self.fit_history = {}\n",
    "        self.fit_gram_list = ()\n",
    "        self._tokenizer_words = TweetTokenizer()\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'TwoGramFeatures(min_doc_count={self.min_doc_count}, top_n={self.top_n}): {len(self.fit_history)} 2gram(s)'\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fit_gram_list)\n",
    "        \n",
    "    def _reverse_entropy(self, prob_list):\n",
    "        \"\"\"Calculate the entropy of given probability list and take the reciprocal of it.\n",
    "        \n",
    "        For how to calculate entropy, please refer to:\n",
    "        https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "        \n",
    "        Below are few examples for reference:\n",
    "        ```\n",
    "        (0, 1) with reverse-entropy as 9223372036854775807\n",
    "        (1, 0) with reverse-entropy as 9223372036854775807\n",
    "        (0.99, 0.01) with reverse-entropy as 17.85665359923201\n",
    "        (0.1, 0.9) with reverse-entropy as 3.0761377305228823\n",
    "        (0.3, 0.7) with reverse-entropy as 1.6370247805217761\n",
    "        (0.5, 0.5) with reverse-entropy as 1.4426950408889634\n",
    "        (0.3, 0.3, 0.3, 0.1) with reverse-entropy as 0.7611311434594984\n",
    "        ```\n",
    "        \n",
    "        Args:\n",
    "            prob_list: Probability list with sum up to 1.\n",
    "            \n",
    "        Returns:\n",
    "            The reciprocal of entropy.\n",
    "        \"\"\"\n",
    "        entropy_val = 0\n",
    "        for prob in prob_list:\n",
    "            if prob == 0:\n",
    "                continue\n",
    "            if prob == 1:\n",
    "                break\n",
    "\n",
    "            entropy_val += -1 * math.log(prob) * prob\n",
    "\n",
    "        return 1/entropy_val if entropy_val else sys.maxsize\n",
    "        \n",
    "    def _gen_2grams(self, text_content):\n",
    "        \"\"\"Generates the 2gram counter.\"\"\"\n",
    "        token_counter = Counter()        \n",
    "        for token_list in [self._tokenizer_words.tokenize(t) for t in nltk.sent_tokenize(text_content)]:\n",
    "            first_token = token_list[0]\n",
    "            for token in token_list[1:]:\n",
    "                token_counter.update({(first_token, token): 1})\n",
    "                token = first_token\n",
    "            \n",
    "        return token_counter\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fits the dataframe to collect 2gram information.\"\"\"\n",
    "        self.fit_history = {}\n",
    "        self.fit_gram_list = ()\n",
    "        doc_count = df.shape[0]\n",
    "        token_doc_counter = Counter()\n",
    "        token_label_dict = {}\n",
    "        for ri, row in df.iterrows():\n",
    "            label = row.get(self.label_column_name)\n",
    "            text_content = row.get(self.text_column_name)\n",
    "            gram_counter = self._gen_2grams(text_content)\n",
    "            for gram in gram_counter.keys():\n",
    "                token_doc_counter.update({gram: 1})\n",
    "                if gram in token_label_dict:\n",
    "                    token_label_dict[gram][label] += 1\n",
    "                else:\n",
    "                    token_label_dict[gram] = defaultdict(int)\n",
    "                    token_label_dict[gram][label] += 1\n",
    "        \n",
    "        # Start calculating score and sorting the result\n",
    "        toke_score_dict = {}\n",
    "        for gram, doc_count in token_doc_counter.items():\n",
    "            gram_label_count_list = token_label_dict[gram].values()\n",
    "            selected_label = max(token_label_dict[gram].items(), key=lambda t: t[1])[0]\n",
    "            gram_label_count_sum = sum(gram_label_count_list)\n",
    "            prob_list = [count/gram_label_count_sum for count in gram_label_count_list]\n",
    "            score = self._reverse_entropy(prob_list)\n",
    "            toke_score_dict[gram] = (score, token_doc_counter[gram], selected_label)\n",
    "            \n",
    "        self.fit_history = sorted(filter(\n",
    "            lambda t: t[1][1] >= self.min_doc_count,\n",
    "            toke_score_dict.items()), key=lambda t: t[1], reverse=True)[:self.top_n]\n",
    "        self.fit_gram_list = list(map(lambda t: t[0], self.fit_history))\n",
    "        \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transforms the input dataframe into 2gram feature array.\"\"\"\n",
    "        datas = []\n",
    "        for ri, row in df.iterrows():\n",
    "            data = []\n",
    "            gram_counter = self._gen_2grams(row.get(self.text_column_name))\n",
    "            for gram in self.fit_gram_list:\n",
    "                data.append(gram_counter.get(gram, 0))\n",
    "                \n",
    "            datas.append(data)\n",
    "            \n",
    "        return np.array(datas)\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fits and transforms the input dataframe into 2gram feature array.\"\"\"\n",
    "        if not self.fit_history:\n",
    "            self.fit(df)\n",
    "            \n",
    "        return self.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8b3b8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.59 s, sys: 279 ms, total: 8.87 s\n",
      "Wall time: 8.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "two_gram_feats       =  TwoGramFeatures(text_column_name='tweet', label_column_name='label', top_n=150, min_doc_count=70)\n",
    "train_2gram_features =  two_gram_feats.fit_transform(train_df)\n",
    "test_2gram_features  =  two_gram_feats.transform(test_df)\n",
    "\n",
    "# Converting above list to DataFrame\n",
    "train_2gram_df          = pd.DataFrame(train_2gram_features,\n",
    "                                       columns=[f'2gram_{i}' for i in range(train_2gram_features.shape[1])])\n",
    "test_2gram_df           = pd.DataFrame(test_2gram_features,\n",
    "                                       columns=[f'2gram_{i}' for i in range(test_2gram_features.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0d29c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('the', 'in'), (1.5409374900486654, 244, 'real')),\n",
       " (('the', 'and'), (1.5321818892033032, 184, 'real')),\n",
       " (('“', '”'), (1.5211716964420314, 79, 'real')),\n",
       " (('new', 'http'), (1.5115712940524648, 216, 'real')),\n",
       " (('new', 'co'), (1.5115712940524648, 216, 'real')),\n",
       " (('new', 'in'), (1.5115712940524648, 80, 'real')),\n",
       " (('the', 'the'), (1.5069846308721047, 343, 'real')),\n",
       " (('new', 'a'), (1.5053970611059928, 92, 'real')),\n",
       " (('new', 't'), (1.4979812997754336, 222, 'real')),\n",
       " (('the', 'that'), (1.4840344036129642, 97, 'real')),\n",
       " (('the', 'ha'), (1.4829071420985505, 119, 'real')),\n",
       " (('the', 'a'), (1.4812687067121846, 227, 'real')),\n",
       " (('the', 'i'), (1.4704089646857075, 217, 'real')),\n",
       " (('a', 'india'), (1.463225517964234, 79, 'real')),\n",
       " (('a', 'of'), (1.4576721754261794, 302, 'fake')),\n",
       " (('a', 't'), (1.45434081537911, 266, 'real')),\n",
       " (('a', 'for'), (1.4470427872201101, 124, 'fake')),\n",
       " (('the', 'it'), (1.4461030778165422, 70, 'fake')),\n",
       " (('the', 'covid'), (1.4440431617274436, 139, 'real')),\n",
       " (('a', 'e'), (1.443119920716149, 198, 'fake'))]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(two_gram_feats)\n",
    "list(two_gram_feats.fit_history[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "71c55948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2gram_0</th>\n",
       "      <th>2gram_1</th>\n",
       "      <th>2gram_2</th>\n",
       "      <th>2gram_3</th>\n",
       "      <th>2gram_4</th>\n",
       "      <th>2gram_5</th>\n",
       "      <th>2gram_6</th>\n",
       "      <th>2gram_7</th>\n",
       "      <th>2gram_8</th>\n",
       "      <th>2gram_9</th>\n",
       "      <th>...</th>\n",
       "      <th>2gram_136</th>\n",
       "      <th>2gram_137</th>\n",
       "      <th>2gram_138</th>\n",
       "      <th>2gram_139</th>\n",
       "      <th>2gram_140</th>\n",
       "      <th>2gram_141</th>\n",
       "      <th>2gram_142</th>\n",
       "      <th>2gram_143</th>\n",
       "      <th>2gram_144</th>\n",
       "      <th>2gram_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6402</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2gram_0  2gram_1  2gram_2  2gram_3  2gram_4  2gram_5  2gram_6  2gram_7  \\\n",
       "1851        0        0        0        0        0        0        0        0   \n",
       "1115        0        0        0        0        0        0        0        0   \n",
       "6402        0        0        0        0        0        0        0        0   \n",
       "\n",
       "      2gram_8  2gram_9  ...  2gram_136  2gram_137  2gram_138  2gram_139  \\\n",
       "1851        0        0  ...          0          0          0          0   \n",
       "1115        0        0  ...          0          0          0          0   \n",
       "6402        0        0  ...          0          0          0          0   \n",
       "\n",
       "      2gram_140  2gram_141  2gram_142  2gram_143  2gram_144  2gram_145  \n",
       "1851          0          0          0          0          0          0  \n",
       "1115          0          0          0          0          0          0  \n",
       "6402          0          0          0          0          0          0  \n",
       "\n",
       "[3 rows x 146 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_2gram_df.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "aace61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally merging all features with above TF-IDF. \n",
    "X_train = pd.concat([train_df[features], train_2gram_df, train_tf_idf_df], axis=1)\n",
    "X_test  = pd.concat([test_df[features], test_2gram_df, test_tf_idf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3a433e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 14064)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>quoted_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>...</th>\n",
       "      <th>13862</th>\n",
       "      <th>13863</th>\n",
       "      <th>13864</th>\n",
       "      <th>13865</th>\n",
       "      <th>13866</th>\n",
       "      <th>13867</th>\n",
       "      <th>13868</th>\n",
       "      <th>13869</th>\n",
       "      <th>13870</th>\n",
       "      <th>13871</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>173</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>148</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>264</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  sent_count  capital_char_count  \\\n",
       "4401         173          29           3                   3   \n",
       "3585         148          27           5                   2   \n",
       "5966         264          39           3                   7   \n",
       "\n",
       "      capital_word_count  quoted_word_count  stopword_count  \\\n",
       "4401                   0                  0               9   \n",
       "3585                   0                  0              12   \n",
       "5966                   1                  0              13   \n",
       "\n",
       "      unique_word_count  htag_count  mention_count  ...  13862  13863  13864  \\\n",
       "4401                 28           0              0  ...    0.0    0.0    0.0   \n",
       "3585                 25           0              0  ...    0.0    0.0    0.0   \n",
       "5966                 36           0              0  ...    0.0    0.0    0.0   \n",
       "\n",
       "      13865  13866  13867  13868  13869  13870  13871  \n",
       "4401    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3585    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "5966    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 14064 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7821d835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 14064)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>capital_char_count</th>\n",
       "      <th>capital_word_count</th>\n",
       "      <th>quoted_word_count</th>\n",
       "      <th>stopword_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>htag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>...</th>\n",
       "      <th>13862</th>\n",
       "      <th>13863</th>\n",
       "      <th>13864</th>\n",
       "      <th>13865</th>\n",
       "      <th>13866</th>\n",
       "      <th>13867</th>\n",
       "      <th>13868</th>\n",
       "      <th>13869</th>\n",
       "      <th>13870</th>\n",
       "      <th>13871</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>242</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>300</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>114</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 14064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      char_count  word_count  sent_count  capital_char_count  \\\n",
       "1873         242          38           5                  27   \n",
       "623          300          48           2                  14   \n",
       "436          114          20           1                   8   \n",
       "\n",
       "      capital_word_count  quoted_word_count  stopword_count  \\\n",
       "1873                   2                  0              14   \n",
       "623                    0                  0              20   \n",
       "436                    1                  0               7   \n",
       "\n",
       "      unique_word_count  htag_count  mention_count  ...  13862  13863  13864  \\\n",
       "1873                 34           0              0  ...    0.0    0.0    0.0   \n",
       "623                  39           0              0  ...    0.0    0.0    0.0   \n",
       "436                  19           0              0  ...    0.0    0.0    0.0   \n",
       "\n",
       "      13865  13866  13867  13868  13869  13870  13871  \n",
       "1873    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "623     0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "436     0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[3 rows x 14064 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "X_test.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "963cc3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50min 1s, sys: 1h 5min 16s, total: 1h 55min 18s\n",
      "Wall time: 7min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = make_pipeline(StandardScaler(), SVC())\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2518b6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "959f7d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.5%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.93      0.93      0.93      1020\n",
      "        real       0.94      0.94      0.94      1120\n",
      "\n",
      "    accuracy                           0.93      2140\n",
      "   macro avg       0.93      0.93      0.93      2140\n",
      "weighted avg       0.93      0.93      0.93      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SVC + tf-idf + feature engineering + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)',\n",
       " 14064,\n",
       " 0.9345794392523364,\n",
       " 0.9284313725490196,\n",
       " 0.9339250493096647,\n",
       " 0.9311701081612587,\n",
       " datetime.timedelta(seconds=444, microseconds=486139))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, f'SVC + tf-idf + feature engineering + {two_gram_feats}',\n",
    "    X_train.shape[1],\n",
    "    train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a9fa76",
   "metadata": {},
   "source": [
    "<a id='sect4_9'></a>\n",
    "### <font color='darkgreen'>SVC + word2vec + PCA(tf-idf & feature engineering,100) </font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b059fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Feature engineering\n",
    "X_train = pd.merge(pd.DataFrame(vectorizer.transform(train_df['tweet']).toarray()),\n",
    "                   train_df[features], left_index=True, right_index=True)\n",
    "X_test  = pd.merge(pd.DataFrame(vectorizer.transform(test_df['tweet']).toarray()),\n",
    "                   test_df[features], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f3317e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=100)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_n_component = 100\n",
    "pca_columns = columns=[f'pca_{i}' for i in range(pca_n_component)]\n",
    "pca = PCA(n_components=pca_n_component)\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "b72ad004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare PCA(100) on tf-idf + feature engineering\n",
    "X_train_pca_df = pd.DataFrame(pca.transform(X_train),\n",
    "                              columns=pca_columns)\n",
    "X_test_pca_df = pd.DataFrame(pca.transform(X_test),\n",
    "                             columns=pca_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8c2e9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word2vec(300)\n",
    "X_train_w2v_df = text2w2v(train_df, only_w2v=True)\n",
    "X_test_w2v_df = text2w2v(test_df, only_w2v=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "c1980298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine PCA(100) + word2vec(300)\n",
    "X_train = pd.concat([X_train_pca_df, X_train_w2v_df], axis=1)\n",
    "X_test = pd.concat([X_test_pca_df, X_test_w2v_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "963c27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_pca_df.shape)\n",
    "# print(X_train_w2v_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "42f8b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 400)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>...</th>\n",
       "      <th>w290</th>\n",
       "      <th>w291</th>\n",
       "      <th>w292</th>\n",
       "      <th>w293</th>\n",
       "      <th>w294</th>\n",
       "      <th>w295</th>\n",
       "      <th>w296</th>\n",
       "      <th>w297</th>\n",
       "      <th>w298</th>\n",
       "      <th>w299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>-51.539445</td>\n",
       "      <td>5.629274</td>\n",
       "      <td>1.359153</td>\n",
       "      <td>-3.786479</td>\n",
       "      <td>2.407886</td>\n",
       "      <td>-1.145536</td>\n",
       "      <td>-1.534596</td>\n",
       "      <td>0.926409</td>\n",
       "      <td>0.992538</td>\n",
       "      <td>0.139425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302917</td>\n",
       "      <td>1.123871</td>\n",
       "      <td>-0.713104</td>\n",
       "      <td>0.526428</td>\n",
       "      <td>-0.237915</td>\n",
       "      <td>-1.088757</td>\n",
       "      <td>-1.758789</td>\n",
       "      <td>-1.093750</td>\n",
       "      <td>-1.001343</td>\n",
       "      <td>1.657166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>93.964504</td>\n",
       "      <td>-4.692317</td>\n",
       "      <td>0.559970</td>\n",
       "      <td>3.620832</td>\n",
       "      <td>1.907594</td>\n",
       "      <td>2.368824</td>\n",
       "      <td>0.752730</td>\n",
       "      <td>-0.013715</td>\n",
       "      <td>-0.114086</td>\n",
       "      <td>-0.273923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.657349</td>\n",
       "      <td>-0.394531</td>\n",
       "      <td>-1.828560</td>\n",
       "      <td>-0.245117</td>\n",
       "      <td>2.077881</td>\n",
       "      <td>0.959991</td>\n",
       "      <td>0.515457</td>\n",
       "      <td>-0.954285</td>\n",
       "      <td>-1.612137</td>\n",
       "      <td>0.531799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>91.623416</td>\n",
       "      <td>-6.467287</td>\n",
       "      <td>6.976855</td>\n",
       "      <td>2.967421</td>\n",
       "      <td>-6.471648</td>\n",
       "      <td>1.227645</td>\n",
       "      <td>-0.738837</td>\n",
       "      <td>0.804611</td>\n",
       "      <td>0.189021</td>\n",
       "      <td>-0.410385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387238</td>\n",
       "      <td>0.114929</td>\n",
       "      <td>-2.497040</td>\n",
       "      <td>3.079620</td>\n",
       "      <td>-1.336224</td>\n",
       "      <td>-1.514481</td>\n",
       "      <td>-2.195477</td>\n",
       "      <td>-1.147064</td>\n",
       "      <td>-1.457703</td>\n",
       "      <td>2.532001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca_0     pca_1     pca_2     pca_3     pca_4     pca_5     pca_6  \\\n",
       "2845 -51.539445  5.629274  1.359153 -3.786479  2.407886 -1.145536 -1.534596   \n",
       "1682  93.964504 -4.692317  0.559970  3.620832  1.907594  2.368824  0.752730   \n",
       "5097  91.623416 -6.467287  6.976855  2.967421 -6.471648  1.227645 -0.738837   \n",
       "\n",
       "         pca_7     pca_8     pca_9  ...      w290      w291      w292  \\\n",
       "2845  0.926409  0.992538  0.139425  ... -0.302917  1.123871 -0.713104   \n",
       "1682 -0.013715 -0.114086 -0.273923  ... -0.657349 -0.394531 -1.828560   \n",
       "5097  0.804611  0.189021 -0.410385  ...  0.387238  0.114929 -2.497040   \n",
       "\n",
       "          w293      w294      w295      w296      w297      w298      w299  \n",
       "2845  0.526428 -0.237915 -1.088757 -1.758789 -1.093750 -1.001343  1.657166  \n",
       "1682 -0.245117  2.077881  0.959991  0.515457 -0.954285 -1.612137  0.531799  \n",
       "5097  3.079620 -1.336224 -1.514481 -2.195477 -1.147064 -1.457703  2.532001  \n",
       "\n",
       "[3 rows x 400 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fdb179d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.43 s, sys: 3.64 ms, total: 4.43 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = make_pipeline(StandardScaler(), SVC())\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8af5091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1f65031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.8%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.93      0.93      1020\n",
      "        real       0.94      0.94      0.94      1120\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.94      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SVC + word2vec + PCA(tf-idf & feature engineering,100)',\n",
       " 400,\n",
       " 0.9378504672897197,\n",
       " 0.9313725490196079,\n",
       " 0.9378084896347483,\n",
       " 0.9345794392523366,\n",
       " datetime.timedelta(seconds=4, microseconds=452017))"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, f'SVC + word2vec + PCA(tf-idf & feature engineering,100)',\n",
    "    X_train.shape[1],\n",
    "    train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458bba3",
   "metadata": {},
   "source": [
    "<a id='sect4_10'></a>\n",
    "### <font color='darkgreen'>SVC + word2vec + PCA(tf-idf & feature engineering,100) + 2gram </font> ([back](#sect4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cd06a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally merging all features with above TF-IDF. \n",
    "X_train = pd.concat([X_train_pca_df, X_train_w2v_df, train_2gram_df], axis=1)\n",
    "X_test  = pd.concat([X_test_pca_df, X_test_w2v_df, test_2gram_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4e0b27cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6420, 546)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_0</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>...</th>\n",
       "      <th>2gram_136</th>\n",
       "      <th>2gram_137</th>\n",
       "      <th>2gram_138</th>\n",
       "      <th>2gram_139</th>\n",
       "      <th>2gram_140</th>\n",
       "      <th>2gram_141</th>\n",
       "      <th>2gram_142</th>\n",
       "      <th>2gram_143</th>\n",
       "      <th>2gram_144</th>\n",
       "      <th>2gram_145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>20.241945</td>\n",
       "      <td>-4.504939</td>\n",
       "      <td>4.394871</td>\n",
       "      <td>2.879675</td>\n",
       "      <td>-2.294235</td>\n",
       "      <td>-0.831527</td>\n",
       "      <td>-0.609774</td>\n",
       "      <td>-0.045007</td>\n",
       "      <td>-0.604361</td>\n",
       "      <td>-0.539040</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>58.827792</td>\n",
       "      <td>-8.728886</td>\n",
       "      <td>-10.771235</td>\n",
       "      <td>14.149495</td>\n",
       "      <td>-2.792136</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-0.197456</td>\n",
       "      <td>0.508971</td>\n",
       "      <td>-0.833536</td>\n",
       "      <td>-0.848457</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>-96.159092</td>\n",
       "      <td>-2.586863</td>\n",
       "      <td>11.260712</td>\n",
       "      <td>-6.483547</td>\n",
       "      <td>1.823557</td>\n",
       "      <td>0.441469</td>\n",
       "      <td>-0.340822</td>\n",
       "      <td>-0.613586</td>\n",
       "      <td>0.519628</td>\n",
       "      <td>-0.034353</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 546 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca_0     pca_1      pca_2      pca_3     pca_4     pca_5     pca_6  \\\n",
       "2196  20.241945 -4.504939   4.394871   2.879675 -2.294235 -0.831527 -0.609774   \n",
       "5342  58.827792 -8.728886 -10.771235  14.149495 -2.792136  0.013999 -0.197456   \n",
       "5360 -96.159092 -2.586863  11.260712  -6.483547  1.823557  0.441469 -0.340822   \n",
       "\n",
       "         pca_7     pca_8     pca_9  ...  2gram_136  2gram_137  2gram_138  \\\n",
       "2196 -0.045007 -0.604361 -0.539040  ...          0          0          0   \n",
       "5342  0.508971 -0.833536 -0.848457  ...          1          2          1   \n",
       "5360 -0.613586  0.519628 -0.034353  ...          0          0          0   \n",
       "\n",
       "      2gram_139  2gram_140  2gram_141  2gram_142  2gram_143  2gram_144  \\\n",
       "2196          0          0          0          0          0          0   \n",
       "5342          0          0          0          0          0          1   \n",
       "5360          0          0          0          0          0          0   \n",
       "\n",
       "      2gram_145  \n",
       "2196          0  \n",
       "5342          0  \n",
       "5360          0  \n",
       "\n",
       "[3 rows x 546 columns]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b65946ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.88 s, sys: 9.21 ms, total: 7.89 s\n",
      "Wall time: 7.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = datetime.now()\n",
    "clf_model = make_pipeline(StandardScaler(), SVC())\n",
    "clf_model.fit(X_train, y_train)\n",
    "train_time = datetime.now() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "26ffbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ff06c941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.9%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.94      0.93      0.94      1020\n",
      "        real       0.94      0.94      0.94      1120\n",
      "\n",
      "    accuracy                           0.94      2140\n",
      "   macro avg       0.94      0.94      0.94      2140\n",
      "weighted avg       0.94      0.94      0.94      2140\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SVC + word2vec + PCA(tf-idf & feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)',\n",
       " 546,\n",
       " 0.938785046728972,\n",
       " 0.9323529411764706,\n",
       " 0.9387956564659428,\n",
       " 0.9355632070831283,\n",
       " datetime.timedelta(seconds=7, microseconds=895081))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_perf_data(\n",
    "    y_pred, f'SVC + word2vec + PCA(tf-idf & feature engineering,100) + {two_gram_feats}',\n",
    "    X_train.shape[1],\n",
    "    train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7b767",
   "metadata": {},
   "source": [
    "<a id='sect5'></a>\n",
    "## <font color='darkblue'>Conclusion</font> ([back](#sect0))\n",
    "Then we can evaluate and sort them according to interested metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "929c8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.DataFrame(perf_data, columns=['name', 'feature size', 'accuracy', 'recall', 'precision', 'f1', 'train time'])\n",
    "perf_df = perf_df.set_index('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "d75f8c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>546</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.932353</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0 days 00:00:07.895081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100)</th>\n",
       "      <td>400</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0 days 00:00:04.452017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>14064</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.928431</td>\n",
       "      <td>0.933925</td>\n",
       "      <td>0.931170</td>\n",
       "      <td>0 days 00:07:24.486139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.889216</td>\n",
       "      <td>0.964894</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0 days 00:01:02.310574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.929212</td>\n",
       "      <td>0.921404</td>\n",
       "      <td>0 days 00:07:14.036797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.924766</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.921492</td>\n",
       "      <td>0.921040</td>\n",
       "      <td>0 days 00:00:13.514017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grid search SVC + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.920093</td>\n",
       "      <td>0.907843</td>\n",
       "      <td>0.923230</td>\n",
       "      <td>0.915472</td>\n",
       "      <td>0 days 00:04:37.684995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf</th>\n",
       "      <td>13872</td>\n",
       "      <td>0.913084</td>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.938947</td>\n",
       "      <td>0.905584</td>\n",
       "      <td>0 days 00:03:03.325149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.890187</td>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.904222</td>\n",
       "      <td>0.881969</td>\n",
       "      <td>0 days 00:01:33.162035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    feature size  accuracy  \\\n",
       "name                                                                         \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           546  0.938785   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           400  0.937850   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...         14064  0.934579   \n",
       "randomforest + tf-idf + feature engineering                13918  0.931776   \n",
       "SVC + tf-idf + feature engineering                         13918  0.925701   \n",
       "SVC + word2vec + feature engineering                         946  0.924766   \n",
       "Grid search SVC + word2vec + feature engineering             946  0.920093   \n",
       "randomforest + tf-idf                                      13872  0.913084   \n",
       "randomforest + word2vec + feature engineering                946  0.890187   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "name                                                                      \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.932353   0.938796   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.931373   0.937808   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.928431   0.933925   \n",
       "randomforest + tf-idf + feature engineering         0.889216   0.964894   \n",
       "SVC + tf-idf + feature engineering                  0.913725   0.929212   \n",
       "SVC + word2vec + feature engineering                0.920588   0.921492   \n",
       "Grid search SVC + word2vec + feature engineering    0.907843   0.923230   \n",
       "randomforest + tf-idf                               0.874510   0.938947   \n",
       "randomforest + word2vec + feature engineering       0.860784   0.904222   \n",
       "\n",
       "                                                          f1  \\\n",
       "name                                                           \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.935563   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.934579   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.931170   \n",
       "randomforest + tf-idf + feature engineering         0.925510   \n",
       "SVC + tf-idf + feature engineering                  0.921404   \n",
       "SVC + word2vec + feature engineering                0.921040   \n",
       "Grid search SVC + word2vec + feature engineering    0.915472   \n",
       "randomforest + tf-idf                               0.905584   \n",
       "randomforest + word2vec + feature engineering       0.881969   \n",
       "\n",
       "                                                               train time  \n",
       "name                                                                       \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:07.895081  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:04.452017  \n",
       "SVC + tf-idf + feature engineering + TwoGramFea... 0 days 00:07:24.486139  \n",
       "randomforest + tf-idf + feature engineering        0 days 00:01:02.310574  \n",
       "SVC + tf-idf + feature engineering                 0 days 00:07:14.036797  \n",
       "SVC + word2vec + feature engineering               0 days 00:00:13.514017  \n",
       "Grid search SVC + word2vec + feature engineering   0 days 00:04:37.684995  \n",
       "randomforest + tf-idf                              0 days 00:03:03.325149  \n",
       "randomforest + word2vec + feature engineering      0 days 00:01:33.162035  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Accuracy:\n",
    "perf_df.sort_values(by='accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "df3e98fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>546</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.932353</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0 days 00:00:07.895081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100)</th>\n",
       "      <td>400</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0 days 00:00:04.452017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>14064</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.928431</td>\n",
       "      <td>0.933925</td>\n",
       "      <td>0.931170</td>\n",
       "      <td>0 days 00:07:24.486139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.889216</td>\n",
       "      <td>0.964894</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0 days 00:01:02.310574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.929212</td>\n",
       "      <td>0.921404</td>\n",
       "      <td>0 days 00:07:14.036797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    feature size  accuracy  \\\n",
       "name                                                                         \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           546  0.938785   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           400  0.937850   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...         14064  0.934579   \n",
       "randomforest + tf-idf + feature engineering                13918  0.931776   \n",
       "SVC + tf-idf + feature engineering                         13918  0.925701   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "name                                                                      \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.932353   0.938796   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.931373   0.937808   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.928431   0.933925   \n",
       "randomforest + tf-idf + feature engineering         0.889216   0.964894   \n",
       "SVC + tf-idf + feature engineering                  0.913725   0.929212   \n",
       "\n",
       "                                                          f1  \\\n",
       "name                                                           \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.935563   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.934579   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.931170   \n",
       "randomforest + tf-idf + feature engineering         0.925510   \n",
       "SVC + tf-idf + feature engineering                  0.921404   \n",
       "\n",
       "                                                               train time  \n",
       "name                                                                       \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:07.895081  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:04.452017  \n",
       "SVC + tf-idf + feature engineering + TwoGramFea... 0 days 00:07:24.486139  \n",
       "randomforest + tf-idf + feature engineering        0 days 00:01:02.310574  \n",
       "SVC + tf-idf + feature engineering                 0 days 00:07:14.036797  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by F1:\n",
    "perf_df.sort_values(by='f1', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "09770f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>546</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.932353</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0 days 00:00:07.895081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100)</th>\n",
       "      <td>400</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0 days 00:00:04.452017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>14064</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.928431</td>\n",
       "      <td>0.933925</td>\n",
       "      <td>0.931170</td>\n",
       "      <td>0 days 00:07:24.486139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.924766</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.921492</td>\n",
       "      <td>0.921040</td>\n",
       "      <td>0 days 00:00:13.514017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.925701</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.929212</td>\n",
       "      <td>0.921404</td>\n",
       "      <td>0 days 00:07:14.036797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    feature size  accuracy  \\\n",
       "name                                                                         \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           546  0.938785   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           400  0.937850   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...         14064  0.934579   \n",
       "SVC + word2vec + feature engineering                         946  0.924766   \n",
       "SVC + tf-idf + feature engineering                         13918  0.925701   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "name                                                                      \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.932353   0.938796   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.931373   0.937808   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.928431   0.933925   \n",
       "SVC + word2vec + feature engineering                0.920588   0.921492   \n",
       "SVC + tf-idf + feature engineering                  0.913725   0.929212   \n",
       "\n",
       "                                                          f1  \\\n",
       "name                                                           \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.935563   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.934579   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.931170   \n",
       "SVC + word2vec + feature engineering                0.921040   \n",
       "SVC + tf-idf + feature engineering                  0.921404   \n",
       "\n",
       "                                                               train time  \n",
       "name                                                                       \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:07.895081  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:04.452017  \n",
       "SVC + tf-idf + feature engineering + TwoGramFea... 0 days 00:07:24.486139  \n",
       "SVC + word2vec + feature engineering               0 days 00:00:13.514017  \n",
       "SVC + tf-idf + feature engineering                 0 days 00:07:14.036797  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by recall:\n",
    "perf_df.sort_values(by='recall', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a2908c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.889216</td>\n",
       "      <td>0.964894</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0 days 00:01:02.310574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf</th>\n",
       "      <td>13872</td>\n",
       "      <td>0.913084</td>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.938947</td>\n",
       "      <td>0.905584</td>\n",
       "      <td>0 days 00:03:03.325149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>546</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.932353</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0 days 00:00:07.895081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100)</th>\n",
       "      <td>400</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0 days 00:00:04.452017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + tf-idf + feature engineering + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>14064</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.928431</td>\n",
       "      <td>0.933925</td>\n",
       "      <td>0.931170</td>\n",
       "      <td>0 days 00:07:24.486139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    feature size  accuracy  \\\n",
       "name                                                                         \n",
       "randomforest + tf-idf + feature engineering                13918  0.931776   \n",
       "randomforest + tf-idf                                      13872  0.913084   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           546  0.938785   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           400  0.937850   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...         14064  0.934579   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "name                                                                      \n",
       "randomforest + tf-idf + feature engineering         0.889216   0.964894   \n",
       "randomforest + tf-idf                               0.874510   0.938947   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.932353   0.938796   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.931373   0.937808   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.928431   0.933925   \n",
       "\n",
       "                                                          f1  \\\n",
       "name                                                           \n",
       "randomforest + tf-idf + feature engineering         0.925510   \n",
       "randomforest + tf-idf                               0.905584   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.935563   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.934579   \n",
       "SVC + tf-idf + feature engineering + TwoGramFea...  0.931170   \n",
       "\n",
       "                                                               train time  \n",
       "name                                                                       \n",
       "randomforest + tf-idf + feature engineering        0 days 00:01:02.310574  \n",
       "randomforest + tf-idf                              0 days 00:03:03.325149  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:07.895081  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:04.452017  \n",
       "SVC + tf-idf + feature engineering + TwoGramFea... 0 days 00:07:24.486139  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by precision:\n",
    "perf_df.sort_values(by='precision', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e184bdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature size</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>train time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100)</th>\n",
       "      <td>400</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0 days 00:00:04.452017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + PCA(tf-idf &amp; feature engineering,100) + TwoGramFeatures(min_doc_count=70, top_n=150): 146 2gram(s)</th>\n",
       "      <td>546</td>\n",
       "      <td>0.938785</td>\n",
       "      <td>0.932353</td>\n",
       "      <td>0.938796</td>\n",
       "      <td>0.935563</td>\n",
       "      <td>0 days 00:00:07.895081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.924766</td>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.921492</td>\n",
       "      <td>0.921040</td>\n",
       "      <td>0 days 00:00:13.514017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + tf-idf + feature engineering</th>\n",
       "      <td>13918</td>\n",
       "      <td>0.931776</td>\n",
       "      <td>0.889216</td>\n",
       "      <td>0.964894</td>\n",
       "      <td>0.925510</td>\n",
       "      <td>0 days 00:01:02.310574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest + word2vec + feature engineering</th>\n",
       "      <td>946</td>\n",
       "      <td>0.890187</td>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.904222</td>\n",
       "      <td>0.881969</td>\n",
       "      <td>0 days 00:01:33.162035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    feature size  accuracy  \\\n",
       "name                                                                         \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           400  0.937850   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...           546  0.938785   \n",
       "SVC + word2vec + feature engineering                         946  0.924766   \n",
       "randomforest + tf-idf + feature engineering                13918  0.931776   \n",
       "randomforest + word2vec + feature engineering                946  0.890187   \n",
       "\n",
       "                                                      recall  precision  \\\n",
       "name                                                                      \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.931373   0.937808   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.932353   0.938796   \n",
       "SVC + word2vec + feature engineering                0.920588   0.921492   \n",
       "randomforest + tf-idf + feature engineering         0.889216   0.964894   \n",
       "randomforest + word2vec + feature engineering       0.860784   0.904222   \n",
       "\n",
       "                                                          f1  \\\n",
       "name                                                           \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.934579   \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri...  0.935563   \n",
       "SVC + word2vec + feature engineering                0.921040   \n",
       "randomforest + tf-idf + feature engineering         0.925510   \n",
       "randomforest + word2vec + feature engineering       0.881969   \n",
       "\n",
       "                                                               train time  \n",
       "name                                                                       \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:04.452017  \n",
       "SVC + word2vec + PCA(tf-idf & feature engineeri... 0 days 00:00:07.895081  \n",
       "SVC + word2vec + feature engineering               0 days 00:00:13.514017  \n",
       "randomforest + tf-idf + feature engineering        0 days 00:01:02.310574  \n",
       "randomforest + word2vec + feature engineering      0 days 00:01:33.162035  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by train time:\n",
    "perf_df.sort_values(by='train time', ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1095",
   "metadata": {},
   "source": [
    "<b>The above results show that if we do feature engineering, we can achieve greater accuracy using classical Machine learning algorithms.</b> Using a transformer-based model is a time-consuming and resource-expensive algorithms. If we do feature engineering in the right way that is after analyzing our dataset we can get comparable results.\n",
    "\n",
    "We can also do some other feature engineering like, counting the number of emojis used, type of emojis used, what frequencies of unique words, etc. <b>We can define our features by analyzing the dataset and the only limit is your imagination.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d294e6f",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Supplement</font>\n",
    "* [Medium - Grid Search VS Random Search VS Bayesian Optimization](https://github.com/johnklee/ml_articles/blob/master/medium/Grid_Search_VS_Random_Search_VS_Bayesian_Optimization/notebook.ipynb)\n",
    "* [Python Datascience Handbook - In Depth: Principal Component Analysis](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html)\n",
    "* [Kaggle - Gensim Word2Vec Tutorial](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook)\n",
    "* [Sklearn - Principal component analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "* [FAQ - ImportError: cannot import name 'HalvingGridSearchCV' from 'sklearn.model_selection'](https://stackoverflow.com/questions/70908124/importerror-cannot-import-name-halvinggridsearchcv-from-sklearn-model-select)\n",
    "* [SVM Hyperparameter Tuning using GridSearchCV](https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/)\n",
    "* [用白話文談數學公式 - 熵(entropy)](https://myapollo.com.tw/zh-tw/shannon-entropy-explanation/)\n",
    "> <b>熵（音同商），簡而言之，用以衡量一組資料的不確定性(uncertainty)。</b><br/><br/>\n",
    "> 假設有 1 枚硬幣 2 面都是人頭，那麼不管如何投擲，我們都能夠確定它的結果是人頭，因此毫無不確定性可言，那麼熵值就會是 0 。<br/>\n",
    "> 假設有 1 枚硬幣 正面人頭，背面為字，那麼我們有 50% 的機率猜中其投擲結果，因此該情況下具有不確定性，其熵值為 1 。<br/>\n",
    "> 假設有 2 枚硬幣，那麼我們約有 33% 的機率猜中其投擲結果（正正、反反、正反），該情況的不確定性相對於只有 1 枚硬幣而言更高，其熵值約為 1.58 。<br/>\n",
    "> 可以看到隨著<b>不確定性增加，熵值也會增加</b>。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
