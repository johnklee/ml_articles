{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e96482-be00-4f69-9995-1bfbc24e7166",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://realpython.com/chromadb-vector-database/)) <b><font size='3ptx'>The era of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) is here, bringing with it rapidly evolving libraries like [ChromaDB](https://docs.trychroma.com/) that help augment LLM applications. You’ve most likely heard of chatbots like OpenAI’s [ChatGPT](https://realpython.com/chatgpt-coding-mentor-python/), and perhaps you’ve even experienced their remarkable ability to reason about [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) problems.</font></b>\n",
    "\n",
    "Modern LLMs, while imperfect, can accurately solve a wide range of problems and provide correct answers to many questions. But, <b>due to the limits of their training and the number of text tokens they can process, LLMs aren’t a silver bullet for all tasks</b>.\n",
    "\n",
    "<b>You wouldn’t expect an LLM to provide relevant responses about topics that don’t appear in their training data.</b> For example, if you asked ChatGPT to summarize information in confidential company documents, then you’d be out of luck. <b>You could show some of these documents to ChatGPT, but there’s a limited number of documents that you can upload before you exceed ChatGPT’s maximum number of tokens. How would you select documents to show ChatGPT?</b>\n",
    "\n",
    "<b>To address these shortcomings and scale your LLM applications, one great option is to use a <font color='darkblue'>vector database</font> like ChromaDB</b>. A vector database allows you to store encoded unstructured objects, like text, as lists of numbers that you can compare to one another. You can, for example, find a collection of documents relevant to a question that you want an LLM to answer.\n",
    "\n",
    "In this tutorial, you’ll learn about:\n",
    "* Representing unstructured objects with vectors\n",
    "* Using word and text embeddings in Python\n",
    "* Harnessing the power of vector databases\n",
    "* Encoding and querying over documents with ChromaDB\n",
    "* Providing context to LLMs like ChatGPT with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44417089-fd5b-4117-865d-aeca4c601b02",
   "metadata": {},
   "source": [
    "<a id='agenda'></a>\n",
    "### <b><font color='darkgreen'>Agenda</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1'>Represent Data as Vectors</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2'>Encode Objects in Embeddings</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect3'>Get Started With ChromaDB, an Open-Source Vector Database</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect4'>Practical Example: Add Context for a Large Language Model (LLM)</a></font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f8168e-f4dc-4e60-8fbc-fe76aa284f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822eb1-090b-4532-af81-51f26966d9d8",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <b><font color='darkblue'>Represent Data as Vectors</font></b>\n",
    "<b><font size='3ptx'>Before diving into embeddings and vector databases, you should understand what vectors are and what they represent.</font></b> Feel free to skip ahead to the next section if you’re already comfortable with vector concepts. If you’re not or if you could use a refresher, then keep reading!\n",
    "* <b><a href='#sect1_1'>Vector Basics</a></b>\n",
    "* <b><a href='#sect1_2'>Vector Similarity</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cf2c8c-c85f-4bc9-bea0-20c3aba6022e",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <b><font color='darkgreen'>Vector Basics</font></b>\n",
    "<b><font size='3ptx'>You can describe vectors with variable levels of complexity, but one great starting place is to think of a vector as an [array](https://realpython.com/numpy-array-programming/#getting-into-shape-intro-to-numpy-arrays) of numbers.</font></b> For example, you could represent vectors using NumPy arrays as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda635dd-689f-488d-82cc-5d75cf398845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 = np.array([1, 0])\n",
    "vector2 = np.array([0, 1])\n",
    "vector1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9205c37b-e19f-4e70-b49b-3de858d2fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b89905-745b-42f0-925f-3b620f224188",
   "metadata": {},
   "source": [
    "You’ve created two NumPy arrays that represent vectors. Now what? It turns out you can do a lot of cool things with vectors, but before continuing on, you’ll need to understand some key definitions and properties:\n",
    "* **Dimension**: The dimension of a vector is the number of elements that it contains. In the example above, `vector1` and `vector2` are both two-dimensional since they each have two elements. You can only visualize vectors with three dimensions or less, but generally, vectors can have any number of dimensions. In fact, as you’ll see later, vectors that encode words and text tend to have hundreds or thousands of dimensions.\n",
    "* **Magnitude**: The magnitude of a vector is a non-negative number that represents the vector’s size or length. You can also refer to the magnitude of a vector as the **norm**, and you can denote it with ||v|| or |v|. There are many different definitions of magnitude or norm, but the most common is the [**Euclidean norm**](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) or 2-norm. You’ll learn how to compute this later.\n",
    "* **Unit vector**: A unit vector is a vector with a magnitude of one. In the example above, `vector1` and `vector2` are unit vectors.\n",
    "* **Direction**: The direction of a vector specifies the line along which the vector points. You can represent direction using angles, unit vectors, or coordinates in different coordinate systems.\n",
    "* [**Dot product**](https://en.wikipedia.org/wiki/Dot_product) (<font color='brown'>scalar product</font>): The dot product of two vectors, `u` and `v`, is a number given by $u ⋅ v = ||u|| ||v|| cos(θ)$, where `θ` is the angle between the two vectors. Another way to compute the dot product is to do an element-wise multiplication of `u` and `v` and sum the results. The dot product is one of the most important and widely used vector operations because it measures the similarity between two vectors. You’ll see more of this later on.\n",
    "* **Orthogonal vectors**: Vectors are orthogonal if their dot product is zero, meaning that they’re at a 90 degree angle to each other. You can think of orthogonal vectors as being completely unrelated to each other.\n",
    "* **Dense vector**: A vector is considered dense if most of its elements are non-zero. Later on, you’ll see that words and text are most usefully represented with dense vectors because each dimension encodes meaningful information.\n",
    "\n",
    "While there are many more definitions and properties to learn, these six are most important for this tutorial. To solidify these ideas with code, check out the following block. Note that for the rest of this tutorial, you’ll use `v1`, `v2`, and `v3` to name your vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fbe012-5b13-42d3-8876-f9402823cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([0, 1])\n",
    "v3 = np.array([np.sqrt(2), np.sqrt(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fd5e25-a8fc-4ad2-a912-cd79fbee4033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension\n",
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8153baff-5d54-4dfd-8533-85b854f1bdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Magnitude\n",
    "np.sqrt(np.sum(v1**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b4c4560-7a2a-40f7-bf71-efb4eca19c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f6376b-7d2d-4f92-89b5-ad1b2a5cf9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f854abd-44b4-42ff-8518-efcbe8b419ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot product\n",
    "np.sum(v1 * v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde175b9-44cf-454d-9295-8d950c7f7b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 @ v3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e9dee-9708-4846-b649-b5acc5d76bab",
   "metadata": {},
   "source": [
    "A better way to compute the dot product is to use the at-operator (`@`), as you see with $v1 @ v3$. This is because `@` can perform both vector and matrix multiplications, and the syntax is cleaner.\n",
    "\n",
    "While all of these vector definitions and properties may seem straightforward to compute, you might still be wondering what they actually mean and why they’re important to understand. One way to better understand vectors is to visualize them in two dimensions. In this context, you can represent vectors as arrows, like in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d98f96ca-1c61-47c3-a790-fc63bef1433d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the plot\n",
    "plt.rcParams['figure.figsize'] = [4, 4]\n",
    "fig, ax = plt.subplots()\n",
    "V = np.array([v1, v2, v3])\n",
    "origin = np.array([[0, 0, 0],[0, 0, 0]]) # origin point\n",
    "ax.quiver(*origin, V[:,0], V[:,1], color=['r','b','g'], scale=1, angles='xy', scale_units='xy')\n",
    "ax.set_xlim([-0.5, 2])\n",
    "ax.set_ylim([-0.5, 2])\n",
    "plt.grid()\n",
    "plt.title('Representing vectors as arrows in two dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a397158d-9bde-4df8-a642-d0e60d8f788b",
   "metadata": {},
   "source": [
    "The above plot shows the visual representation of the vectors `v1` (R), `v2` (B), and `v3` (G) that you worked with in the last example. The tail of each vector arrow always starts at the origin, and the tip is located at the coordinates specified by the vector. As an example, the tip of `v1` lies at (1, 0), and the tip of `v3` lies at roughly (1.414, 1.414). The length of each vector arrow corresponds to the magnitude that you calculated earlier.\n",
    "\n",
    "From this visual, you can make the following key inferences:\n",
    "1. `v1` and `v2` are unit vectors because their magnitude, given by the arrow length, is one. `v3` isn’t a unit vector, and its magnitude is two, twice the size of `v1` and `v2`.\n",
    "2. `v1` and `v2` are **orthogonal** because their tails meet at a 90 degree angle. You see this visually but can also verify it computationally by computing the dot product between `v1` and `v2`. By using the dot product definition, $v1 ⋅ v2 = ||v1|| ||v2|| cos(θ)$, you can see that when $θ = 90$, $cos(θ) = 0$ and $v1 ⋅ v2 = 0$. Intuitively, you can think of `v1` and `v2` as being totally unrelated or having nothing to do with each other. This will become important later.\n",
    "3. `v3` makes a 45 degree angle with both `v1` and `v2`. This means that `v3` will have a non-zero dot product with `v1` and `v2`. This also means that `v3` is equally related to both `v1` and `v2`. In general, the smaller the angle between two vectors, the more they point toward a common direction.\n",
    "\n",
    "You’ve now seen how vectors are characterized both computationally and visually. With this understanding, you’re ready to take a slightly deeper dive into the idea of vector similarity. If you only take away one thing from this introduction, it should be what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b569442-4331-424e-88d6-ab1c126cddbe",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <b><font color='darkgreen'>Vector Similarity</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>The ability to measure vector similarity is crucial in machine learning and mathematics more broadly. </font></b>\n",
    "\n",
    "The foundation for this measurement lies in the dot product, which serves as the bedrock for many vector similarity metrics. <b>One issue with the dot product, when used in isolation, is that it can take on any value and is therefore difficult to interpret in absolute terms. For example, if you know only that the dot product between two vectors is -3, then it’s unclear what that means without more context</b>.\n",
    "\n",
    "To overcome this shortcoming, one common approach is to use [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity), a normalized form of the dot product. You compute cosine similarity by taking the cosine of the angle between two vectors. In essence, you rearrange the cosine definition of the dot product from earlier to solve for cos(θ). The equation for cosine similarity looks like this:\n",
    "![cosine similarity](https://realpython.com/cdn-cgi/image/width=982,format=auto/https://files.realpython.com/media/Screenshot_2023-10-30_at_1.17.14_PM.56871536fa90.png)\n",
    "\n",
    "<b>Cosine similarity disregards the magnitude of both vectors, forcing the calculation to lie between -1 and 1</b>. This is a really nice property because it gives cosine similarity the following interpretations:\n",
    "* **A value of 1 means the angle between the two vectors is 0 degrees**. In other words, the two vectors are **similar** because they point in the exact same direction. Keep in mind this doesn’t mean that the vectors have the same magnitude.\n",
    "* **A value of 0 means the angle between the two vectors is 90 degrees**. In this case, the vectors are orthogonal and **unrelated** to each other.\n",
    "* **A value of -1 means the angle between the two vectors is 180 degrees**. This is an interesting case where the vectors are **dissimilar** because they point in opposite directions.\n",
    "\n",
    "In short, a cosine similarity of 1 means the vectors are similar, 0 means the vectors are unrelated, and -1 means the vectors are opposite. Any values in between represent varying degrees of similarity or dissimilarity. \n",
    "\n",
    "<b>You now have a feel for what vectors are and how you can assess their similarity</b>. While there are many more vector concepts to learn about, you know enough to speak the language of embeddings and vector databases. In the next section, you’ll see how to convert words and sentences to vectors, a key prerequisite to text-based vector databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98eb85e-0bdf-4b6f-97e2-d0c4536864b1",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <b><font color='darkblue'>Encode Objects in Embeddings</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>The next step in your journey to understanding and using vector databases like ChromaDB is to get a feel for [embeddings](https://en.wikipedia.org/wiki/Word_embedding). </font></b>\n",
    "* <b><a href='#sect2_1'>Word Embeddings</a></b>\n",
    "* <b><a href='#sect2_2'>Text Embeddings</a></b>\n",
    "\n",
    "[**Embeddings**](https://en.wikipedia.org/wiki/Word_embedding) are a way to represent data such as words, text, images, and audio in a numerical format that computational algorithms can more easily process.\n",
    "\n",
    "More specifically, **embeddings are dense vectors that characterize meaningful information about the objects that they encode. The most common kinds of embeddings are word and text embeddings, and that’s what you’ll focus on in this tutorial**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c85f37-88bf-4db0-bbb5-d333e1043dc7",
   "metadata": {},
   "source": [
    "<a id='sect2_1'></a>\n",
    "### <b><font color='darkgreen'>Word Embeddings</font></b>\n",
    "<b><font size='3ptx'>A word embedding is a vector that captures the semantic meaning of word. </font></b>\n",
    "\n",
    "<b>Ideally, words that are semantically similar in natural language should have embeddings that are similar to each other in the encoded vector space</b>. Analogously, words that are unrelated or opposite of one another should be further apart in the vector space.\n",
    "\n",
    "One of the best ways to conceptualize this idea is to plot example word vectors in two dimensions. Take a good look at this scatterplot:\n",
    "![word embedding](https://files.realpython.com/media/Screenshot_2023-09-03_at_2.52.02_PM.5681ade2fa81.png)\n",
    "<center>Example word embeddings in two dimensions</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "This plot shows hand-crafted word embeddings plotted in two dimensions. Each point indicates where the word embedding’s tail lies. You’ll notice how related words are clustered together, while unrelated words are far from each other.\n",
    "\n",
    "<b>Word embeddings try to capture these semantic relationships for a large vocabulary of words, and as you might imagine, there are a lot of complex relationships to consider</b>. This is why, in practice, word embeddings often require hundreds or thousands of dimensions to account for the complexities of human language.\n",
    "\n",
    "You’re now ready to get started using word vectors in Python. For this, you’ll use the popular spaCy library, a general-purpose NLP library. To install [**spaCy**](https://realpython.com/natural-language-processing-spacy-python/), create a virtual environment, activate it, and run the following command:\n",
    "```shell\n",
    "(venv) $ python -m pip install spacy\n",
    "```\n",
    "\n",
    "After you’ve installed spaCy, you’ll also need to download a [**model**](https://spacy.io/models) that provides word embeddings, among other features. For this tutorial, you’ll want to install the medium or large English model:\n",
    "```shell\n",
    "(venv) $ python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "<b>SpaCy’s `en_core_web_md` model includes 20,000 pre-trained word embeddings, each of which has 300 dimensions</b>. This is more than enough for the examples that you’ll see next, but if you have the appetite for more word embeddings, then you can download the `en_core_web_lg` model, which has 514,000 embeddings.\n",
    "\n",
    "With spaCy’s medium or large English model installed, you’re ready to get started using word embeddings. It only takes a few lines of code to look up embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65dc0c14-e903-4b84-b72e-a4bbaa8c7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75198c9e-9202-4997-8d6b-0619da6a86b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.2330e+00,  4.2963e+00, -7.9738e+00, -1.0121e+01,  1.8207e+00,\n",
       "        1.4098e+00, -4.5180e+00, -5.2261e+00, -2.9157e-01,  9.5234e-01,\n",
       "        6.9880e+00,  5.0637e+00, -5.5726e-03,  3.3395e+00,  6.4596e+00,\n",
       "       -6.3742e+00,  3.9045e-02, -3.9855e+00,  1.2085e+00, -1.3186e+00,\n",
       "       -4.8886e+00,  3.7066e+00, -2.8281e+00, -3.5447e+00,  7.6888e-01,\n",
       "        1.5016e+00, -4.3632e+00,  8.6480e+00, -5.9286e+00, -1.3055e+00,\n",
       "        8.3870e-01,  9.0137e-01, -1.7843e+00, -1.0148e+00,  2.7300e+00,\n",
       "       -6.9039e+00,  8.0413e-01,  7.4880e+00,  6.1078e+00, -4.2130e+00,\n",
       "       -1.5384e-01, -5.4995e+00,  1.0896e+01,  3.9278e+00, -1.3601e-01,\n",
       "        7.7732e-02,  3.2218e+00, -5.8777e+00,  6.1359e-01, -2.4287e+00,\n",
       "        6.2820e+00,  1.3461e+01,  4.3236e+00,  2.4266e+00, -2.6512e+00,\n",
       "        1.1577e+00,  5.0848e+00, -1.7058e+00,  3.3824e+00,  3.2850e+00,\n",
       "        1.0969e+00, -8.3711e+00, -1.5554e+00,  2.0296e+00, -2.6796e+00,\n",
       "       -6.9195e+00, -2.3386e+00, -1.9916e+00, -3.0450e+00,  2.4890e+00,\n",
       "        7.3247e+00,  1.3364e+00,  2.3828e-01,  8.4388e-02,  3.1480e+00,\n",
       "       -1.1128e+00, -3.5598e+00, -1.2115e-01, -2.0357e+00, -3.2731e+00,\n",
       "       -7.7205e+00,  4.0948e+00, -2.0732e+00,  2.0833e+00, -2.2803e+00,\n",
       "       -4.9850e+00,  9.7667e+00,  6.1779e+00, -1.0352e+01, -2.2268e+00,\n",
       "        2.5765e+00, -5.7440e+00,  5.5564e+00, -5.2735e+00,  3.0004e+00,\n",
       "       -4.2512e+00, -1.5682e+00,  2.2698e+00,  1.0491e+00, -9.0486e+00,\n",
       "        4.2936e+00,  1.8709e+00,  5.1985e+00, -1.3153e+00,  6.5224e+00,\n",
       "        4.0113e-01, -1.2583e+01,  3.6534e+00, -2.0961e+00,  1.0022e+00,\n",
       "       -1.7873e+00, -4.2555e+00,  7.7471e+00,  1.0173e+00,  3.1626e+00,\n",
       "        2.3558e+00,  3.3589e-01, -4.4178e+00,  5.0584e+00, -2.4118e+00,\n",
       "       -2.7445e+00,  3.4170e+00, -1.1574e+01, -2.6568e+00, -3.6933e+00,\n",
       "       -2.0398e+00,  5.0976e+00,  6.5249e+00,  3.3573e+00,  9.5334e-01,\n",
       "       -9.4430e-01, -9.4395e+00,  2.7867e+00, -1.7549e+00,  1.7287e+00,\n",
       "        3.4942e+00, -1.6883e+00, -3.5771e+00, -1.9013e+00,  2.2239e+00,\n",
       "       -5.4335e+00, -6.5724e+00, -6.7228e-01, -1.9748e+00, -3.1080e+00,\n",
       "       -1.8570e+00,  9.9496e-01,  8.9135e-01, -4.4254e+00,  3.3125e-01,\n",
       "        5.8815e+00,  1.9384e+00,  5.7294e-01, -2.8830e+00,  3.8087e+00,\n",
       "       -1.3095e+00,  5.9208e+00,  3.3620e+00,  3.3571e+00, -3.8807e-01,\n",
       "        9.0022e-01, -5.5742e+00, -4.2939e+00,  1.4992e+00, -4.7080e+00,\n",
       "       -2.9402e+00, -1.2259e+00,  3.0980e-01,  1.8858e+00, -1.9867e+00,\n",
       "       -2.3554e-01, -5.4535e-01, -2.1387e-01,  2.4797e+00,  5.9710e+00,\n",
       "       -7.1249e+00,  1.6257e+00, -1.5241e+00,  7.5974e-01,  1.4312e+00,\n",
       "        2.3641e+00, -3.5566e+00,  9.2066e-01,  4.4934e-01, -1.3233e+00,\n",
       "        3.1733e+00, -4.7059e+00, -1.2090e+01, -3.9241e-01, -6.8457e-01,\n",
       "       -3.6789e+00,  6.6279e+00, -2.9937e+00, -3.8361e+00,  1.3868e+00,\n",
       "       -4.9002e+00, -2.4299e+00,  6.4312e+00,  2.5056e+00, -4.5080e+00,\n",
       "       -5.1278e+00, -1.5585e+00, -3.0226e+00, -8.6811e-01, -1.1538e+00,\n",
       "       -1.0022e+00, -9.1651e-01, -4.7810e-01, -1.6084e+00, -2.7307e+00,\n",
       "        3.7080e+00,  7.7423e-01, -1.1085e+00, -6.8755e-01, -8.2901e+00,\n",
       "        3.2405e+00, -1.6108e-01, -6.2837e-01, -5.5960e+00, -4.4865e+00,\n",
       "        4.0115e-01, -3.7063e+00, -2.1704e+00,  4.0789e+00, -1.7973e+00,\n",
       "        8.9538e+00,  8.9421e-01, -4.8128e+00,  4.5367e+00, -3.2579e-01,\n",
       "       -5.2344e+00, -3.9766e+00, -2.1979e+00,  3.5699e+00,  1.4982e+00,\n",
       "        6.0972e+00, -1.9704e+00,  4.6522e+00, -3.7734e-01,  3.9101e-02,\n",
       "        2.5361e+00, -1.8096e+00,  8.7035e+00, -8.6372e+00, -3.5257e+00,\n",
       "        3.1034e+00,  3.2635e+00,  4.5437e+00, -5.7290e+00, -2.9141e-01,\n",
       "       -2.0011e+00,  8.5328e+00, -4.5064e+00, -4.8276e+00, -1.1786e+01,\n",
       "        3.5607e-01, -5.7115e+00,  6.3122e+00, -3.6650e+00,  3.3597e-01,\n",
       "        2.5017e+00, -3.5025e+00, -3.7891e+00, -3.1343e+00, -1.4429e+00,\n",
       "       -6.9119e+00, -2.6114e+00, -5.9757e-01,  3.7847e-01,  6.3187e+00,\n",
       "        2.8965e+00, -2.5397e+00,  1.8022e+00,  3.5486e+00,  4.4721e+00,\n",
       "       -4.8481e+00, -3.6252e+00,  4.0969e+00, -2.0081e+00, -2.0122e-01,\n",
       "        2.5244e+00, -6.8817e-01,  6.7184e-01, -7.0466e+00,  1.6641e+00,\n",
       "       -2.2308e+00, -3.8960e+00,  6.1320e+00, -8.0335e+00, -1.7130e+00,\n",
       "        2.5688e+00, -5.2547e+00,  6.9845e+00,  2.7835e-01, -6.4554e+00,\n",
       "       -2.1327e+00, -5.6515e+00,  1.1174e+01, -8.0568e+00,  5.7985e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_embedding = nlp.vocab[\"dog\"].vector\n",
    "print(type(dog_embedding))\n",
    "print(dog_embedding.shape)\n",
    "dog_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fdb5dc-137f-4569-9b1d-412b6f38e3d2",
   "metadata": {},
   "source": [
    "This is pretty neat! The `nlp.vocab` object allows you to find the word embedding for any word in the model’s vocabulary. You can now assess the similarity between word embeddings using metrics like cosine similarity. To do this, create a new file called cosine_similarity.py in your working directory that contains the following function:\n",
    "- `cosine_similarity.py`\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def compute_cosine_similarity(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"Compute the cosine similarity between two vectors\"\"\"\n",
    "\n",
    "    return (u @ v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d4234-82cf-44a6-82d5-fb203cad1444",
   "metadata": {},
   "source": [
    "This function computes the cosine similarity between two NumPy arrays, `u` and `v`, using the definition discussed previously. You can pass word embeddings directly from spaCy into <font color='blue'>compute_cosine_similarity()</font> to see how related they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55cb70f9-c003-4bbd-ac06-167804a94ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosine_similarity import compute_cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62d873e-7b6c-46e3-b7fd-887e5edd1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_embedding = nlp.vocab[\"dog\"].vector\n",
    "cat_embedding = nlp.vocab[\"cat\"].vector\n",
    "apple_embedding = nlp.vocab[\"apple\"].vector\n",
    "tasty_embedding = nlp.vocab[\"tasty\"].vector\n",
    "delicious_embedding = nlp.vocab[\"delicious\"].vector\n",
    "truck_embedding = nlp.vocab[\"truck\"].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3cf456-1b5b-4674-b208-7b36f9135275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8220817"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(dog_embedding, cat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c94fcb2-24c1-44b1-a055-03cb84a435df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8482092"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(delicious_embedding, tasty_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6089aa1d-562b-4ae6-abba-e1261c62a081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5347654"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(apple_embedding, delicious_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31ef7fd0-2ed7-49b0-8c01-1ad6200e3a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22881007"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(dog_embedding, apple_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50cd2989-c503-4096-91b0-37e39dc0788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0897876"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(truck_embedding, delicious_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e588e4-789c-4054-8d1f-3ce81e03f914",
   "metadata": {},
   "source": [
    "In this block, you import spacy and <font color='blue'>compute_cosine_similarity()</font>, and you instantiate an `nlp` object using the medium-size English model. Next, you look up and store embeddings for six common words from the model’s vocabulary. By computing the cosine similarity between these embeddings, you get a sense for how the model views their semantic relationship. Here are some important observations about the similarity scores:\n",
    "* The `cat` and `dog` embeddings have a relatively high cosine similarity. This is likely because `cats` and `dogs` are common house pets, and you can find the word `dog` close to the word `cat` in English texts.\n",
    "* The `delicious` and `tasty` embeddings also have a high cosine similarity because they have almost the same meaning. However, unlike the `dog` and `cat` embeddings, `delicious` and `tasty` have similar word embeddings because you can use them interchangeably.\n",
    "* The `truck` and `delicious` embeddings have a cosine similarity close to 0. As you might expect, `truck` and `delicious` aren’t words that commonly appear in the same context.\n",
    "\n",
    "<b>Word embeddings are great for capturing the semantic relationships between words, but what if you wanted to take things to the next level and analyze the similarity between sentences or documents?</b> It turns out you accomplish this with text embeddings, and these are the kinds of embeddings that you’ll most often store in vector databases. More on that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd72a0-ed61-4752-88c9-ec31d9da3a5f",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <b><font color='darkgreen'>Text Embeddings</font></b> ([back](#sect2))\n",
    "<b><font size='3ptx'>Text embeddings encode information about sentences and documents, not just individual words, into vectors.</font></b>\n",
    "\n",
    "<b>This allows you to compare larger bodies of text to each other just like you did with word vectors</b>. Because they encode more information than a single word embedding, text embeddings are a more powerful representation of information.\n",
    "\n",
    "<b>Text embeddings are typically the fundamental objects stored in vector databases like ChromaDB, and in this section, you’ll learn how to create and compare them.</b>\n",
    "\n",
    "<b><font color='darkred'>Notes.</font></b>: If you’re curious about how the leap from word embeddings to text embeddings happens, then check out some of the [publications](https://www.sbert.net/docs/publications.html) on popular text embedding models. The best text embedding models are built using [**transformers**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), which leverage a mechanism known as [**attention**](https://en.wikipedia.org/wiki/Attention_(machine_learning)). To oversimplify things, the attention mechanism helps create context-specific word embeddings that fuse into text embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8615e6-e938-46f1-9586-3b23768119b5",
   "metadata": {},
   "source": [
    "<b>The most efficient way to generate text embeddings is to use pretrained models</b>. These models vary in size, but they’re all typically trained on a large corpus of text, enabling them to pick up on complex semantic relationships. The [**SentenceTransformers**](https://www.sbert.net/index.html) library in Python is one of the best tools for this. You can install `sentence-transformers` with the following command:\n",
    "```shell\n",
    "(venv) $ python -m pip install sentence-transformers\n",
    "```\n",
    "\n",
    "Generating text embeddings with [**SentenceTransformers**](https://www.sbert.net/index.html) is just as straightforward as using word vectors in spaCy. Here’s an example to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48b7aa6e-3bbd-4d52-b52b-ba5f2a3952c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d7d73e-467a-4fd2-8bba-7131d3bdfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The canine barked loudly.\",\n",
    "    \"The dog made a noisy bark.\",\n",
    "    \"He ate a lot of pizza.\",\n",
    "    \"He devoured a large quantity of pizza pie.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47afdf81-b5d1-44fa-80e9-872edb4ad541",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2f2e51a-fc67-4258-855a-7d24174ce310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59ab8f51-fff5-4420-af45-220f15c83205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 384)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c746fbe-38d3-4c49-8372-b25e52926ee8",
   "metadata": {},
   "source": [
    "You first import the SentenceTransformer class and load the \"[**all-MiniLM-L6-v2**](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\" model into an object called `model`. **This is one of the smallest pretrained models available, but it’s a great one to start with**.\n",
    "\n",
    "<b><font color='darkred'>Note</font></b>: The first time you use a model in SentenceTransformers, you’ll automatically download and save it in your environment. The initial download will take a few seconds depending on how large the model is, but after that, the model should load quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffff373-2e48-463b-a4cf-71e313b17aa0",
   "metadata": {},
   "source": [
    "Next, you define a list of sentences and call <font color='blue'>model.encode(texts)</font> to create the corresponding text embeddings. Notice that `text_embeddings` is a NumPy array with <b>the shape (4, 384), which means that it has 4 rows and 384 columns. This is because you encoded 4 texts, and \"all-MiniLM-L6-v2\" generates 384-dimensional embeddings</b>.\n",
    "\n",
    "While all the texts in this example are single sentences, <b>you can encode longer texts up to a specified word length. For example, \"all-MiniLM-L6-v2\" encodes texts [up to 256 words](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#:~:text=By%20default%2C%20input%20text%20longer%20than%20256%20word%20pieces%20is%20truncated.). It’ll truncate any text longer than this</b>.\n",
    "\n",
    "You now have a text embedding for all four texts, and <b>just like with word embeddings, you can compare them using cosine similarity</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03cbb273-7132-4ab1-9d2a-50f0e19a2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings_dict = dict(zip(texts, list(text_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "605e8782-fb0a-4b8c-bd74-9df429ab53d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77686167"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_text_1 = \"The canine barked loudly.\"\n",
    "dog_text_2 = \"The dog made a noisy bark.\"\n",
    "compute_cosine_similarity(text_embeddings_dict[dog_text_1],\n",
    "                          text_embeddings_dict[dog_text_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b5cf782-fbff-480a-bc90-4395a14c5867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78713393"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pizza_text_1 = \"He ate a lot of pizza.\"\n",
    "pizza_test_2 = \"He devoured a large quantity of pizza pie.\"\n",
    "compute_cosine_similarity(text_embeddings_dict[pizza_text_1],\n",
    "                          text_embeddings_dict[pizza_test_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "662c5265-7fcb-4ab5-87dd-465864f0ef62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.091282666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(text_embeddings_dict[dog_text_1],\n",
    "                          text_embeddings_dict[pizza_text_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe83a7-3a0b-4c34-be88-d46966b211f1",
   "metadata": {},
   "source": [
    "Here are some important conclusions:\n",
    "* The cosine similarity between `The canine barked loudly` and `The dog made a noisy bark` is relatively high even though the two sentences use different words. The same is true for the similarity between `He ate a lot of pizza` and `He devoured a large quantity of pizza pie`. Because the text embeddings encode semantic meaning, any pair of related texts should have a high cosine similarity.\n",
    "* As you might expect, the cosine similarity between `The canine barked loudly` and `He ate a lot of pizza is low` because the sentences are unrelated to each other.\n",
    "\n",
    "This example, while straightforward, illustrates a powerful idea that underpins vector databases. That is, <b>you can take a collection of unstructured objects, compute and store their embeddings, and then compare these embeddings to one another or to new embeddings. In this case, the unstructured objects are text, but keep in mind that the same idea can work for other data like images and audio.</b>\n",
    "\n",
    "Now that you’re up to speed on vectors and embeddings, you’re ready to get started with ChromaDB! In the next section, you’ll learn about vector databases and get a hands-on overview of ChromaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fcd3d-9fad-48ac-9758-e982a3ecea8a",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <b><font color='darkblue'>Get Started With ChromaDB, an Open-Source Vector Database</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>Now that you understand the mechanisms behind ChromaDB, you’re ready to tackle a real-world scenario. Say you have a library of thousands of documents, and you need a way to search through them.</font></b>\n",
    "* <b><a href='#sect3_1'>What Is a Vector Database?</a></b>\n",
    "* <b><a href='#sect3_2'>Meet ChromaDB for LLM Applications</a></b>\n",
    "\n",
    "<b>In particular, you want to be able to make queries that point you to relevant documents</b>. For example, if your query is find me documents containing financial information, then you want whatever system you use to point you to a financial document in your library.\n",
    "\n",
    "How would you design this system? With your knowledge of vectors and embeddings, your first inclination might be to run all of the documents through an embedding algorithm and store the documents and embeddings together. You’d then <b>convert a new query to an embedding and use cosine similarity to find the documents that are most relevant to the query</b>.\n",
    "\n",
    "While you’re perfectly capable of writing the code for this, you’re sure there has to be something out there to do this for you. Enter vector databases!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5b30f-2652-4840-baae-96ba18db25f2",
   "metadata": {},
   "source": [
    "<a id='sect3_1'></a>\n",
    "### <b><font color='darkgreen'>What Is a Vector Database?</font></b>\n",
    "<b><font size='3ptx'>A vector database is a database that allows you to efficiently store and query embedding data. </font></b>\n",
    "\n",
    "Vector databases extend the capabilities of traditional relational databases to embeddings. However, the <b>key distinguishing feature of a vector database is that query results aren’t an exact match to the query. Instead, using a specified similarity metric, the vector database returns embeddings that are similar to a query</b>.\n",
    "\n",
    "As an example use case, suppose you’ve stored company documents in a vector database. This means each document has been embedded and can be compared to other embeddings through a similarity metric like cosine similarity.\n",
    "\n",
    "The vector database will accept a query like `how much revenue did the company make in Q2 2023` and embed the query. It’ll then compare the embedded query to other embeddings in the vector database and return the documents that have embeddings that are most similar to the query embedding.\n",
    "\n",
    "In this example, perhaps the most similar document says something like `Company XYZ reported $15 million in revenue for Q2 2023`. <b>The vector database identified the document that had an embedding most similar to how much revenue did the company make in Q2 2023, which likely had a high similarity score based on the document’s semantics</b>.\n",
    "\n",
    "To make this possible, vector databases are equipped with features that balance the speed and accuracy of query results. Here are the <b>core components of a vector database that you should know about</b>:\n",
    "* **Embedding function**: When using a vector database, oftentimes you’ll store and query data in its raw form, rather than uploading embeddings themselves. Internally, the vector database needs to know how to convert your data to embeddings, and you have to specify an embedding function for this. For text, you can use the embedding functions available in the SentenceTransformers library or any other function that maps raw text to vectors.\n",
    "* **Similarity metric**: To assess embedding similarity, you need a similarity metric like cosine similarity, the dot product, or Euclidean distance. As you learned previously, cosine similarity is a popular choice, but choosing the right similarity metric depends on your application.\n",
    "* **Indexing**: When you’re dealing with a large number of embeddings, comparing a query embedding to every embedding stored in the database is often too slow. To overcome this, vector databases employ indexing algorithms that group similar embeddings together.\n",
    "\n",
    "<b>At query time, the query embedding is compared to a smaller subset of embeddings based on the index</b>. Because the embeddings recommended by the index aren’t guaranteed to have the highest similarity to the query, this is called approximate nearest neighbor search.\n",
    "* **Metadata**: You can store metadata with each embedding to help give context and make query results more precise. You can filter your embedding searches on metadata much like you would in a relational database. For example, you could store the year that a document was published as metadata and only look for similar documents that were published in a given year.\n",
    "* **Storage location**: With any kind of database, you need a place to store the data. Vector databases can store embeddings and metadata both in memory and on disk. Keeping data in memory allows for faster reads and writes, while writing to disk is important for persistent storage.\n",
    "* **CRUD operations**: Most vector databases support create, read, update, and delete (CRUD) operations. This means you can maintain and interact with data like you would in a relational database.\n",
    "\n",
    "There’s a whole lot more detail and complexity that you could explore with vector databases, but these core concepts should be enough to get you going. **Next up, you’ll get your hands dirty with ChromaDB, one of the most popular and user-friendly vector databases around.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b25a30-13ed-47bf-bd27-b4df93596fca",
   "metadata": {},
   "source": [
    "<a id='sect3_2'></a>\n",
    "### <b><font color='darkgreen'>Meet ChromaDB for LLM Applications</font></b> ([back](#sect3))\n",
    "<b><font size='3ptx'>[ChromaDB](https://docs.trychroma.com/) is an open-source vector database designed specifically for LLM applications.</font></b>\n",
    "\n",
    "<b>ChromaDB offers you both a user-friendly API and impressive performance, making it a great choice for many embedding applications</b>. To get started, activate your virtual environment and run the following command:\n",
    "```shell\n",
    "(venv) $ python -m pip install chromadb\n",
    "```\n",
    "\n",
    "If you have any issues installing ChromaDB, take a look at the [**troubleshooting guide**](https://docs.trychroma.com/troubleshooting#build-error-when-running-pip-install-chromadb) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5f984-8bbd-4b68-abef-f483169b2bad",
   "metadata": {},
   "source": [
    "Because you have a grasp on [**vectors**](https://realpython.com/chromadb-vector-database/#represent-data-as-vectors) and [**embeddings**](#sect2), and you understand the motivation behind vector databases, the best way to get started is with an example. For this example, you’ll store ten documents to search over. To illustrate the power of embeddings and semantic search, each document covers a different topic, and you’ll see how well ChromaDB associates your queries with similar documents.\n",
    "\n",
    "You’ll start by importing dependencies, defining configuration variables, and creating a ChromaDB client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c97bd3e-b403-4729-98d3-9383053c982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28bbf687-205e-426b-8f39-6d0b4366a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3525525b-1d24-489f-9d77-89f7856b2474",
   "metadata": {},
   "source": [
    "Next, you instantiate your embedding function and the ChromaDB collection to store your documents in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26a5ff0f-4940-4c9d-a17d-4b3b17054227",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_registered = False\n",
    "if not collections:\n",
    "    embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=EMBED_MODEL\n",
    "    )\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_func,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "else:\n",
    "    has_registered = True\n",
    "    collection = collections[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4faeea-b441-4922-98ca-5930839d4710",
   "metadata": {},
   "source": [
    "You specify an embedding function from the SentenceTransformers library. ChromaDB will use this to embed all your documents and queries. In this example, you’ll continue using the \"`all-MiniLM-L6-v2`\" model. You then create your first collection.\n",
    "\n",
    "<b>A collection is the object that stores your embedded documents along with any associated metadata. If you’re familiar with relational databases, then you can think of a collection as a table</b>. In this example, your collection is named `demo_docs`, it uses the \"`all-MiniLM-L6-v2`\" embedding function that you instantiated, and it uses the cosine similarity distance function as specified by `metadata={\"hnsw:space\": \"cosine\"}`.\n",
    "\n",
    "The last step in setting up your collection is to add documents and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a7067c8-0618-4a83-b5a5-23bd24bf9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "     \"The latest iPhone model comes with impressive features and a powerful camera.\",\n",
    "     \"Exploring the beautiful beaches and vibrant culture of Bali is a dream for many travelers.\",\n",
    "     \"Einstein's theory of relativity revolutionized our understanding of space and time.\",\n",
    "     \"Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens.\",\n",
    "     \"The American Revolution had a profound impact on the birth of the United States as a nation.\",\n",
    "     \"Regular exercise and a balanced diet are essential for maintaining good physical health.\",\n",
    "     \"Leonardo da Vinci's Mona Lisa is considered one of the most iconic paintings in art history.\",\n",
    "     \"Climate change poses a significant threat to the planet's ecosystems and biodiversity.\",\n",
    "     \"Startup companies often face challenges in securing funding and scaling their operations.\",\n",
    "     \"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\",\n",
    "]\n",
    "\n",
    "genres = [\n",
    "     \"technology\",\n",
    "     \"travel\",\n",
    "     \"science\",\n",
    "     \"food\",\n",
    "     \"history\",\n",
    "     \"fitness\",\n",
    "     \"art\",\n",
    "     \"climate change\",\n",
    "     \"business\",\n",
    "     \"music\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1a35830-9d1b-4f0a-91f4-ff111828cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not has_registered:\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "         ids=[f\"id{i}\" for i in range(len(documents))],\n",
    "         metadatas=[{\"genre\": g} for g in genres]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d735e9-c792-471f-bc2e-299979d28176",
   "metadata": {},
   "source": [
    "<b>The `metadatas` argument is optional, but most of the time, it’s useful to store metadata with your embeddings</b>. In this case, you define a single metadata field, \"`genre`\", that records the genre of each document. When you query a document, metadata provides you with additional information that can be helpful to better understand the document’s contents. You can also filter on metadata fields, just like you would in a relational database query.\n",
    "\n",
    "With documents embedded and stored in a collection, you’re ready to run some semantic queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cae4a64-3a19-4c48-b2b9-099d35eee2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Find me some delicious food!\"],\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ade6c2a-aedd-4673-8d22-523fbecd6e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f9ca7c2-6abb-46e1-bbfe-ee926c99d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens.']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bf3fda5-cc3f-4f3c-9d9d-d77dc07d296b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id3']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24a312d9-c73c-4201-bcd1-32fe0229448b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7638262433196752]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0267a137-7db6-4228-9bb9-85db98e653fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'genre': 'food'}]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40685a-ebd2-4562-93db-aba93a267e88",
   "metadata": {},
   "source": [
    "In this example, you query the `demo_docs` collection for documents that are most similar to the sentence `Find me some delicious food`!. You accomplish this using <font color='blue'>collection.query()</font>, where you pass your queries in `query_texts` and specify the number of similar documents to find with `n_results`. In this case, you only asked for the single document that’s most similar to your query.\n",
    "\n",
    "The results returned by <font color='blue'>collection.query()</font> are stored in a dictionary with the keys `ids`, `distances`, `metadatas`, `embeddings`, and `documents`. This is the same information that you added to your collection at the beginning, but it’s filtered down to match your query. In other words, <font color='blue'>collection.query()</font> returns all of the stored information about documents that are most similar to your query.\n",
    "\n",
    "As you can see, the embedding for `Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens` was most similar to the query `Find me some delicious food`. You probably agree that this document is the closest match. You can also see the ID, metadata, and distance associated with the matching document embedding. Here, you’re using **cosine distance**, which is one minus the cosine similarity between two embeddings.\n",
    "\n",
    "With <font color='blue'>collection.query()</font>, you’re not limited to single queries or single results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c7b12fd-b0f8-464c-822d-70f0224ad3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Teach me about history\",\n",
    "                 \"What's going on in the world?\"],\n",
    "    include=[\"documents\", \"distances\"],\n",
    "    n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a60e78d5-268c-413a-aae5-0a19790cc5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The American Revolution had a profound impact on the birth of the United States as a nation.',\n",
       " \"Leonardo da Vinci's Mona Lisa is considered one of the most iconic paintings in art history.\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b893499-8e86-43a1-a452-0df743d3c200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6904192684086343, 0.8771599658375056]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db15dc75-d804-4101-aa03-0862a5396cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Climate change poses a significant threat to the planet's ecosystems and biodiversity.\",\n",
       " 'The American Revolution had a profound impact on the birth of the United States as a nation.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "249a5125-0296-41f7-9e18-b38dd697a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.800294367638156, 0.9402921048592136]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a6d95-bf58-40e4-890f-36c32a812fa3",
   "metadata": {},
   "source": [
    "For this query, the two most similar documents weren’t as strong of a match as in the first query. Recall that cosine distance is one minus cosine similarity, so a cosine distance of 0.80 corresponds to a cosine similarity of 0.20.\n",
    "\n",
    "<b><font color='darkred'>Note: </font></b>\n",
    "> <b>Keep in mind that so-called similar documents returned from a semantic search over embeddings may not actually be relevant to the task that you’re trying to solve</b>. The success of a semantic search is somewhat subjective, and you or your stakeholders might not agree on the quality of the results.\n",
    "> <br/><br/>\n",
    "> <b>If there are no relevant documents in your collection for a given query, or your embedding algorithm wasn’t trained on the right or enough data, then your results might be poor</b>. It’s up to you to understand your application, your stakeholders’ expectations, and the limitations of your embedding algorithm and document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57170c2e-135d-4745-ba11-07de94780404",
   "metadata": {},
   "source": [
    "<b>Another awesome feature of ChromaDB is the ability to filter queries on metadata</b>. To motivate this, suppose you want to find the single document that’s most related to music history. You might run this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aab6179f-cf8d-4b62-93ae-d2d7deb1922a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id9']],\n",
       " 'distances': [[0.8186328397223677]],\n",
       " 'metadatas': [[{'genre': 'music'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce9cf5-5d6e-4ac8-b5e5-7695e22b38b3",
   "metadata": {},
   "source": [
    "Your query is `Teach me about music history`, and the most similar document is `Einstein’s theory of relativity revolutionized our understanding of space and time`. While Einstein is a historical figure who was a musician and teacher, this isn’t quite the result that you’re looking for. Because you’re particularly interested in music history, you can filter on the \"genre\" metadata field to search over more relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0c6acaa-397a-4c58-8c1b-018baf9fe54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id9']],\n",
       " 'distances': [[0.8186328397223677]],\n",
       " 'metadatas': [[{'genre': 'music'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    where={\"genre\": {\"$eq\": \"music\"}},\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac835e6-15df-44bc-ae20-5702087619e3",
   "metadata": {},
   "source": [
    "As you can see, the document about Beethoven’s Symphony No. 9 is the most similar document. Of course, for this example, there’s only one document with the music genre. o make it slightly more difficult, you could filter on both history and music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8faffa2-6fa9-4a59-a18f-0e057dc1bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    where={\"genre\": {\"$in\": [\"music\", \"history\"]}},\n",
    "    n_results=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b9f67bd-fdd3-4216-9de9-2572003b68ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\",\n",
       "  'The American Revolution had a profound impact on the birth of the United States as a nation.']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e24c239-8fa2-4105-95ea-0d943c84f008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8186328397223677, 0.8200413907242148]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7672d27-49ed-4a25-b589-ca5137ab7fcf",
   "metadata": {},
   "source": [
    "This query filters the collection of documents that have either a `music` or `history` genre, as specified by `where={\"genre\": {\"$in\": [\"music\", \"history\"]}}`. As you can see, the Beethoven document is still the most similar, while the American Revolution document is a close second. These were straightforward filtering examples on a single metadata field, but ChromaDB also supports [other filtering operations](https://docs.trychroma.com/usage-guide#:~:text=Filtering%20metadata%20supports%20the%20following%20operators%3A) that you might need.\n",
    "\n",
    "<b>If you want to update existing documents, embeddings, or metadata, then you can use <font color='blue'>collection.update()</font>.</b> This requires you to know the IDs of the data that you want to update. In this example, you’ll update both the documents and metadata for \"`id1`\" and \"`id2`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d656f79f-8199-4fa6-9dfe-73969f9d52a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n",
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "collection.update(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    documents=[\n",
    "        \"The new iPhone is awesome!\",\n",
    "        \"Bali has beautiful beaches\"],\n",
    "    metadatas=[{\"genre\": \"tech\"}, {\"genre\": \"beaches\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16e2aa83-3333-4eb6-b38c-8e02f14f50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.get(ids=[\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccc85d74-f5be-4b40-924e-ee85fc44bd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82e2db3-9b73-4efd-8bfd-be8b35103598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb6fc7-44a8-4771-ae65-c599e096c782",
   "metadata": {},
   "source": [
    "Lastly, <b>if you want to delete any items in the collection, then you can use <font color='blue'>collection.delete()</font></b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9ca8ae-e05e-4d5b-b91f-ead0392d46ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deletion, we have 8 document(s)!\n"
     ]
    }
   ],
   "source": [
    "print(f'Before deletion, we have {collection.count()} document(s)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fcd5137-2ce1-4e76-b006-78e081f39a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n",
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "collection.delete(ids=[\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51d916b0-6637-47ae-911d-1fffa0213d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get([\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c027cd4c-8cc5-4e1a-b5ad-efc9310b972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deletion, we have 8 document(s)!\n"
     ]
    }
   ],
   "source": [
    "print(f'After deletion, we have {collection.count()} document(s)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02dc2c-82c9-458e-83f2-26ef22fddedc",
   "metadata": {},
   "source": [
    "You’ve now seen many of ChromaDB’s main features, and you can learn more with the [getting started guide](https://docs.trychroma.com/getting-started) or [API cheat sheet](https://docs.trychroma.com/api-reference). You used a collection of ten hand-crafted documents that allowed you to get familiar with ChromaDB’s syntax and querying functionality, but this was by no means a realistic use case. <b>In the next section, you’ll see ChromaDB shine while you embed and query over thousands of real-world documents</b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c82e9-40c4-40cf-8549-44e54b1871ee",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <b><font color='darkblue'>Practical Example: Add Context for a Large Language Model (LLM)</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>Vector databases are capable of storing all types of embeddings, such as text, audio, and images. However, as you’ve learned, ChromaDB was initially designed with text embeddings in mind, and it’s most often used to build LLM applications. </font></b>\n",
    "* <b><a href='#sect4_1'>Prepare and Inspect Your Dataset</a></b>\n",
    "* <b><a href='#sect4_2'>Create a Collection and Add Reviews</a></b>\n",
    "* <b><a href='#sect4_3'>Connect to an LLM Service</a></b>\n",
    "* <b><a href='#sect4_4'>Provide Context to the LLM</a></b>\n",
    "\n",
    "<b>In this section, you’ll get hands-on experience using ChromaDB to provide context to OpenAI’s ChatGPT LLM.</b> To set the scene, you’re a data scientist who works for a large car dealership. The dealership has sold hundreds of thousands of cars and received many reviews. Your stakeholders would like you to create a system that summarizes different types of car reviews. They’ll use these summaries to improve marketing and prevent poor customer experiences in the future.\n",
    "\n",
    "You’re responsible for designing and implementing the back-end logic that creates these summaries. You’ll take the following steps:\n",
    "1. Create a ChromaDB collection that stores car reviews along with associated metadata.\n",
    "2. Create a system that accepts a query, finds semantically similar documents, and uses the similar documents as context to an LLM. The LLM will use the documents to answer the question posed in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bad616-624b-4976-acdd-41fd7cc5227d",
   "metadata": {},
   "source": [
    "<b>This process of retrieving relevant documents and using them as context for a generative model is known as <font color='darkblue'>retrieval-augmented generation</font> (RAG)</b>. This allows LLMs to make inferences using information that wasn’t included in their training dataset, and this is the most common way to apply ChromaDB in LLM applications.\n",
    "\n",
    "There are lots of factors and variations to consider when implementing a RAG system, but for this example, you’ll only need to know the fundamentals. Here’s what a RAG system might look like with ChromaDB:\n",
    "![RAG diagram](images/rag_diagram.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254e716-18fa-48b3-8b1a-db111cbc0baa",
   "metadata": {},
   "source": [
    "You first embed and store your documents in a ChromaDB collection. In this example, those documents are car reviews. You then run a query like `find and summarize the best car reviews` through ChromaDB to find semantically relevant documents, and you pass the query and relevant documents to an LLM to generate a context-informed response.\n",
    "\n",
    "<b>The key here is that the LLM takes both the original query and the relevant documents as input, allowing it to generate a meaningful response that it wouldn’t be able to create without the documents</b>.\n",
    "\n",
    "In reality, your deliverable for this project would likely be a [**chatbot**](https://realpython.com/build-a-chatbot-python-chatterbot/) that stakeholders use to ask questions about car reviews through a user interface. While building a full-fledged chatbot is beyond the scope of this tutorial, you can check out libraries like [**LangChain**](https://python.langchain.com/docs/get_started/introduction) that are designed specifically to help you assemble LLM applications.\n",
    "\n",
    "<b>The focus of this example is for you to see how you can use ChromaDB for RAG</b>. This practical knowledge will help reduce the learning curve for [**LangChain**](https://python.langchain.com/docs/get_started/introduction) if you choose to go that route in the future. With that, you’re ready to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e5598-c92e-4c5d-bd9c-4fb57d1217bf",
   "metadata": {},
   "source": [
    "<a id='sect4_1'></a>\n",
    "### <b><font color='darkgreen'>Prepare and Inspect Your Dataset</font></b> ([back](#sect4))\n",
    "<b><font size='3ptx'>You’ll use the Edmunds-Consumer Car Ratings and Reviews dataset from Kaggle to create the review collection. This dataset contains over 200,000 reviews and ratings covering 62 major car brands.</font></b>\n",
    "\n",
    "Once you’ve downloaded the dataset, unzip the file and store the data in your project directory inside a subdirectory called <font color='olive'>data/</font>. There’s one CSV file per car, and you should store all of them within <font color='olive'>data/archive/</font>.\n",
    "\n",
    "To start, you can take a look at the dataset using Polars, a popular DataFrame library. Make sure that you have Polars installed in your environment:\n",
    "```shell\n",
    "(venv) $ python -m pip install polars\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286f14c-cf61-48ad-bb48-adffcaf4248a",
   "metadata": {},
   "source": [
    "The focus of this tutorial isn’t on Polars, so you won’t get a detailed explanation of the Polars code. If you’re interested in learning more about Polars, then check out this [**Polars tutorial**](https://realpython.com/polars-python/).\n",
    "\n",
    "Here’s a function that you can use to prepare the car reviews dataset for ChromaDB:\n",
    "- `car_data_etl.py`\n",
    "\n",
    "```python\n",
    "import pathlib\n",
    "import polars as pl\n",
    "\n",
    "def prepare_car_reviews_data(data_path: pathlib.Path, vehicle_years: list[int] = [2017]):\n",
    "  \"\"\"Prepare the car reviews dataset for ChromaDB\"\"\"\n",
    "\n",
    "  # Define the schema to ensure proper data types are enforced\n",
    "  dtypes = {\n",
    "      \"\": pl.Int64,\n",
    "      \"Review_Date\": pl.Utf8,\n",
    "      \"Author_Name\": pl.Utf8,\n",
    "      \"Vehicle_Title\": pl.Utf8,\n",
    "      \"Review_Title\": pl.Utf8,\n",
    "      \"Review\": pl.Utf8,\n",
    "      \"Rating\": pl.Float64,\n",
    "  }\n",
    "\n",
    "  # Scan the car reviews dataset(s)\n",
    "  car_reviews = pl.scan_csv(data_path, dtypes=dtypes)\n",
    "  # Extract the vehicle title and year as new columns\n",
    "  # Filter on selected years\n",
    "  car_review_db_data = (\n",
    "      car_reviews.with_columns(\n",
    "          [\n",
    "              (pl.col(\"Vehicle_Title\").str.split(\n",
    "                      by=\" \").list.get(0).cast(pl.Int64)\n",
    "              ).alias(\"Vehicle_Year\"),\n",
    "              (pl.col(\"Vehicle_Title\").str.split(by=\" \").list.get(1)).alias(\n",
    "                  \"Vehicle_Model\"\n",
    "              ),\n",
    "          ]\n",
    "      ).filter(pl.col(\"Vehicle_Year\").is_in(vehicle_years))\n",
    "       .select([\"Review_Title\", \"Review\", \"Rating\", \"Vehicle_Year\", \"Vehicle_Model\"])\n",
    "       .sort([\"Vehicle_Model\", \"Rating\"])\n",
    "       .collect()\n",
    "  )\n",
    "\n",
    "  # Create ids, documents, and metadatas data in the format chromadb expects\n",
    "  ids = [f\"review{i}\" for i in range(car_review_db_data.shape[0])]\n",
    "  documents = car_review_db_data[\"Review\"].to_list()\n",
    "  metadatas = car_review_db_data.drop(\"Review\").to_dicts()\n",
    "\n",
    "  return {\"ids\": ids, \"documents\": documents, \"metadatas\": metadatas}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108296d7-66c6-42bc-ab38-41b410e63f48",
   "metadata": {},
   "source": [
    "In your <font color='olive'>car_data_etl.py</font> script, <font color='blue'>prepare_car_reviews_data()</font> accepts the path to the car reviews dataset and a list of vehicle years to filter on, and it returns a dictionary with the review data properly formatted for ChromaDB. You can include different vehicle years, but keep in mind that the more years you include, the longer it’ll take to build the collection. By default, you’re only including vehicles from 2017.\n",
    "\n",
    "You can see this function in action with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcb8ee18-4f54-49fa-a8e5-b1fdf3b163fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from car_data_etl import prepare_car_reviews_data\n",
    "\n",
    "DATA_PATH = \"data/archive/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "477fc577-21e2-4606-abeb-a5348ce1345c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'documents', 'metadatas'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_car_reviews_dict = prepare_car_reviews_data(DATA_PATH)\n",
    "chroma_car_reviews_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5937513c-fd8e-48a8-99ea-51390e96129f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'review5860'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_car_reviews_dict[\"ids\"][-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "129895d8-8ad7-4340-8e58-1e0852bb818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I've never had a perfect car for me but this is quite close. My husband refused to ever drive an SU\n"
     ]
    }
   ],
   "source": [
    "print(chroma_car_reviews_dict[\"documents\"][-10][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce6b798d-85ab-454d-af6e-5bf04cb20667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Review_Title': 'Very happy!',\n",
       " 'Rating': 5.0,\n",
       " 'Vehicle_Year': 2017,\n",
       " 'Vehicle_Model': 'Volvo'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_car_reviews_dict[\"metadatas\"][-10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f864eab-adda-4595-ab85-1ad133d03630",
   "metadata": {},
   "source": [
    "In this block, you import <font color='blue'>prepare_car_reviews_data()</font> from <font color='olive'>car_data_etl.py</font>, store the path to the raw review CSV datasets, and create `chroma_car_reviews_dict`, which stores the reviews in a ChromaDB-compatible format. You then display the ID, document text, and metadata associated with one of the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4682c46-f501-463d-87dd-896a41481a28",
   "metadata": {},
   "source": [
    "<a id='sect4_2'></a>\n",
    "### <b><font color='darkgreen'>Create a Collection and Add Reviews</font></b> ([back](#sect4))\n",
    "<b><font size='3ptx'>Next, you’ll create a collection and add the reviews.</font></b>\n",
    "\n",
    "This function will help you create a collection in a modular way. Before running this function, make sure you’ve installed [**more-itertools**](https://pypi.org/project/more-itertools/):\n",
    "```shell\n",
    "(venv) $ python -m pip install more-itertools\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57baf95-3e1e-49ad-88f8-d297216b7694",
   "metadata": {},
   "source": [
    "If you’re using Python 3.12 or higher, then you can use <font color='blue'>itertools.batched()</font> to accomplish the same task as below. However, using [**more-itertools**](https://pypi.org/project/more-itertools/) means that your code is fully backward compatible:\n",
    "- `chroma_utils.py`\n",
    "\n",
    "```python\n",
    "import pathlib\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from more_itertools import batched\n",
    "\n",
    "\n",
    "def build_chroma_collection(\n",
    "    chroma_path: pathlib.Path,\n",
    "    collection_name: str,\n",
    "    embedding_func_name: str,\n",
    "    ids: list[str],\n",
    "    documents: list[str],\n",
    "    metadatas: list[dict],\n",
    "    distance_func_name: str = \"cosine\"):\n",
    "  \"\"\"Create a ChromaDB collection\"\"\"\n",
    "  chroma_client = chromadb.PersistentClient(chroma_path)\n",
    "\n",
    "  embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "      model_name=embedding_func_name)\n",
    "\n",
    "  collection = chroma_client.create_collection(\n",
    "      name=collection_name,\n",
    "      embedding_function=embedding_func,\n",
    "      metadata={\"hnsw:space\": distance_func_name})\n",
    "\n",
    "  document_indices = list(range(len(documents)))\n",
    "\n",
    "  for batch in batched(document_indices, 166):\n",
    "    start_idx = batch[0]\n",
    "    end_idx = batch[-1]\n",
    "\n",
    "    collection.add(\n",
    "        ids=ids[start_idx:end_idx],\n",
    "        documents=documents[start_idx:end_idx],\n",
    "        metadatas=metadatas[start_idx:end_idx])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24545c28-8501-43a2-9133-6702d58d9bf3",
   "metadata": {},
   "source": [
    "Calling <font color='blue'>batched(document_indices, 166)</font> breaks document_indices into a list of tuples, each with size 166. <b>ChromaDB’s current maximum batch size is 166, but this might change in the future</b>. You can now create the collection that stores car reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52919926-41da-47c9-ae6d-89a22da6531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from car_data_etl import prepare_car_reviews_data\n",
    "from chroma_utils import build_chroma_collection\n",
    "\n",
    "DATA_PATH = \"data/archive/*\"\n",
    "CHROMA_PATH = \"car_review_embeddings\"\n",
    "EMBEDDING_FUNC_NAME = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "COLLECTION_NAME = \"car_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79f9af32-38d2-4bb0-afd3-3a5a84412205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 478 ms, sys: 79.7 ms, total: 558 ms\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "chroma_car_reviews_dict = prepare_car_reviews_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d596703-d310-4317-9f0a-61fd0ce95907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 13s, sys: 3min 32s, total: 18min 45s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "build_chroma_collection(\n",
    "    CHROMA_PATH,\n",
    "    COLLECTION_NAME,\n",
    "    EMBEDDING_FUNC_NAME,\n",
    "    chroma_car_reviews_dict[\"ids\"],\n",
    "    chroma_car_reviews_dict[\"documents\"],\n",
    "    chroma_car_reviews_dict[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f91eb-8c77-4aed-92d7-87d629a43c70",
   "metadata": {},
   "source": [
    "Building the collection will take a few minutes, but once it completes, you can run queries like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5eb3b9-afda-4150-9645-b0b74b27a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "177343b9-5a24-42d6-b483-fd724a46356c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_FUNC_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ce42c9-2a2a-430d-b32b-b719289b2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\n",
    "    name=COLLECTION_NAME, embedding_function=embedding_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcaf4255-5915-43b4-9334-6589145104b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "great_reviews = collection.query(\n",
    "    query_texts=[\"Find me some positive reviews that discuss the car's performance\"],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"distances\", \"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a99f274-ddcb-497d-9218-878750ce9ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Great all around car with great balance of performance and comfort. Terrific technology too.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "great_reviews[\"documents\"][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335421e-9215-4468-a58f-e722fb8a7bf2",
   "metadata": {},
   "source": [
    "You query the `car_reviews` collection with Find me some positive reviews that discuss the car’s performance, and you display the most similar result. All of your reviews are now embedded, and you’re ready to integrate them into the summarization application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77cf58e-3cb9-48bd-970d-0682eca1cba8",
   "metadata": {},
   "source": [
    "<a id='sect4_3'></a>\n",
    "### <b><font color='darkgreen'>Connect to an LLM Service</font></b> ([back](#sect4))\n",
    "<b><font size='3ptx'>As you know, you’re going to use the car reviews as context to an LLM.</font></b>\n",
    "\n",
    "This means that you’ll ask the LLM a question like `How would you summarize the most common complaints from negative car reviews?`, and you’ll provide relevant reviews to help the LLM answer this question. To do this, you’ll first need to install the [**openai**](https://github.com/openai/openai-python) library:\n",
    "```shell\n",
    "(venv) $ python -m pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc1198-0f62-412c-b986-ef3554f10dcb",
   "metadata": {},
   "source": [
    "You need an API key to interact with the models in the openai library, and you can check out [this tutorial](https://realpython.com/generate-images-with-dalle-openai-api/#get-your-openai-api-key) to help you get set up. Once you have your API key, you can store it as an environment variable or add it to a configuration file like this [**JSON**](https://realpython.com/python-json/) file that you could name <font color='olive'>config.json</font>:\n",
    "```json\n",
    "{\n",
    "    \"openai-secret-key\": \"<your-api-key>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792083cf-38cb-440e-a997-007d127d57c5",
   "metadata": {},
   "source": [
    "To make sure your API works and everything is running properly, you can run the following code, which will ask the LLM a question without considering any of the documents in your ChromaDB collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba56daa-eaa9-460f-877a-a346db24d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "with open(\"config.json\", mode=\"r\") as json_file:\n",
    "    config_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e1f1fb-9105-4704-a926-30b5fd16c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=config_data.get(\"openai-secret-key\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc2c3cc3-4353-4afe-a7ec-c8cd2584f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"You are a customer success employee at a large car dealership.\"\n",
    "question = \"What's the key to great customer satisfaction?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92916eee-f102-4a17-b1d0-7223762db6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01d30847-1a43-4478-81bd-218d2cc11fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The key to great customer satisfaction is providing excellent customer service. This includes being attentive to customers' needs, being knowledgeable about the products or services you offer, being responsive to their inquiries or concerns, and going above and beyond to ensure they have a positive experience. Building trust and rapport with customers is also crucial in creating a lasting relationship and ensuring their satisfaction.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db0964-583b-42b2-abfe-d2c22b70b67d",
   "metadata": {},
   "source": [
    "In this block, you import `os`, `json`, and `openai` and set the <font color='orange'>TOKENIZERS_PARALLELISM</font> environment variable to \"false\". Setting this environment variable to \"false\" will suppress a warning related to [**huggingface tokenizers**](https://huggingface.co/docs/transformers/main_classes/tokenizer). You then load the JSON object that stores your OpenAI API key.\n",
    "\n",
    "The context message, `You are a customer success employee at a large car dealership`, helps set the behavior of the LLM so that its responses are more likely to have a desired tone. This type of message is also sometimes called a [**role prompt**](https://realpython.com/practical-prompt-engineering/#add-a-role-prompt-to-set-the-tone). <b>The user message, What’s the key to great customer satisfaction?, is the actual question or task that you want the LLM to respond to</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6319193e-aeed-4129-9ad7-fbda7342de75",
   "metadata": {},
   "source": [
    "<a id='sect4_4'></a>\n",
    "### <b><font color='darkgreen'>Provide Context to the LLM</font></b> ([back](#sect4))\n",
    "<b><font size='3ptx'>As you can see, the LLM gives you a fairly generic description of what it takes to promote customer satisfaction. None of this information is particularly useful to you because it isn’t specific to your car dealership. </font></b>\n",
    "\n",
    "To make this response more tailored to your business, you need to provide the LLM with some reviews as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0faca022-fbc8-4a88-915d-1a0eaa997d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_format = \"\"\"\n",
    "You are a customer success employee at a large\n",
    "car dealership. Use the following car reviews\n",
    "to answer questions: {}\n",
    "\"\"\"\n",
    "\n",
    "question_format = \"\"\"\n",
    "{}\n",
    "\n",
    "Please give a short description coming after with the bullet point list as summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "170b2793-0cdc-4ff7-94e7-1cc3dec3f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=10,\n",
    "    include=[\"documents\"],\n",
    "    where={\"Rating\": {\"$gte\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4adb6a7-4ed5-4a5b-a953-edc8dcc7e6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Great value, awesome reliability, Very  pleased! Great deal! So far so good!, Excellent quality, technology, comfort and value. Way impressed with this vehicle., Things I love: styling, performance, large info center, smooth acceleration, no gas stations, low maintenance costs, incredible sound system, frunk and sub-trunk, rear cargo space, HOV lane, federal tax credit, summons feature, replacement parts seem nicely priced, my first Service Center experience was excellent, my first body shop experience was very good, many people think you are \"cool\" and environmentally friendly (and you are).          Things I wish were better: fit and finish of body parts (not up to premium car standard), auto pilot still not a wow...needs more development, blind spot monitor is below expectations for such an advance vehicle, difficulty getting in and out of front seat...if you are tall, the front seat goes behind the \"B\" pillar too far thus the entering and exiting is more difficult. Final Verdict. I love the car. I doubt I would ever buy an ICE car again., Good overall but a lot of others out there., Excellent choice, After all the facts and findings over all review is outstanding., Excellent features and performance!, High marks on comfort, options, materials and workmanship. Lacking a bit in performance and roominess. Overall, excellent!, Great buy! Nice amenities at a great value. We are extremely happy with the purchase!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_str = \",\".join(good_reviews[\"documents\"][0])\n",
    "reviews_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf6095cf-f2b6-4d27-ae23-0fc12d58149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_review_summaries = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context_format.format(reviews_str)},\n",
    "        {\"role\": \"user\", \"content\": question_format.format(\"What's the key to great customer satisfaction based on detailed positive reviews?\")},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbbdb4cf-395f-4faa-acd9-672f295ebad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key to great customer satisfaction, based on detailed positive reviews, is a combination of excellent quality, value, reliability, and customer service. Customers are highly satisfied when they feel they are getting a great deal on a high-quality product that meets or exceeds their expectations. Additionally, positive interactions with the service center and body shop contribute to a positive overall experience. Here is a summary of the key points from the reviews:\n",
      "\n",
      "- Great value\n",
      "- Awesome reliability\n",
      "- Excellent quality, technology, comfort, and value\n",
      "- Impressive features and performance\n",
      "- Low maintenance costs\n",
      "- Positive service center and body shop experiences\n",
      "- Environmentally friendly image\n",
      "- Overall outstanding review and satisfaction\n"
     ]
    }
   ],
   "source": [
    "print(good_review_summaries.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a59d6-1ec5-41e5-bc7f-fcbd918165db",
   "metadata": {},
   "source": [
    "As before, you import dependencies, define configuration variables, set your OpenAI API key, and load the `car_reviews collection`. You then define `context` and `question` variables that you’ll feed into an LLM for inference. The key difference in context is the `{}` at the end, which will be replaced with relevant reviews that give the LLM context to base its answers on.\n",
    "\n",
    "You then pass the question into <font color='blue'>collection.query()</font> and request ten reviews that are most related to the question. In this query, <font color='blue'>where={\"Rating\": {\"$gte\": 3}}</font> filters the collection to reviews that have a rating greater than or equal to 3. Lastly, you pass the comma-separated `review_str` into context and request an answer from \"`gpt-3.5-turbo`\".\n",
    "\n",
    "Notice how much more specific and detailed ChatGPT’s response is now that you’ve given it relevant car reviews as context. For example, if you look through the documents in good_reviews, then you’ll see reviews that mention smooth acceleration and federal tax credits, both of which are incorporated into the LLM’s response.\n",
    "\n",
    "<b><font color='darkred'>Note:</font></b>\n",
    "> <b>It’s a common misconception that setting <font color='blue'>temperature=0</font> guarantees deterministic responses from ChatGPT</b>. While responses are closer to deterministic when temperature=0, [there’s no guarantee](https://arxiv.org/pdf/2308.02828#:~:text=We%20study%20the%20influence%20of,contrary%20to%20many%20people's%20beliefs.) that you’ll get the same response for identical requests. Because of this, ChatGPT might output slightly different results than what you see in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be69b3-1a46-4320-8894-692d4e9960ef",
   "metadata": {},
   "source": [
    "Now, even though ChatGPT used relevant reviews to inform its response, you might still be thinking that the response was fairly generic. <b>To really see the power of using ChromaDB to provide ChatGPT with context, you can ask a question about a specific review</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f7ba536-ddf3-4f7f-8c6e-60909f2e8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_format = \"\"\"\n",
    "You are a customer success employee at a large\n",
    "car dealership. Use the following car reviews\n",
    "to answer questions: {}\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Which of these poor reviews has the worst implications about our dealership? Explain why.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4b39729-fc2c-4faf-94cc-2d8203a92be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_reviews = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=5,\n",
    "    include=[\"documents\"],\n",
    "    where={\"Rating\": {\"$lte\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2836ebf2-2a7e-4ea8-83be-12fc78388cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_str = \",\".join(poor_reviews[\"documents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d787143e-a721-442b-aec1-bd2d0cff11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_review_summaries = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context_format.format(reviews_str)},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23842ca4-576e-4406-b9ca-0b85f0b7b55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first review detailing unresolved electrical issues with the car has the worst implications about the dealership. This review mentions multiple serious issues with the vehicle, including problems with Bluetooth, backup camera, trunk, black screen, clock, and seatbelts not working. The customer expresses frustration and anger at the dealership for not being able to resolve these issues despite multiple visits. The review also mentions dropping off the car at the dealership and purchasing something else, indicating a loss of trust and loyalty towards the dealership. The customer's dissatisfaction with the quality of the vehicle and the dealership's inability to address the issues effectively reflect poorly on the dealership's service and customer satisfaction levels.\n"
     ]
    }
   ],
   "source": [
    "print(poor_review_summaries.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3dae8f-e796-4a49-926e-6d256a853cc9",
   "metadata": {},
   "source": [
    "In this example, you query the collection for five reviews that have the worst implications on the dealership, and you filter on reviews that have a rating less than or equal to 3. You then pass this question, along with the five relevant reviews, to ChatGPT.\n",
    "\n",
    "ChatGPT points to a specific review where a customer had a poor experience at the dealership, quoting the review directly. ChatGPT has no knowledge of this review without your providing it, and you may not have found this review without a vector database capable of accurate semantic search. This is the power that you unlock when combining vector databases with LLMs.\n",
    "\n",
    "<b>You’ve now seen why vector databases like ChromaDB are so useful for adding context to LLMs. In this example, you’ve scratched the surface of what you can create with ChromaDB, so just think about all the potential use cases for applications like this</b>. The LLM and vector database landscape will likely continue to evolve at a rapid pace, but you can now feel confident in your understanding of how the two technologies interplay with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a88d6-f858-409d-a960-eb8cadae1fa1",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Medium - Code Generation using Retrieval Augmented Generation + LangChain](https://medium.com/@rubenszimbres/code-generation-using-retrieval-augmented-generation-langchain-861e3c1a1a53)\n",
    "* [Chroma API Cheetsheet](https://docs.trychroma.com/api-reference)\n",
    "* [RealPython - Prompt Engineering: A Practical Example](https://realpython.com/practical-prompt-engineering/#add-a-role-prompt-to-set-the-tone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
