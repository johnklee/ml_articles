{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed38fb3-f04d-46b8-b686-875f9242f3e3",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://realpython.com/build-llm-rag-chatbot-with-langchain/)) <b><font size='3ptx'>You’ve likely interacted with large language models (LLMs), like the ones behind OpenAI’s ChatGPT, and experienced their remarkable ability to answer questions, summarize documents, write code, and much more. While LLMs are remarkable by themselves, with a little programming knowledge, you can leverage libraries like [LangChain](https://python.langchain.com/docs/get_started/introduction) to create your own LLM-powered chatbots that can do just about anything.</font></b>\n",
    "\n",
    "<b>In an enterprise setting, one of the most popular ways to create an LLM-powered chatbot is through [retrieval-augmented generation](https://www.promptingguide.ai/techniques/rag)</b> (<font color='brown'>RAG</font>). When you design a RAG system, you use a retrieval model to retrieve relevant information, usually from a database or corpus, and provide this retrieved information to an LLM to generate contextually relevant responses.\n",
    "\n",
    "<b>In this tutorial, you’ll step into the shoes of an AI engineer working for a large hospital system. You’ll build a RAG chatbot in LangChain that uses Neo4j to retrieve data about the patients, patient experiences, hospital locations, visits, insurance payers, and physicians in your hospital system</b>.\n",
    "\n",
    "In this tutorial, you’ll learn how to:\n",
    "* Use **LangChain** to build custom **chatbots**\n",
    "* Design a **chatbot using your understanding of the business requirements and hospital system data**.\n",
    "* Work with **graph databases**.\n",
    "* Set up a Neo4j AuraDB instance.\n",
    "* **Build a RAG chatbot** that retrieves both structured and unstructured data from Neo4j\n",
    "* **Deploy your chatbot** with **FastAPI** and **Streamlit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc63ff9-c1ce-4449-8fc9-aab7688b9322",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Prerequisites</font></b>\n",
    "This tutorial is best suited for intermediate Python developers who want to get hands-on experience creating custom chatbots. Aside from intermediate Python knowledge, you’ll benefit from having a high-level understanding of the following concepts and technologies:\n",
    "* Large language models (LLMs) and [prompt engineering](https://realpython.com/practical-prompt-engineering/).\n",
    "* [Text embeddings and vector databases](https://realpython.com/chromadb-vector-database/#represent-data-as-vectors).\n",
    "* [Graph databases](https://neo4j.com/developer/graph-database/) and [Neo4j](https://neo4j.com/docs/getting-started/languages-guides/neo4j-python/).\n",
    "* [The OpenAI developer ecosystem](https://openai.com/product).\n",
    "* [REST APIs](https://realpython.com/api-integration-in-python/) and [FastAPI](https://realpython.com/fastapi-python-web-apis/).\n",
    "* Asynchronous programming\n",
    "* Docker and Docker Compose\n",
    "\n",
    "Nothing listed above is a hard prerequisite, so don’t worry if you don’t feel knowledgeable in any of them. You’ll be introduced to each concept and technology along the way. Besides, there’s no better way to learn these prerequisites than to implement them yourself in this tutorial.\n",
    "\n",
    "Next up, you’ll get a brief project overview and begin learning about LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025bd10-0a81-49cc-8c23-6e4353668730",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Project Overview</font></b>\n",
    "Throughout this tutorial, you’ll create a few directories that make up your final chatbot. Here’s a breakdown of each directory:\n",
    "* **<font color='#556B2F'>langchain_intro/</font>** will help you get familiar with LangChain and equip you with the tools that you need to build the chatbot you saw in the demo, and it won’t be included in your final chatbot. You’ll cover this in [Step 1](#step1).\n",
    "* **<font color='#556B2F'>data/</font>** has the raw hospital system data stored as CSV files. You’ll explore this data in [Step 2](https://realpython.com/build-llm-rag-chatbot-with-langchain/#step-2-understand-the-business-requirements-and-data). In [Step 3](https://realpython.com/build-llm-rag-chatbot-with-langchain/#step-3-set-up-a-neo4j-graph-database), you’ll move this data into a Neo4j database that your chatbot will query to answer questions.\n",
    "* **<font color='#556B2F'>hospital_neo4j_etl/</font>** contains a script that loads the raw data from <font color='#556B2F'>data/</font> into your Neo4j database. You have to run this before building your chatbot, and you’ll learn everything you need to know about setting up a Neo4j instance in Step 3.\n",
    "* **<font color='#556B2F'>chatbot_api/</font>** is your [FastAPI](https://realpython.com/fastapi-python-web-apis/) app that serves your chatbot as a REST endpoint, and it’s the core deliverable of this project. The <font color='#556B2F'>`chatbot_api/src/agents/`</font> and <font color='#556B2F'>`chatbot_api/src/chains/`</font> subdirectories contain the LangChain objects that comprise your chatbot. You’ll learn what agents and chains are later, but for now, just know that your chatbot is actually a LangChain agent composed of chains and functions.\n",
    "* **<font color='#556B2F'>tests/</font>** includes two scripts that test how fast your chatbot can answer a series of questions. This will give you a feel for how much time you save by making asynchronous requests to LLM providers like OpenAI.\n",
    "* **<font color='#556B2F'>chatbot_frontend/</font>** is your Streamlit app that interacts with the chatbot endpoint in <font color='#556B2F'>chatbot_api/</font>. This is the UI that you saw in the demo, and you’ll build this in [Step 5](https://realpython.com/build-llm-rag-chatbot-with-langchain/#step-5-deploy-the-langchain-agent).\n",
    "\n",
    "All the environment variables needed to build and run your chatbot will be stored in a .env file. You’ll deploy the code in <font color='#556B2F'>hospital_neo4j_etl/</font>, `chatbot_api`, and `chatbot_frontend` as Docker containers that’ll be orchestrated with Docker Compose. If you want to experiment with the chatbot before going through the rest of this tutorial, then you can download the materials and follow the instructions in the README file to get things running:\n",
    "> Get Your Code: [Click here to download the free source code](https://realpython.com/bonus/build-llm-rag-chatbot-with-langchain-code/) for your LangChain chatbot.\n",
    "\n",
    "<br/>\n",
    "With the project overview and prerequisites behind you, you’re ready to get started with the first step—getting familiar with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346911e4-ef0d-4dd3-95c1-edd99a5e7612",
   "metadata": {},
   "source": [
    "<a id='agenda'></a>\n",
    "### <b><font color='darkgreen'>Agenda</font></b>\n",
    "* <font size='3ptx'><b><a href='#step1'>Step 1: Get Familiar With LangChain</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step2'>Step 2: Understand the Business Requirements and Data</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step3'>Step 3: Set Up a Neo4j Graph Database</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step4'>Step 4: Build a Graph RAG Chatbot in LangChain</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb58c6d-49dc-46e2-8e56-f99f89b9ee60",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## <b><font color='darkblue'>Step 1: Get Familiar With LangChain</font></b>\n",
    "* <font size='3ptx'><b><a href='#step1_1'>Chat Models</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step1_2'>Prompt Templates</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step1_3'>Chains and LangChain Expression Language (LCEL)</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step1_4'>Retrieval Objects</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step1_5'>Agents</a></b></font>\n",
    "\n",
    "<b><font size='3ptx'>Before you design and develop your chatbot, you need to know how to use [LangChain](https://github.com/langchain-ai/langchain). </font></b>\n",
    "\n",
    "<b>In this section, you’ll get to know LangChain’s main components and features by building a preliminary version of your hospital system chatbot</b>. This will give you all the necessary tools to build your full chatbot.\n",
    "\n",
    "Use your favorite code editor to create a new Python project, and be sure to <b>create a virtual environment for its dependencies. Make sure you have Python 3.10 or later installed. Activate your virtual environment and install the following libraries</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8ace76-c05d-484c-a10c-c56dbab80aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install langchain==0.1.0 openai==1.7.2 langchain-openai==0.0.2 langchain-community==0.0.12 langchainhub==0.1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3f7c8-1f21-4165-8338-8992f0a13aeb",
   "metadata": {},
   "source": [
    "You’ll also want to install [**python-dotenv**](https://pypi.org/project/python-dotenv/) to help you manage environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2192dc23-766d-4faa-a2c0-d6b87a05862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899d1e2-8562-4f78-b31a-78da4f0750a4",
   "metadata": {},
   "source": [
    "**[Python-dotenv](https://pypi.org/project/python-dotenv/) loads environment variables from `.env` files into your Python environment, and you’ll find this handy as you develop your chatbot**. However, you’ll eventually deploy your chatbot with Docker, which can handle environment variables for you, and you won’t need Python-dotenv anymore.\n",
    "\n",
    "If you haven’t already, you’ll need to download <font color='olive'>reviews.csv</font> from the materials or [GitHub repo](https://github.com/hfhoffman1144/langchain_neo4j_rag_app/blob/main/data/reviews.csv) for this tutorial:\n",
    "> Get Your Code: [Click here to download the free source code](https://realpython.com/bonus/build-llm-rag-chatbot-with-langchain-code/) for your LangChain chatbot.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, open the project directory and add the following folders and files:\n",
    "```\n",
    "./\n",
    "│\n",
    "├── data/\n",
    "│   └── reviews.csv\n",
    "│\n",
    "├── langchain_intro/\n",
    "│   ├── chatbot.py\n",
    "│   ├── create_retriever.py\n",
    "│   └── tools.py\n",
    "│\n",
    "└── .env\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The <font color='olive'>reviews.csv</font> file in <font color='olive'>data/</font> is the one you just downloaded, and the remaining files you see should be empty. <b>You’re now ready to get started building your first chatbot with LangChain!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e4289-6739-47a2-9e86-d29b47591c18",
   "metadata": {},
   "source": [
    "<a id='step1_1'></a>\n",
    "### <b><font color='darkgreen'>Chat Models</font></b>\n",
    "<b><font size='3ptx'>You might’ve guessed that the core component of LangChain is the [LLM](https://python.langchain.com/docs/modules/model_io/llms/). </font></b>\n",
    "\n",
    "<b>LangChain provides a modular interface for working with LLM providers such as OpenAI, Cohere, HuggingFace, Anthropic, Together AI, and others</b>. In most cases, all you need is an API key from the LLM provider to get started using the LLM with LangChain. LangChain also supports LLMs or other language models hosted on your own machine.\n",
    "\n",
    "<b>You’ll use OpenAI for this tutorial, but keep in mind there are many great open- and closed-source providers out there</b>. You can always test out different providers and optimize depending on your application’s needs and cost constraints. Before moving forward, make sure you’re signed up for an OpenAI account and you have a valid [**API key**](https://openai.com/product).\n",
    "\n",
    "Once you have your OpenAI API key, add it to your `.env` file:\n",
    "```\n",
    "OPENAI_API_KEY=<YOUR-OPENAI-API-KEY>\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "While you can interact directly with LLM objects in LangChain, a more common abstraction is the [**chat model**](https://python.langchain.com/docs/modules/model_io/chat/). <b>Chat models use LLMs under the hood, but they’re designed for conversations, and they interface with chat messages rather than [raw text](https://python.langchain.com/docs/modules/model_io/chat/quick_start#messages)</b>.\n",
    "\n",
    "<b>Using chat messages, you provide an LLM with additional detail about the kind of message you’re sending. All messages have `role` and `content` properties</b>. The role tells the LLM who is sending the message, and the content is the message itself. Here are the most commonly used messages:\n",
    "* [**HumanMessage**](https://python.langchain.com/docs/modules/model_io/concepts#humanmessage): A message from the user interacting with the language model.\n",
    "* [**AIMessage**](https://python.langchain.com/docs/modules/model_io/concepts#aimessage): A message from the language model.\n",
    "* [**SystemMessage**](https://python.langchain.com/docs/modules/model_io/concepts#systemmessage): A message that tells the language model how to behave. Not all providers support the [**SystemMessage**](https://python.langchain.com/docs/modules/model_io/concepts#systemmessage).\n",
    "\n",
    "There are other messages types, like [**FunctionMessage**](https://python.langchain.com/docs/modules/model_io/concepts#functionmessage) and [**ToolMessage**](https://python.langchain.com/docs/modules/model_io/concepts#toolmessage), but you’ll learn more about those when you build an [**agent**](https://python.langchain.com/docs/modules/agents/).\n",
    "\n",
    "Getting started with chat models in LangChain is straightforward. <b>To instantiate an OpenAI chat model, navigate to <font color='olive'>langchain_intro</font> and add the following code to <font color='olive'>chatbot.py</font></b>:\n",
    "```python\n",
    "\"\"\"Chat bot module.\"\"\"\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "You first <font color='blue'>import dotenv</font> and [**ChatOpenAI**](https://python.langchain.com/docs/integrations/chat/openai). Then you call <font color='blue'>dotenv.load_dotenv()</font> which reads and stores environment variables from <font color='olive'>.env</font>. By default, <font color='blue'>dotenv.load_dotenv()</font> assumes <font color='olive'>.env</font> is located in the current working directory, but you can pass the path to other directories if <font color='olive'>.env</font> is located elsewhere.\n",
    "\n",
    "**You then instantiate a [ChatOpenAI](https://python.langchain.com/docs/integrations/chat/openai) model using GPT 3.5 Turbo as the base LLM, and you set `temperature` to 0**. OpenAI offers a diversity of models with varying price points, capabilities, and performances. GPT 3.5 turbo is a great model to start with because it performs well in many use cases and is cheaper than more recent models like GPT 4 and beyond.\n",
    "\n",
    "<b><font color='darkred'>Note</font></b>: It’s a common misconception that setting `temperature=0` guarantees deterministic responses from GPT models. <b>While responses are closer to deterministic when `temperature=0`, there’s no guarantee that you’ll get the same response for identical requests</b>. Because of this, GPT models might output slightly different results than what you see in the examples throughout this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b261fbb-b0e3-4054-bbf3-5bb4467ab4f8",
   "metadata": {},
   "source": [
    "To use `chat_model`, open the project directory, start a Python interpreter, and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c50921-4884-44ac-9087-eebf22c225dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from langchain_intro.chat_bot import chat_model\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"\"\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"\"\"\n",
    "    ),\n",
    "    HumanMessage(content=\"What is Medicaid managed care?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b8d956-7709-4764-a643-75388d65c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Medicaid managed care is a system in which states contract with managed care organizations (MCOs) to provide healthcare services to Medicaid beneficiaries. These MCOs are responsible for coordinating and delivering healthcare services to enrollees in exchange for a fixed monthly payment per enrollee. Medicaid managed care aims to improve access to care, enhance quality, and control costs for Medicaid beneficiaries.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816a61e-0e6c-4a46-bbf8-89bd1334e61b",
   "metadata": {},
   "source": [
    "In this block, you import [**HumanMessage**](https://python.langchain.com/docs/modules/model_io/concepts#humanmessage) and [**SystemMessage**](https://python.langchain.com/docs/modules/model_io/concepts#systemmessage), as well as your chat model. You then define a list with a [**SystemMessage**](https://python.langchain.com/docs/modules/model_io/concepts#systemmessage) and a [**HumanMessage**](https://python.langchain.com/docs/modules/model_io/concepts#humanmessage) and run them through `chat_model` with <font color='blue'>chat_model.invoke()</font>. Under the hood, `chat_model` makes a request to an OpenAI endpoint serving gpt-3.5-turbo-0125, and the results are returned as an [**AIMessage**](https://python.langchain.com/docs/modules/model_io/concepts#aimessage).\n",
    "\n",
    "As you can see, the chat model answered `What is Medicaid managed care?` provided in the [**HumanMessage**](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html). You might be wondering what the chat model did with the [**SystemMessage**](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html) in this context. Notice what happens when you ask the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473abf61-2a82-476d-ad99-5289da5673c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"\"\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"\"\"),\n",
    "    HumanMessage(content=\"How do I change a tire?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e83d268-2ba2-4caf-bf0f-6df29d9dd6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm here to help with healthcare-related questions. If you have any health-related inquiries, feel free to ask!\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b24d83-916f-4ff5-a7d2-f497441a21f6",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>As described earlier, the [SystemMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html) tells the model how to behave</font></b>. In this case, you told the model to only answer healthcare-related questions. This is why it refuses to tell you how to change your tire. <b>The ability to control how an LLM relates to the user through text instructions is powerful, and this is the foundation for creating customized chatbots through prompt engineering</b>.\n",
    "\n",
    "While chat messages are a nice abstraction and are good for ensuring that you’re giving the LLM the right kind of message, <b>you can also pass raw strings into chat models</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d7ed885-35e5-4c97-943f-8ec3d83552b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Blood pressure is the force of blood pushing against the walls of the arteries as the heart pumps blood throughout the body. It is measured in millimeters of mercury (mmHg) and is typically recorded as two numbers: systolic pressure (the top number) and diastolic pressure (the bottom number). A normal blood pressure reading is typically around 120/80 mmHg. High blood pressure (hypertension) can increase the risk of heart disease, stroke, and other health problems, while low blood pressure (hypotension) can cause dizziness, fainting, and other symptoms.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"What is blood pressure?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e4e96-4621-4960-90e6-e7660d71d828",
   "metadata": {},
   "source": [
    "In this code block, you pass the string `What is blood pressure?` directly to <font color='blue'>chat_model.invoke()</font>. <b>If you want to control the LLM’s behavior without a [SystemMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html) here, you can include instructions in the string input</b>.\n",
    "\n",
    "<b><font color='darkred'>Note:</font></b>\n",
    "> In these examples, you used `.invoke()`, but LangChain has [other methods](https://python.langchain.com/docs/expression_language/interface) that interact with LLMs. For instance, `.stream()` returns the response one token at time, and `.batch()` accepts a list of messages that the LLM responds to in one call.\n",
    "> Each method also has an analogous asynchronous method. For instance, you can run `.invoke()` asynchronously with `ainvoke()`.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next up, you’ll learn a modular way to guide your model’s response, as you did with the [**SystemMessage**](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.system.SystemMessage.html), making it easier to customize your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08690b6c-2805-47b5-a150-e59c47d834b2",
   "metadata": {},
   "source": [
    "<a id='step1_2'></a>\n",
    "### <b><font color='darkgreen'>Prompt Templates</font></b>\n",
    "<b><font size='3ptx'>LangChain allows you to design modular prompts for your chatbot with [prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start).</font></b>\n",
    "\n",
    "Quoting LangChain’s documentation, <b>you can think of prompt templates as predefined recipes for generating prompts for language models</b>.\n",
    "\n",
    "Suppose you want to build a chatbot that answers questions about patient experiences from their reviews. Here’s what a prompt template might look like for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba033346-624c-4596-8b07-92655c1f4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "review_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a hospital.\n",
    "Use the following context to answer questions. Be as detailed\n",
    "as possible, but don't make up any information that's not\n",
    "from the context. If you don't know an answer, say you don't know.\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "review_template = ChatPromptTemplate.from_template(review_template_str)\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565d601f-a56a-4bd6-b5b6-c7cacc7aab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Your job is to use patient\n",
      "reviews to answer questions about their experience at a hospital.\n",
      "Use the following context to answer questions. Be as detailed\n",
      "as possible, but don't make up any information that's not\n",
      "from the context. If you don't know an answer, say you don't know.\n",
      "\n",
      "I had a great stay!\n",
      "\n",
      "Did anyone have a positive experience?\n"
     ]
    }
   ],
   "source": [
    "print(review_template.format(context=context, question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04068b78-a49a-421a-b0df-967211fef225",
   "metadata": {},
   "source": [
    "You first import [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) and define `review_template_str`, which contains the instructions that you’ll pass to the model, along with the variables `context` and `question` in [replacement fields](https://realpython.com/python-f-strings/#the-strformat-method) that LangChain delimits with curly braces (`{}`). You then create a [**ChatPromptTemplate**](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) object from `review_template_str` using the class method `.from_template()`.\n",
    "\n",
    "With `review_template` instantiated, you can pass `context` and `question` into the string template with <font color='blue'>review_template.format()</font>. The results may look like you’ve done nothing more than [standard Python string interpolation](https://realpython.com/python-f-strings/), but prompt templates have a lot of useful features that allow them to integrate with chat models.\n",
    "\n",
    "Notice how your previous call to <font color='blue'>review_template.format()</font> generated a string with `Human` at the beginning. <b>This is because <font color='blue'>**ChatPromptTemplate**.from_template()</font> assumes the string template is a human message by default. To change this, you can create more detailed prompt templates for each chat message that you want the model to process</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d4b3282-0c4d-4a81-bb0d-afde4c05817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> <class 'langchain_core.messages.system.SystemMessage'>:\n",
      "Your job is to use patient\n",
      "reviews to answer questions about their experience at a\n",
      "hospital. Use the following context to answer questions.\n",
      "Be as detailed as possible, but don't make up any information\n",
      "that's not from the context. If you don't know an answer, say\n",
      "you don't know.\n",
      "\n",
      "I had a great stay!\n",
      "\n",
      "\n",
      ">>> <class 'langchain_core.messages.human.HumanMessage'>:\n",
      "Did anyone have a positive experience?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "        PromptTemplate,\n",
    "        SystemMessagePromptTemplate,\n",
    "        HumanMessagePromptTemplate,\n",
    "        ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "review_system_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at a\n",
    "hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information\n",
    "that's not from the context. If you don't know an answer, say\n",
    "you don't know.\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"], template=review_system_template_str\n",
    "    )\n",
    ")\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"], template=\"{question}\"\n",
    ")\n",
    ")\n",
    "\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "review_prompt_template = ChatPromptTemplate(\n",
    "input_variables=[\"context\", \"question\"],\n",
    "    messages=messages,\n",
    ")\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\"\n",
    "\n",
    "for message in review_prompt_template.format_messages(context=context, question=question):\n",
    "    print(f'>>> {message.__class__}:\\n{message.content}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0adf57-d54d-4c75-917f-03d248a4b46a",
   "metadata": {},
   "source": [
    "To see how to combine chat models and prompt templates, you’ll build a chain with the [**LangChain Expression Language**](https://python.langchain.com/docs/expression_language/) (LCEL). <b>This helps you unlock LangChain’s core functionality of building modular customized interfaces over chat models</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbc722-9b0a-4f77-beed-8cadc4ee196b",
   "metadata": {},
   "source": [
    "<a id='step1_3'></a>\n",
    "### <b><font color='darkgreen'>Chains and LangChain Expression Language (LCEL)</font></b>\n",
    "<b><font size='3ptx'>The glue that connects chat models, prompts, and other objects in LangChain is the [chain](https://python.langchain.com/docs/modules/chains).</font></b>\n",
    "\n",
    "<b>A chain is nothing more than a sequence of calls between objects in LangChain</b>. The recommended way to build chains is to use the [**LangChain Expression Language**](https://python.langchain.com/docs/expression_language/) (LCEL). To see how this works, take a look at how you’d create a chain with a chat model and prompt template:\n",
    "\n",
    "```python\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "review_template_str = \"\"\"Your job is to use patient\n",
    "reviews to answer questions about their experience at\n",
    "a hospital. Use the following context to answer questions.\n",
    "Be as detailed as possible, but don't make up any information\n",
    "that's not from the context. If you don't know an answer, say\n",
    "you don't know.\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "review_system_prompt = SystemMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"context\"],\n",
    "        template=review_template_str,\n",
    "    )\n",
    ")\n",
    "\n",
    "review_human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\",\n",
    "    )\n",
    ")\n",
    "messages = [review_system_prompt, review_human_prompt]\n",
    "\n",
    "review_prompt_template = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "review_chain = review_prompt_template | chat_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db39a6e-5636-460a-9d5f-ab6dd2dc439d",
   "metadata": {},
   "source": [
    "This creates an object, `review_chain`, that can pass questions through `review_prompt_template` and `chat_model` in a single function call. In essence, this abstracts away all of the internal details of `review_chain`, allowing you to interact with the chain as if it were a chat model.\n",
    "\n",
    "After saving the updated <font color='olive'>chatbot.py</font>, start a new REPL session in your base project folder. Here’s how you can use `review_chain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e8d6af4-4b93-42e3-ba2b-e34bd4ab3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_intro.chat_bot import review_chain\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ac00862-9efd-4297-a8d8-2a264f4a7eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, the patient who left the review had a positive experience during their stay at the hospital.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_chain.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26164546-008a-489e-aa9d-a1a9bd37631e",
   "metadata": {},
   "source": [
    "In this block, you import `review_chain` and define context and question as before. You then pass a dictionary with the keys `context` and `question` into <font color='blue'>review_chan.invoke()</font>. This passes `context` and `question` through the prompt template and chat model to generate an answer.\n",
    "\n",
    "<b><font color='darkred'>Note:</font></b> When calling chains, you can use all of the same methods that a chat model supports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7c109-c8f6-473c-b818-e120b1090551",
   "metadata": {},
   "source": [
    "In general, the LCEL allows you to create arbitrary-length chains with the pipe symbol (`|`). For instance, if you wanted to format the model’s response, then you could add an output parser to the chain:\n",
    "\n",
    "```python\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ...\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "review_chain = review_prompt_template | chat_model\n",
    "review_chain_with_stdout = review_chain | output_parser\n",
    "```\n",
    "\n",
    "Here, you add a [**StrOutputParser**](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html) instance to `review_chain`, which will make the model’s response more readable. Start a new REPL session and give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9069adc5-ebe5-43aa-b97b-5e6bc671f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_intro.chat_bot import review_chain_with_stdout\n",
    "\n",
    "context = \"I had a great stay!\"\n",
    "question = \"Did anyone have a positive experience?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d00e603-206e-49f5-aba9-a174a406c6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, the patient who left the review had a great stay, indicating a positive experience at the hospital.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_chain_with_stdout.invoke({\"context\": context, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28953db0-6aa1-40e0-9745-b5d90b6d17ff",
   "metadata": {},
   "source": [
    "This block is the same as before, except now you can see that `review_chain_with_stdout` returns a nicely-formatted string rather than an [**AIMessage**](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html).\n",
    "\n",
    "<b>The power of chains is in the creativity and flexibility they afford you. You can chain together complex pipelines to create your chatbot, and you end up with an object that executes your pipeline in a single method call</b>. Next up, you’ll layer another object into `review_chain_with_stdout` to retrieve documents from a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb1d52-85b8-4530-869a-3f6e6e68e58e",
   "metadata": {},
   "source": [
    "<a id='step1_4'></a>\n",
    "### <b><font color='darkgreen'>Retrieval Objects</font></b>\n",
    "<b><font size='3ptx'>The goal of `review_chain_with_stdout` is to answer questions about patient experiences in the hospital from their reviews.</font></b>\n",
    "\n",
    "<b>So far, you’ve manually passed reviews in as context for the question. While this can work for a small number of reviews, it doesn’t scale well</b>. Moreover, even if you can fit all reviews into the model’s context window, there’s no guarantee it will use the correct reviews when answering a question.\n",
    "\n",
    "<b>To overcome this, you need a [retriever](https://python.langchain.com/docs/modules/data_connection/). The process of retrieving relevant documents and passing them to a language model to answer questions is known as [retrieval-augmented generation](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation)</b> (RAG).\n",
    "\n",
    "For this example, you’ll store all the reviews in a [**vector database**](https://en.wikipedia.org/wiki/Vector_database) called [**ChromaDB**](https://www.trychroma.com/). If you’re unfamiliar with this database tool and topics, then check out [Embeddings and Vector Databases with ChromaDB](https://realpython.com/chromadb-vector-database/) before continuing.\n",
    "\n",
    "You can install [**ChromaDB**](https://www.trychroma.com/) with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1dbd9b-84fc-4382-b2e5-449d8d443fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install chromadb==0.4.22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f90e82-20d9-44f6-8c6b-516ff087c64b",
   "metadata": {},
   "source": [
    "With this installed, you can use the following code to create a ChromaDB vector database with patient reviews:\n",
    "* <font color='olive'>`langchain_intro/create_retriever.py`</font>\n",
    "\n",
    "```python\n",
    "import dotenv\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "REVIEWS_CSV_PATH = \"data/reviews.csv\"\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# lines 11 and 12\n",
    "loader = CSVLoader(file_path=REVIEWS_CSV_PATH, source_column=\"review\")\n",
    "reviews = loader.load()\n",
    "\n",
    "# lines 14 to 16\n",
    "reviews_vector_db = Chroma.from_documents(\n",
    "    reviews, OpenAIEmbeddings(), persist_directory=REVIEWS_CHROMA_PATH\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57025e4-9ede-4012-af8f-6b1e6eecb28c",
   "metadata": {},
   "source": [
    "In lines 11 and 12, you load the reviews using LangChain’s [**CSVLoader**](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html). In lines 14 to 16, you create a ChromaDB instance from reviews using the default [**OpenAI embedding model**](https://platform.openai.com/docs/guides/embeddings), and you store the review embeddings at `REVIEWS_CHROMA_PATH`.\n",
    "\n",
    "<b><font color='darkred'>Note:</font></b>\n",
    "> In practice, if you’re embedding a large document, you should use a [**text splitter**](https://python.langchain.com/docs/modules/data_connection/document_transformers/). <b>Text splitters break the document into smaller chunks before running them through an embedding model</b>. This is important because embedding models have a fixed-size context window, and as the size of the text grows, an embedding’s ability to accurately represent the text decreases.\n",
    "> <br/><br/>\n",
    "> For this example, you can embed each review individually because they’re relatively small.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, open a terminal and run the following command from the project directory:\n",
    "\n",
    "```shell\n",
    "(venv) $ python langchain_intro/create_retriever.py\n",
    "```\n",
    "\n",
    "It should only take a minute or so to run, and afterwards you can start <b>performing semantic search over the review embeddings</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6491507-d1a4-47b3-b1b8-d2abcaad1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "reviews_vector_db = Chroma(\n",
    "    persist_directory=REVIEWS_CHROMA_PATH,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a705093-e43e-4ac8-a9ac-aa69c75528da",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
    "relevant_docs = reviews_vector_db.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12fd65bb-1dd7-47a6-8225-6b2f94be349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id: 73\n",
      "visit_id: 7696\n",
      "review: I had a frustrating experience at the hospital. The communication between the medical staff and me was unclear, leading to misunderstandings about my treatment plan. Improvement is needed in this area.\n",
      "physician_name: Maria Thompson\n",
      "hospital_name: Little-Spencer\n",
      "patient_name: Terri Smith\n"
     ]
    }
   ],
   "source": [
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8bd52b0-446f-4160-8c70-e5a997eb9fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id: 521\n",
      "visit_id: 631\n",
      "review: I had a challenging time at the hospital. The medical care was adequate, but the lack of communication between the staff and me left me feeling frustrated and confused about my treatment plan.\n",
      "physician_name: Samantha Mendez\n",
      "hospital_name: Richardson-Powell\n",
      "patient_name: Kurt Gordon\n"
     ]
    }
   ],
   "source": [
    "print(relevant_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4efbf54-747c-4ed0-a527-36d79b63035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id: 785\n",
      "visit_id: 2593\n",
      "review: My stay at the hospital was challenging. The medical care was adequate, but the lack of communication from the staff created some frustration.\n",
      "physician_name: Brittany Harris\n",
      "hospital_name: Jones, Taylor and Garcia\n",
      "patient_name: Ryan Jacobs\n"
     ]
    }
   ],
   "source": [
    "print(relevant_docs[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049fda1-5ec2-44ec-bdbc-4d7fe9788a31",
   "metadata": {},
   "source": [
    "You import the dependencies needed to call ChromaDB and specify the path to the stored ChromaDB data in `REVIEWS_CHROMA_PATH`. You then load environment variables using <font color='blue'>dotenv.load_dotenv()</font> and create a new Chroma instance pointing to your vector database. <b>Notice how you have to specify an embedding function again when connecting to your vector database. Be sure this is the same embedding function that you used to create the embeddings</b>.\n",
    "\n",
    "Next, you define a question and call <font color='blue'>.similarity_search()</font> on `reviews_vector_db`, passing in `question` and `k=3`. This creates an embedding for the question and searches the vector database for the three most similar review embeddings to question embedding. In this case, you see three reviews where patients complained about communication, which is exactly what you asked for!\n",
    "\n",
    "<b>The last thing to do is add your reviews retriever to `review_chain_with_stdout` so that relevant reviews are passed to the prompt as context</b>. Here’s how you do that:\n",
    "\n",
    "- `langchain_intro/chatbot.py`\n",
    "\n",
    "```python\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "REVIEWS_CHROMA_PATH = \"chroma_data/\"\n",
    "\n",
    "# ...\n",
    "\n",
    "reviews_vector_db = Chroma(\n",
    "    persist_directory=REVIEWS_CHROMA_PATH,\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "reviews_retriever  = reviews_vector_db.as_retriever(k=10)\n",
    "\n",
    "review_chain_with_rag = (\n",
    "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}  # New here\n",
    "    | review_prompt_template\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c67093-ecc3-440b-ba6b-7f222fb1c344",
   "metadata": {},
   "source": [
    "As before, you import ChromaDB’s dependencies, specify the path to your ChromaDB data, and instantiate a new Chroma object. You then create `reviews_retriever` by calling <font color='blue'>.as_retriever()</font> on `reviews_vector_db` to create a retriever object that you’ll add to `review_chain_with_rag`. Because you specified `k=10`, the retriever will fetch the ten reviews most similar to the user’s question.\n",
    "\n",
    "You then add a dictionary with c`ontext` and `question` keys to the front of `review_chain_with_rag`. Instead of passing `context` in manually, `review_chain_with_rag` will pass your `question` to the retriever to pull relevant reviews. <b>Assigning `question` to a [**RunnablePassthrough**](https://python.langchain.com/docs/expression_language/primitives/passthrough/) object ensures the `question` gets passed unchanged to the next step in the chain</b>.\n",
    "\n",
    "You now have a fully functioning chain that can answer questions about patient experiences from their reviews. Start a new REPL session and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c29cd27-5e06-4b3b-bb80-5193ed654330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_intro.chat_bot import review_chain_with_rag\n",
    "\n",
    "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2becc8f1-d446-466f-be8e-8fad8d26c91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, several patients have complained about communication with the hospital staff. For example, Terri Smith mentioned that the communication between the medical staff and her was unclear, leading to misunderstandings about her treatment plan. Kurt Gordon also expressed frustration and confusion about his treatment plan due to a lack of communication between the staff and himself. Ryan Jacobs and Shannon Williams also mentioned challenges and frustration caused by a lack of communication from the hospital staff.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_chain_with_rag.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f1962-d554-4928-bc0d-29fe445fee23",
   "metadata": {},
   "source": [
    "As you can see, you only call <font color='blue'>review_chain_with_rag.invoke(question)</font> to get retrieval-augmented answers about patient experiences from their reviews. You’ll improve upon this chain later by storing review embeddings, along with other metadata, in Neo4j.\n",
    "\n",
    "Now that you understand chat models, prompts, chains, and retrieval, you’re ready to dive into the last LangChain concept—agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f61da1-49dd-48be-849a-81f6a7b6cfef",
   "metadata": {},
   "source": [
    "<a id='step1_5'></a>\n",
    "### <b><font color='darkgreen'>Agents</font></b>\n",
    "<b><font size='3ptx'>So far, you’ve created a chain to answer questions using patient reviews. What if you want your chatbot to also answer questions about other hospital data, such as hospital wait times? </font></b>\n",
    "\n",
    "Ideally, your chatbot can seamlessly switch between answering patient review and wait time questions depending on the user’s query. To accomplish this, you’ll need the following components:\n",
    "1. The patient review chain you already created.\n",
    "2. A function that can look up wait times at a hospital.\n",
    "3. A way for an LLM to know when it should answer questions about patient experiences or look up wait times.\n",
    "\n",
    "To accomplish the third capability, you need an [**agent**](https://python.langchain.com/docs/modules/agents/).\n",
    "\n",
    "<b>An agent is a language model that decides on a sequence of actions to execute</b>. Unlike chains where the sequence of actions is hard-coded, agents use a language model to determine which actions to take and in which order.\n",
    "\n",
    "Before building the agent, <b>create the following function to generate fake wait times for a hospital</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe459d92-41cf-4002-af66-cb256a27da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def get_current_wait_time(hospital: str) -> int | str:\n",
    "    \"\"\"Dummy function to generate fake wait times\"\"\"\n",
    "\n",
    "    if hospital not in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "        return f\"Hospital {hospital} does not exist\"\n",
    "\n",
    "    # Simulate API call delay\n",
    "    time.sleep(1)\n",
    "\n",
    "    return random.randint(0, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070303bf-e30c-4f9c-8b0a-5097d728446e",
   "metadata": {},
   "source": [
    "<b>In `get_current_wait_time()`, you pass in a hospital name, check if it’s valid, and then generate a random number to simulate a wait time</b>. In reality, this would be some sort of database query or API call, but this will serve the same purpose for this demonstration.\n",
    "\n",
    "You can now create an agent that decides between `get_current_wait_time()` and `review_chain_with_rag.invoke()` depending on the question:\n",
    "\n",
    "* <font color='olive'>`langchain_intro/chatbot.py`</font>\n",
    "\n",
    "```python\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents import (\n",
    "    create_openai_functions_agent,\n",
    "    Tool,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain import hub\n",
    "from langchain_intro.tools import get_current_wait_time\n",
    "\n",
    "# ...\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Reviews\",\n",
    "        func=review_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions\n",
    "        about patient reviews or experiences at the hospital.\n",
    "        Not useful for answering questions about specific visit\n",
    "        details such as payer, billing, treatment, diagnosis,\n",
    "        chief complaint, hospital, or physician information.\n",
    "        Pass the entire question as input to the tool. For instance,\n",
    "        if the question is \"What do patients think about the triage system?\",\n",
    "        the input should be \"What do patients think about the triage system?\"\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Waits\",\n",
    "        func=get_current_wait_time,\n",
    "        description=\"\"\"Use when asked about current wait times\n",
    "        at a specific hospital. This tool can only get the current\n",
    "        wait time at a hospital and does not have any information about\n",
    "        aggregate or historical wait times. This tool returns wait times in\n",
    "        minutes. Do not pass the word \"hospital\" as input,\n",
    "        only the hospital name itself. For instance, if the question is\n",
    "        \"What is the wait time at hospital A?\", the input should be \"A\".\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "hospital_agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "hospital_agent = create_openai_functions_agent(\n",
    "    llm=agent_chat_model,\n",
    "    prompt=hospital_agent_prompt,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "hospital_agent_executor = AgentExecutor(\n",
    "    agent=hospital_agent,\n",
    "    tools=tools,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692433b6-b511-4eae-876c-ffaf74fa5786",
   "metadata": {},
   "source": [
    "In this block, you import a few additional dependencies that you’ll need to create the agent. You then define a list of [**Tool**](https://python.langchain.com/docs/modules/agents/tools/) objects. A [**Tool**](https://python.langchain.com/docs/modules/agents/tools/) is an interface that an agent uses to interact with a function. For instance, the first tool is named `Reviews` and it calls <font color='blue'>review_chain.invoke()</font> if the question meets the criteria of `description`.\n",
    "\n",
    "<b>Notice how `description` gives the agent instructions as to when it should call the tool</b>. This is where good prompt engineering skills are paramount to ensuring the LLM calls the correct tool with the correct inputs.\n",
    "\n",
    "The second [**Tool**](https://python.langchain.com/docs/modules/agents/tools/) in tools is named `Waits`, and it calls <font color='blue'>get_current_wait_time()</font>. Again, <b>the agent has to know when to use the `Waits` tool and what inputs to pass into it depending on the `description`</b>.\n",
    "\n",
    "Next, you initialize a ChatOpenAI object using **gpt-3.5-turbo-1106** as your language model. You then create an OpenAI functions agent with <font color='blue'>create_openai_functions_agent()</font>. This creates an agent designed to pass inputs to functions. It does this by returning valid JSON objects that store function inputs and their corresponding value.\n",
    "\n",
    "To create the agent run time, you pass the agent and tools into [**AgentExecutor**](https://python.langchain.com/docs/modules/agents/concepts#agentexecutor). Setting `return_intermediate_steps` and `verbose` to True will allow you to see the agent’s thought process and the tools it calls.\n",
    "\n",
    "Start a new REPL session to give your new agent a spin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c09717f-2fd1-460b-83ec-e494e2e965d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_intro.chat_bot import hospital_agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eb19bdb-4943-42e5-aa31-e3cb561ace5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Waits` with `C`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m1215\u001b[0m\u001b[32;1m\u001b[1;3mThe current wait time at hospital C is 12-15 minutes.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current wait time at hospital C?',\n",
       " 'output': 'The current wait time at hospital C is 12-15 minutes.',\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Waits', tool_input='C', log='\\nInvoking: `Waits` with `C`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"C\"}', 'name': 'Waits'}})]),\n",
       "   1215)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "    {\"input\": \"What is the current wait time at hospital C?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1736f4-17a2-4d90-b801-227943d03033",
   "metadata": {},
   "source": [
    "You first import the agent and then call <font color='blue'>hospital_agent_executor.invoke()</font> with a question about a wait time. As indicated in the output, the agent knows that you’re asking about a wait time, and it passes `C` as input to the `Waits` tool. The `Waits` tool then calls <font color='blue'>get_current_wait_time(hospital=\"C\")</font> and returns the corresponding wait time to the agent. The agent then uses this wait time to generate its final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3d35435-d7d0-44a1-b96e-6ffef7c2def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Reviews` with `What have patients said about their comfort at the hospital?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPatients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay, which impacted their overall comfort despite other positive aspects like knowledgeable doctors and a clean environment.\u001b[0m\u001b[32;1m\u001b[1;3mPatients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay, which impacted their overall comfort despite other positive aspects like knowledgeable doctors and a clean environment.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What have patients said about their comfort at the hospital?',\n",
       " 'output': \"Patients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay, which impacted their overall comfort despite other positive aspects like knowledgeable doctors and a clean environment.\",\n",
       " 'intermediate_steps': [(AgentActionMessageLog(tool='Reviews', tool_input='What have patients said about their comfort at the hospital?', log='\\nInvoking: `Reviews` with `What have patients said about their comfort at the hospital?`\\n\\n\\n', message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"__arg1\":\"What have patients said about their comfort at the hospital?\"}', 'name': 'Reviews'}})]),\n",
       "   \"Patients have mentioned both positive and negative aspects of their comfort at the hospital. One patient appreciated the hospital's dedication to patient comfort, mentioning well-designed private rooms and comfortable furnishings that made their recovery more bearable and contributed to an overall positive experience. However, other patients have complained about uncomfortable beds affecting their ability to get a good night's sleep during their stay, which impacted their overall comfort despite other positive aspects like knowledgeable doctors and a clean environment.\")]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hospital_agent_executor.invoke(\n",
    "    {\"input\": \"What have patients said about their comfort at the hospital?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dadd8-759a-4bb9-a542-1860157b48f9",
   "metadata": {},
   "source": [
    "A similar process happens when you ask the agent about patient experience reviews, except this time the agent knows to call the `Reviews` tool with `What have patients said about their comfort at the hospital?` as input. The `Reviews` tool runs <font color='blue'>review_chain.invoke()</font> using your full question as input, and the agent uses the response to generate its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda80adf-3706-4567-aca8-b0c290f00e9e",
   "metadata": {},
   "source": [
    "This is a profound capability. Agents give language models the ability to perform just about any task that you can write code for. Imagine all of the amazing, and potentially dangerous, chatbots you could build with agents.\n",
    "\n",
    "You now have all of the prerequisite LangChain knowledge needed to build a custom chatbot. Next up, you’ll put on your AI engineer hat and learn about the business requirements and data needed to build your hospital system chatbot.\n",
    "\n",
    "All of the code you’ve written so far was intended to teach you the fundamentals of LangChain, and it won’t be included in your final chatbot. Feel free to start with an empty directory in Step 2, where you’ll begin building your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9ff68-666e-4b30-af45-2679bcd23cd6",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## <b><font color='darkblue'>Step 2: Understand the Business Requirements and Data</font></b> ([back](#agenda))\n",
    "* <b><font size='3ptx'><a href='#step2_1'>Understand the Problem and Requirements</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#step2_2'>Explore the Available Data</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#step2_3'>Wait Times</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#step2_4'>Design the Chatbot</a></font></b>\n",
    "\n",
    "<b><font size='3ptx'>Before you start working on any AI project, you need to understand the problem that you want to solve and make a plan for how you’re going to solve it.</font></b>\n",
    "\n",
    "This involves clearly defining the problem, gathering requirements, understanding the data and technology available to you, and setting clear expectations with stakeholders. For this project, you’ll start by defining the problem and gathering business requirements for your chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46cc76-d06e-47af-b7df-260d82e92365",
   "metadata": {},
   "source": [
    "<a id='step2_1'></a>\n",
    "### <b><font color='darkgreen'>Understand the Problem and Requirements</font></b>\n",
    "<b><font size='3ptx'>Imagine you’re an AI engineer working for a large hospital system in the US. </font></b>\n",
    "\n",
    "<b>Your stakeholders would like more visibility into the ever-changing data they collect</b>. They want answers to ad-hoc questions about patients, visits, physicians, hospitals, and insurance payers without having to understand a query language like SQL, request a report from an analyst, or wait for someone to build a dashboard.\n",
    "\n",
    "<b>To accomplish this, your stakeholders want an internal chatbot tool, similar to ChatGPT, that can answer questions about your company’s data</b>. After meeting to gather requirements, you’re provided with a list of the kinds of questions your chatbot should answer:\n",
    "* What is the current wait time at XYZ hospital?\n",
    "* Which hospital currently has the shortest wait time?\n",
    "* At which hospitals are patients complaining about billing and insurance issues?\n",
    "* Have any patients complained about the hospital being unclean?\n",
    "* What have patients said about how doctors and nurses communicate with them?\n",
    "* What are patients saying about the nursing staff at XYZ hospital?\n",
    "* What was the total billing amount charged to Cigna payers in 2023?\n",
    "* How many patients has Dr. John Doe treated?\n",
    "* How many visits are open and what is their average duration in days?\n",
    "* Which physician has the lowest average visit duration in days?\n",
    "* How much was billed for patient 789’s stay?\n",
    "* Which hospital worked with the most Cigna patients in 2023?\n",
    "* What’s the average billing amount for emergency visits by hospital?\n",
    "* Which state had the largest percent increase inedicaid visits from 2022 to 2023?\n",
    "\n",
    "<br/>\n",
    "\n",
    "You can answer questions like What was the total billing amount charged to Cigna payers in 2023? with aggregate statistics using a query language like SQL. Crucially, these questions have a single objective answer. <b>You could run pre-defined queries to answer these, but any time a stakeholder has a new or slightly nuanced question, you have to write a new query. To avoid this, your chatbot should dynamically generate accurate queries</b>.\n",
    "\n",
    "Questions like `Have any patients complained about the hospital being unclean?` or `What have patients said about how doctors and nurses communicate with them?` are more subjective and might have many acceptable answers. Your chatbot will need to read through documents, such as patient reviews, to answer these kinds of questions.\n",
    "\n",
    "Ultimately, your stakeholders want a single chat interface that can seamlessly answer both subjective and objective questions. This means, when presented with a question, your chatbot needs to know what type of question is being asked and which data source to pull from.\n",
    "\n",
    "For instance, if asked `How much was billed for patient 789’s stay?`, your chatbot should know it needs to query a database to find the answer. If asked `What have patients said about how doctors and nurses communicate with them?`, your chatbot should know it needs to read and summarize patient reviews.\n",
    "\n",
    "<b>Next up, you’ll explore the data your hospital system records, which is arguably the most important prerequisite to building your chatbot</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873ab60-fd7a-4650-a020-9197a57fe519",
   "metadata": {},
   "source": [
    "<a id='step2_2'></a>\n",
    "### <b><font color='darkgreen'>Explore the Available Data</font></b> ([back](#step2))\n",
    "<b><font size='3ptx'>Before building your chatbot, you need a thorough understanding of the data it will use to respond to user queries.</font></b>\n",
    "\n",
    "This will help you determine what’s feasible and how you want to structure the data so that your chatbot can easily access it. All of the data you’ll use in this article was synthetically generated, and much of it was derived from a popular [**health care dataset on Kaggle**](https://www.kaggle.com/datasets/prasad22/healthcare-dataset).\n",
    "\n",
    "In practice, the following datasets would likely be stored as tables in a SQL database, but <b>you’ll work with CSV files to keep the focus on building the chatbot. This section will give you a detailed description of each CSV file</b>.\n",
    "\n",
    "You’ll need to place all CSV files that are part of this project in your <font color='olive'>data/</font> folder before continuing with the tutorial. <b>Make sure that you downloaded them from the materials and placed them in your <font color='olive'>data/</font> folder</b> ([Github link](https://github.com/realpython/materials/tree/master/langchain-rag-app/source_code_final)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce157826-7a33-4abf-b536-8c099f209161",
   "metadata": {},
   "source": [
    "#### <b>`hospitals.csv`</font>\n",
    "The <font color='olive'>hospitals.csv</font> file records information on each hospital that your company manages. There 30 hospitals and three [fields](https://en.wikipedia.org/wiki/Field_(computer_science)) in this file:\n",
    "* **hospital_id**: An integer that uniquely identifies a hospital.\n",
    "* **hospital_name**: The hospital’s name.\n",
    "* **hospital_state**: The state the hospital is located in.\n",
    "\n",
    "If you’re familiar with traditional SQL databases and the [**star schema**](https://en.wikipedia.org/wiki/Star_schema), you can think of <font color='olive'>hospitals.csv</font> as a [**dimension table**](https://en.wikipedia.org/wiki/Star_schema#Dimension_tables). Dimension tables are relatively short and contain descriptive information or attributes that provide context to the data in [**fact tables**](https://en.wikipedia.org/wiki/Star_schema#Fact_tables). Fact tables record events about the entities stored in dimension tables, and they tend to be longer tables.\n",
    "\n",
    "In this case, <font color='olive'>hospitals.csv</font> records information specific to hospitals, but you can join it to fact tables to answer questions about which patients, physicians, and payers are related to the hospital. This will be more clear when you explore <font color='olive'>visits.csv</font>.\n",
    "\n",
    "If you’re curious, you can inspect the first few rows of <font color='olive'>hospitals.csv</font> using a dataframe library like [**Polars**](https://docs.pola.rs/user-guide/getting-started/). Make sure [**Polars**](https://docs.pola.rs/user-guide/getting-started/) is installed in your virtual environment, and run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6bce2c0-030a-4ea5-b201-63107df956a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "HOSPITAL_DATA_PATH = \"data/hospitals.csv\"\n",
    "data_hospitals = pl.read_csv(HOSPITAL_DATA_PATH)\n",
    "print(data_hospitals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb187d36-71ff-4b8b-b5cc-da6f4b161672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>hospital_id</th><th>hospital_name</th><th>hospital_state</th></tr><tr><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;Wallace-Hamilt…</td><td>&quot;CO&quot;</td></tr><tr><td>1</td><td>&quot;Burke, Griffin…</td><td>&quot;NC&quot;</td></tr><tr><td>2</td><td>&quot;Walton LLC&quot;</td><td>&quot;FL&quot;</td></tr><tr><td>3</td><td>&quot;Garcia Ltd&quot;</td><td>&quot;NC&quot;</td></tr><tr><td>4</td><td>&quot;Jones, Brown a…</td><td>&quot;NC&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────────┬───────────────────────────┬────────────────┐\n",
       "│ hospital_id ┆ hospital_name             ┆ hospital_state │\n",
       "│ ---         ┆ ---                       ┆ ---            │\n",
       "│ i64         ┆ str                       ┆ str            │\n",
       "╞═════════════╪═══════════════════════════╪════════════════╡\n",
       "│ 0           ┆ Wallace-Hamilton          ┆ CO             │\n",
       "│ 1           ┆ Burke, Griffin and Cooper ┆ NC             │\n",
       "│ 2           ┆ Walton LLC                ┆ FL             │\n",
       "│ 3           ┆ Garcia Ltd                ┆ NC             │\n",
       "│ 4           ┆ Jones, Brown and Murray   ┆ NC             │\n",
       "└─────────────┴───────────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hospitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760d77a-8a31-4d93-bf29-e4acb894f17f",
   "metadata": {},
   "source": [
    "This shows you, for example, that `Walton, LLC` hospital has an ID of `2` and is located in the state of Florida, `FL`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53800ea-2b0c-4f4a-84d4-2795fee6ff55",
   "metadata": {},
   "source": [
    "#### <b>`physicians.csv`</b>\n",
    "The <font color='olive'>`physicians.csv`</font> file contains data about the physicians that work for your hospital system. This dataset has the following fields:\n",
    "* **physician_id**: An integer that uniquely identifies each physician.\n",
    "* **physician_name**: The physician’s name.\n",
    "* **physician_dob**: The physician’s date of birth.\n",
    "* **physician_grad_year**: The year the physician graduated medical school.\n",
    "* **medical_school**: Where the physician attended medical school.\n",
    "* **salary**: The physician’s salary.\n",
    "\n",
    "This data can again be thought of as a dimension table, and you can inspect the first few rows using Polars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5c1cca-a84d-401a-837a-b8064f1162de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n"
     ]
    }
   ],
   "source": [
    "PHYSICIAN_DATA_PATH = \"data/physicians.csv\"\n",
    "data_physician = pl.read_csv(PHYSICIAN_DATA_PATH)\n",
    "print(data_physician.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68041347-2005-4772-a2f1-789f7c203e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>physician_name</th><th>physician_id</th><th>physician_dob</th><th>physician_grad_year</th><th>medical_school</th><th>salary</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Joseph Johnson…</td><td>0</td><td>&quot;1970-02-22&quot;</td><td>&quot;2000-02-22&quot;</td><td>&quot;Johns Hopkins …</td><td>309534.155076</td></tr><tr><td>&quot;Jason Williams…</td><td>1</td><td>&quot;1982-12-22&quot;</td><td>&quot;2012-12-22&quot;</td><td>&quot;Mayo Clinic Al…</td><td>281114.503559</td></tr><tr><td>&quot;Jesse Gordon&quot;</td><td>2</td><td>&quot;1959-06-03&quot;</td><td>&quot;1989-06-03&quot;</td><td>&quot;David Geffen S…</td><td>305845.584636</td></tr><tr><td>&quot;Heather Smith&quot;</td><td>3</td><td>&quot;1965-06-15&quot;</td><td>&quot;1995-06-15&quot;</td><td>&quot;NYU Grossman M…</td><td>295239.766689</td></tr><tr><td>&quot;Kayla Hunter D…</td><td>4</td><td>&quot;1978-10-19&quot;</td><td>&quot;2008-10-19&quot;</td><td>&quot;David Geffen S…</td><td>298751.355201</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌─────────────────┬──────────────┬───────────────┬────────────────┬────────────────┬───────────────┐\n",
       "│ physician_name  ┆ physician_id ┆ physician_dob ┆ physician_grad ┆ medical_school ┆ salary        │\n",
       "│ ---             ┆ ---          ┆ ---           ┆ _year          ┆ ---            ┆ ---           │\n",
       "│ str             ┆ i64          ┆ str           ┆ ---            ┆ str            ┆ f64           │\n",
       "│                 ┆              ┆               ┆ str            ┆                ┆               │\n",
       "╞═════════════════╪══════════════╪═══════════════╪════════════════╪════════════════╪═══════════════╡\n",
       "│ Joseph Johnson  ┆ 0            ┆ 1970-02-22    ┆ 2000-02-22     ┆ Johns Hopkins  ┆ 309534.155076 │\n",
       "│                 ┆              ┆               ┆                ┆ University     ┆               │\n",
       "│                 ┆              ┆               ┆                ┆ School …       ┆               │\n",
       "│ Jason Williams  ┆ 1            ┆ 1982-12-22    ┆ 2012-12-22     ┆ Mayo Clinic    ┆ 281114.503559 │\n",
       "│                 ┆              ┆               ┆                ┆ Alix School of ┆               │\n",
       "│                 ┆              ┆               ┆                ┆ Medic…         ┆               │\n",
       "│ Jesse Gordon    ┆ 2            ┆ 1959-06-03    ┆ 1989-06-03     ┆ David Geffen   ┆ 305845.584636 │\n",
       "│                 ┆              ┆               ┆                ┆ School of      ┆               │\n",
       "│                 ┆              ┆               ┆                ┆ Medicine …     ┆               │\n",
       "│ Heather Smith   ┆ 3            ┆ 1965-06-15    ┆ 1995-06-15     ┆ NYU Grossman   ┆ 295239.766689 │\n",
       "│                 ┆              ┆               ┆                ┆ Medical School ┆               │\n",
       "│ Kayla Hunter    ┆ 4            ┆ 1978-10-19    ┆ 2008-10-19     ┆ David Geffen   ┆ 298751.355201 │\n",
       "│ DDS             ┆              ┆               ┆                ┆ School of      ┆               │\n",
       "│                 ┆              ┆               ┆                ┆ Medicine …     ┆               │\n",
       "└─────────────────┴──────────────┴───────────────┴────────────────┴────────────────┴───────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_physician.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df0a2c-a5d1-4915-a5d5-f87817231ef8",
   "metadata": {},
   "source": [
    "For instance, `Heather Smith` has a physician ID of `3`, was born on `June 15, 1965`, graduated medical school on `June 15, 1995`, attended `NYU Grossman Medical School`, and her salary is about `$295,239`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e0005-e8b7-4183-b2da-dba94a28e524",
   "metadata": {},
   "source": [
    "#### <b>`payers.csv`</b>\n",
    "The next file, <font color='olive'>`payers.csv`</font>, records information about the insurance companies that your hospitals bills for patient visits. Similar to hospitals.csv, it’s a small file with a couple fields:\n",
    "* **payer_id**: An integer that uniquely identifies each payer.\n",
    "* **payer_name**: The payer’s company name.\n",
    "\n",
    "The only five payers in the data are **Medicaid**, **UnitedHealthcare**, **Aetna**, **Cigna**, and **Blue Cross**. Your stakeholders are very interested in payer activity, so payers.csv will be helpful once it’s connected to patients, hospitals, and physicians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d5a55-b78f-489c-91c0-f2796c375d75",
   "metadata": {},
   "source": [
    "#### <b>`reviews.csv`</b>\n",
    "The <font color='olive'>reviews.csv</font> file contains patient reviews about their experience at the hospital. It has these fields:\n",
    "* **review_id**: An integer that uniquely identifies a review.\n",
    "* **visit_id**: An integer that identifies the patient’s visit that the review was about.\n",
    "* **review**: This is the free form text review left by the patient.\n",
    "* **physician_name**: The name of the physician who treated the patient.\n",
    "* **hospital_name**: The hospital where the patient stayed.\n",
    "* **patient_name**: The patient’s name.\n",
    "\n",
    "This dataset is the first one you’ve seen that contains the free text **review** field, and your chatbot should use this to answer questions about review details and patient experiences. Here’s what <font color='olive'>reviews.csv</font> looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d25a48f4-9bf4-4686-a700-2d8cb878e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005, 6)\n"
     ]
    }
   ],
   "source": [
    "REVIEWS_DATA_PATH = \"data/reviews.csv\"\n",
    "data_reviews = pl.read_csv(REVIEWS_DATA_PATH)\n",
    "print(data_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c202808e-bf43-4674-bd2f-4b8eb76312f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>review_id</th><th>visit_id</th><th>review</th><th>physician_name</th><th>hospital_name</th><th>patient_name</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>6997</td><td>&quot;The medical st…</td><td>&quot;Laura Brown&quot;</td><td>&quot;Wallace-Hamilt…</td><td>&quot;Christy Johnso…</td></tr><tr><td>9</td><td>8138</td><td>&quot;The hospital&#x27;s…</td><td>&quot;Steven Watson&quot;</td><td>&quot;Wallace-Hamilt…</td><td>&quot;Anna Frazier&quot;</td></tr><tr><td>11</td><td>680</td><td>&quot;The hospital&#x27;s…</td><td>&quot;Chase Mcpherso…</td><td>&quot;Wallace-Hamilt…</td><td>&quot;Abigail Mitche…</td></tr><tr><td>892</td><td>9846</td><td>&quot;I had a positi…</td><td>&quot;Jason Martinez…</td><td>&quot;Wallace-Hamilt…</td><td>&quot;Kimberly Rivas…</td></tr><tr><td>822</td><td>7397</td><td>&quot;The medical te…</td><td>&quot;Chelsey Davis&quot;</td><td>&quot;Wallace-Hamilt…</td><td>&quot;Catherine Yang…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌───────────┬──────────┬──────────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
       "│ review_id ┆ visit_id ┆ review           ┆ physician_name   ┆ hospital_name    ┆ patient_name     │\n",
       "│ ---       ┆ ---      ┆ ---              ┆ ---              ┆ ---              ┆ ---              │\n",
       "│ i64       ┆ i64      ┆ str              ┆ str              ┆ str              ┆ str              │\n",
       "╞═══════════╪══════════╪══════════════════╪══════════════════╪══════════════════╪══════════════════╡\n",
       "│ 0         ┆ 6997     ┆ The medical      ┆ Laura Brown      ┆ Wallace-Hamilton ┆ Christy Johnson  │\n",
       "│           ┆          ┆ staff at the     ┆                  ┆                  ┆                  │\n",
       "│           ┆          ┆ hospita…         ┆                  ┆                  ┆                  │\n",
       "│ 9         ┆ 8138     ┆ The hospital's   ┆ Steven Watson    ┆ Wallace-Hamilton ┆ Anna Frazier     │\n",
       "│           ┆          ┆ commitment to    ┆                  ┆                  ┆                  │\n",
       "│           ┆          ┆ pat…             ┆                  ┆                  ┆                  │\n",
       "│ 11        ┆ 680      ┆ The hospital's   ┆ Chase Mcpherson  ┆ Wallace-Hamilton ┆ Abigail Mitchell │\n",
       "│           ┆          ┆ commitment to    ┆ Jr.              ┆                  ┆                  │\n",
       "│           ┆          ┆ pat…             ┆                  ┆                  ┆                  │\n",
       "│ 892       ┆ 9846     ┆ I had a positive ┆ Jason Martinez   ┆ Wallace-Hamilton ┆ Kimberly Rivas   │\n",
       "│           ┆          ┆ experience over… ┆                  ┆                  ┆                  │\n",
       "│ 822       ┆ 7397     ┆ The medical team ┆ Chelsey Davis    ┆ Wallace-Hamilton ┆ Catherine Yang   │\n",
       "│           ┆          ┆ at the hospital… ┆                  ┆                  ┆                  │\n",
       "└───────────┴──────────┴──────────────────┴──────────────────┴──────────────────┴──────────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50561f7c-4237-4979-baa7-8a70b977d171",
   "metadata": {},
   "source": [
    "There are 1005 reviews in this dataset, and you can see how each review relates to a visit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e06a3-b1ca-40b7-8193-0e622c6e73c6",
   "metadata": {},
   "source": [
    "#### <b>`visits.csv`</b>\n",
    "The last file, <font color='olive'>visits.csv</font>, records details about every hospital visit your company has serviced. Continuing with the star schema analogy, you can think of <font color='olive'>visits.csv</font> as a [**fact table**](https://en.wikipedia.org/wiki/Star_schema#Fact_tables) that connects hospitals, physicians, patients, and payers. Here are the fields:\n",
    "* **visit_id**: The unique identifier of a hospital visit.\n",
    "* **patient_id**: The ID of the patient associated with the visit.\n",
    "* **date_of_admission**: The date the patient was admitted to the hospital.\n",
    "* **room_number**: The patient’s room number.\n",
    "* **admission_type**: One of ‘Elective’, ‘Emergency’, or ‘Urgent’.\n",
    "* **chief_complaint**: A string describing the patient’s primary reason for being at the hospital.\n",
    "* **primary_diagnosis**: A string describing the primary diagnosis made by the physician.\n",
    "* **treatment_description**: A text summary of the treatment given by the physician.\n",
    "* **test_results**: One of ‘Inconclusive’, ‘Normal’, or ‘Abnormal’.\n",
    "* **discharge_date**: The date the patient was discharged from the hospital\n",
    "* **physician_id**: The ID of the physician that treated the patient.\n",
    "* **hospital_id**: The ID of the hospital the patient stayed at.\n",
    "* **payer_id**: The ID of the insurance payer used by the patient.\n",
    "* **billing_amount**: The amount of money billed to the payer for the visit.\n",
    "* **visit_status**: One of ‘OPEN’ or ‘DISCHARGED’.\n",
    "\n",
    "<b>This dataset gives you everything you need to answer questions about the relationship between each hospital entity</b>. For example, if you know a physician ID, you can use <font color='olive'>visits.csv</font> to figure out which patients, payers, and hospitals the physician is associated with. Take a look at what <font color='olive'>visits.csv</font> looks like in Polars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad586ef4-9da0-4f3a-8332-7338c940ff6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9998, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VISITS_DATA_PATH = \"data/visits.csv\"\n",
    "data_visits = pl.read_csv(VISITS_DATA_PATH)\n",
    "data_visits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2037beff-154c-430e-a7fd-ed4a0c4e77a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>patient_id</th><th>date_of_admission</th><th>billing_amount</th><th>room_number</th><th>admission_type</th><th>discharge_date</th><th>test_results</th><th>visit_id</th><th>physician_id</th><th>payer_id</th><th>hospital_id</th><th>chief_complaint</th><th>treatment_description</th><th>primary_diagnosis</th><th>visit_status</th></tr><tr><td>i64</td><td>str</td><td>f64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;2022-11-17&quot;</td><td>37490.983364</td><td>146</td><td>&quot;Elective&quot;</td><td>&quot;2022-12-01&quot;</td><td>&quot;Inconclusive&quot;</td><td>0</td><td>102</td><td>1</td><td>0</td><td>null</td><td>null</td><td>null</td><td>&quot;DISCHARGED&quot;</td></tr><tr><td>1</td><td>&quot;2023-06-01&quot;</td><td>47304.064845</td><td>404</td><td>&quot;Emergency&quot;</td><td>null</td><td>&quot;Normal&quot;</td><td>1</td><td>435</td><td>4</td><td>5</td><td>null</td><td>null</td><td>null</td><td>&quot;OPEN&quot;</td></tr><tr><td>2</td><td>&quot;2019-01-09&quot;</td><td>36874.896997</td><td>292</td><td>&quot;Emergency&quot;</td><td>&quot;2019-02-08&quot;</td><td>&quot;Normal&quot;</td><td>2</td><td>348</td><td>2</td><td>6</td><td>null</td><td>null</td><td>null</td><td>&quot;DISCHARGED&quot;</td></tr><tr><td>3</td><td>&quot;2020-05-02&quot;</td><td>23303.322092</td><td>480</td><td>&quot;Urgent&quot;</td><td>&quot;2020-05-03&quot;</td><td>&quot;Abnormal&quot;</td><td>3</td><td>270</td><td>4</td><td>15</td><td>null</td><td>null</td><td>null</td><td>&quot;DISCHARGED&quot;</td></tr><tr><td>4</td><td>&quot;2021-07-09&quot;</td><td>18086.344184</td><td>477</td><td>&quot;Urgent&quot;</td><td>&quot;2021-08-02&quot;</td><td>&quot;Normal&quot;</td><td>4</td><td>106</td><td>2</td><td>29</td><td>&quot;Persistent cou…</td><td>&quot;Prescribed a c…</td><td>&quot;J45.909 - Unsp…</td><td>&quot;DISCHARGED&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 15)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ patient_i ┆ date_of_a ┆ billing_a ┆ room_numb ┆ … ┆ chief_com ┆ treatment ┆ primary_d ┆ visit_st │\n",
       "│ d         ┆ dmission  ┆ mount     ┆ er        ┆   ┆ plaint    ┆ _descript ┆ iagnosis  ┆ atus     │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ion       ┆ ---       ┆ ---      │\n",
       "│ i64       ┆ str       ┆ f64       ┆ i64       ┆   ┆ str       ┆ ---       ┆ str       ┆ str      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ str       ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 0         ┆ 2022-11-1 ┆ 37490.983 ┆ 146       ┆ … ┆ null      ┆ null      ┆ null      ┆ DISCHARG │\n",
       "│           ┆ 7         ┆ 364       ┆           ┆   ┆           ┆           ┆           ┆ ED       │\n",
       "│ 1         ┆ 2023-06-0 ┆ 47304.064 ┆ 404       ┆ … ┆ null      ┆ null      ┆ null      ┆ OPEN     │\n",
       "│           ┆ 1         ┆ 845       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2         ┆ 2019-01-0 ┆ 36874.896 ┆ 292       ┆ … ┆ null      ┆ null      ┆ null      ┆ DISCHARG │\n",
       "│           ┆ 9         ┆ 997       ┆           ┆   ┆           ┆           ┆           ┆ ED       │\n",
       "│ 3         ┆ 2020-05-0 ┆ 23303.322 ┆ 480       ┆ … ┆ null      ┆ null      ┆ null      ┆ DISCHARG │\n",
       "│           ┆ 2         ┆ 092       ┆           ┆   ┆           ┆           ┆           ┆ ED       │\n",
       "│ 4         ┆ 2021-07-0 ┆ 18086.344 ┆ 477       ┆ … ┆ Persisten ┆ Prescribe ┆ J45.909 - ┆ DISCHARG │\n",
       "│           ┆ 9         ┆ 184       ┆           ┆   ┆ t cough   ┆ d a combi ┆ Unspecifi ┆ ED       │\n",
       "│           ┆           ┆           ┆           ┆   ┆ and       ┆ nation of ┆ ed        ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ shortness ┆ inha…     ┆ asthma,   ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆ o…        ┆           ┆ un…       ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb95be2-e5c3-4a09-a136-1e9ebb7b5b9b",
   "metadata": {},
   "source": [
    "You can see there are 9998 visits recorded along with the 15 fields described above. Notice that `chief_complaint`, `treatment_description`, and `primary_diagnosis` might be missing for a visit. You’ll have to keep this in mind as your stakeholders might not be aware that many visits are missing critical data—this may be a valuable insight in itself! Lastly, notice that when a visit is still open, the `discharged_date` will be missing.\n",
    "\n",
    "You now have an understanding of the data you’ll use to build the chatbot your stakeholders want. To recap, the files are broken out to simulate what a traditional SQL database might look like. Every hospital, patient, physician, review, and payer are connected through <font color='olive'>visits.csv</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5d2e4-52db-4717-a325-6f39f55c537d",
   "metadata": {},
   "source": [
    "<a id='step2_3'></a>\n",
    "### <b><font color='darkgreen'>Wait Times</font></b> ([back](#step2))\n",
    "You might have noticed there’s no data to answer questions like `What is the current wait time at XYZ hospital?` or Which hospital currently has the shortest wait time?. Unfortunately, the hospital system doesn’t record historical wait times. Your chatbot will have to call an API to get current wait time information. You’ll see how this works later.\n",
    "\n",
    "<b>With an understanding of the business requirements, available data, and LangChain functionalities, you can create a design for your chatbot.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeea08a-412c-459a-a053-5264170a1eb1",
   "metadata": {},
   "source": [
    "<a id='step2_4'></a>\n",
    "### <b><font color='darkgreen'>Design the Chatbot</font></b> ([back](#step2))\n",
    "<b><font size='3ptx'>Now that you know the business requirements, data, and LangChain prerequisites, you’re ready to design your chatbot. </font></b>\n",
    "\n",
    "<b>A good design gives you and others a conceptual understanding of the components needed to build your chatbot.</b> Your design should clearly illustrate how data flows through your chatbot, and it should serve as a helpful reference during development.\n",
    "\n",
    "<b>Your design should clearly illustrate how data flows through your chatbot, and it should serve as a helpful reference during development.</b> Your chatbot will use multiple tools to answer diverse questions about your hospital system. Here’s a flowchart illustrating how you’ll accomplish this:\n",
    "![flow chart](https://files.realpython.com/media/Screenshot_2024-01-15_at_8.08.18_PM.fe16f8a318cc.png)\n",
    "<center><i>Architecture and data flow for the hospital system chatbot</i></center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "This flowchart illustrates how data moves through your chatbot, starting from the user’s input query all the way to the final response. Here’s a summary of each component:\n",
    "* **LangChain Agent**: The LangChain agent is the brain of your chatbot. Given a user query, the agent decides which tool to call and what to give the tool as input. The agent then observes the tool’s output and decides what to return to the user—this is the agent’s response.\n",
    "* **Neo4j AuraDB**: You’ll store both structured hospital system data and patient reviews in a Neo4j AuraDB graph database. You’ll learn all about this in the next section.\n",
    "* **LangChain Neo4j Cypher Chain**: This chain tries to convert the user query into Cypher, Neo4j’s query language, and execute the Cypher query in Neo4j. The chain then answers the user query using the Cypher query results. The chain’s response is fed back to the LangChain agent and sent to the user.\n",
    "* **LangChain Neo4j Reviews Vector Chain**: This is very similar to the chain you built in [**Step 1**](#step1), except now patient review embeddings are stored in Neo4j. The chain searches for relevant reviews based on those semantically similar to the user query, and the reviews are used to answer the user query.\n",
    "* **Wait Times Function**: Similar to the logic in [**Step 1**](#step1), the LangChain agent tries to extract a hospital name from the user query. The hospital name is passed as input to a Python function that gets wait times, and the wait time is returned to the agent.\n",
    "\n",
    "To walk through an example, suppose a user asks `How many emergency visits were there in 2023?` The LangChain agent will receive this question and decide which tool, if any, to pass the question to. **In this case, the agent should pass the question to the LangChain Neo4j Cypher Chain. The chain will try to convert the question to a Cypher query, run the Cypher query in Neo4j, and use the query results to answer the question**.\n",
    "\n",
    "Once the LangChain Neo4j Cypher Chain answers the question, it will return the answer to the agent, and the agent will relay the answer to the user.\n",
    "\n",
    "With this design in mind, you can start building your chatbot. Your first task is to set up a Neo4j AuraDB instance for your chatbot to access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727cf95-6648-45b8-b749-c415b0566526",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## <b><font color='darkblue'>Step 3: Set Up a Neo4j Graph Database</font></b> ([back](#agenda))\n",
    "* <font size='3ptx'><b><a href='#step3_1'>A Brief Overview of Graph Databases</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step3_2'>Create a Neo4j Account and AuraDB Instance</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step3_3'>Design the Hospital System Graph Database</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step3_4'>Upload Data to Neo4j</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#step3_5'>Query the Hospital System Graph</a></b></font>\n",
    "\n",
    "<b><font size='3ptx'>As you saw in [step 2](#step2), your hospital system data is currently stored in CSV files. Before building your chatbot, you need to store this data in a database that your chatbot can query. You’ll use Neo4j AuraDB for this.</font></b>\n",
    "\n",
    "Before learning how to set up a Neo4j AuraDB instance, you’ll get an overview of graph databases, and <b>you’ll see why using a graph database may be a better choice than a relational database for this project</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a88a-018b-4f24-87f0-b37cd401a8f7",
   "metadata": {},
   "source": [
    "<a id='step3_1'></a>\n",
    "### <b><font color='darkgreen'>A Brief Overview of Graph Databases</font></b>\n",
    "<b><font size='3ptx'>Graph databases, such as Neo4j, are databases designed to represent and process data stored as a graph. </font></b>\n",
    "\n",
    "<b>Graph data consists of nodes, edges or relationships, and properties</b>. Nodes represent entities, relationships connect entities, and properties provide additional metadata about nodes and relationships.\n",
    "\n",
    "For example, here’s how you might represent hospital system nodes and relationships in a graph:\n",
    "![hostpital graph](https://files.realpython.com/media/Screenshot_2024-01-16_at_4.33.31_PM.043fc98132e3.png)\n",
    "\n",
    "This graph has three nodes - <b>Patient</b>, <b>Visit</b>, and <b>Payer</b>. <b>Patient</b> and <b>Visit</b> are connected by the `HAS` relationship, indicating that a hospital patient has a visit. Similarly, <b>Visit</b> and <b>Payer</b> are connected by the `COVERED_BY` relationship, indicating that an insurance payer covers a hospital visit.\n",
    "\n",
    "<b>Notice how the relationships are represented by an arrow indicating their direction</b>. For example, the direction of the `HAS` relationship tells you that a patient can have a visit, but a visit cannot have a patient.\n",
    "\n",
    "<b>Both nodes and relationships can have properties.</b> In this example, <b>Patient</b> nodes have `id`, `name`, and `date of birth` properties, and the `COVERED_BY` relationship has `service date` and `billing amount` properties. Storing data in a graph like this has several advantages:\n",
    "* **Simplicity**: Modeling real-world relationships between entities is natural in graph databases, reducing the need for complex schemas that require multiple join operations to answer queries.\n",
    "* **Relationships**: Graph databases excel at handling complex relationships. Traversing relationships is efficient, making it easy to query and analyze connected data.\n",
    "* **Flexibility**: Graph databases are schema-less, allowing for easy adaptation to changing data structures. This flexibility is beneficial for evolving data models.\n",
    "* **Performance**: Retrieving connected data is faster in graph databases than in relational databases, especially for scenarios involving complex queries with multiple relationships.\n",
    "* **Pattern Matching**: Graph databases support powerful pattern-matching queries, making it easier to express and find specific structures within the data.\n",
    "\n",
    "<b>When you have data with many complex relationships, the simplicity and flexibility of graph databases makes them easier to design and query compared to relational databases</b>. As you’ll see later, specifying relationships in graph database queries is concise and doesn’t involve complicated joins. If you’re interested, Neo4j illustrates this well with a realistic example database in their [documentation](https://neo4j.com/developer/cypher/guide-sql-to-cypher/).\n",
    "\n",
    "Because of this concise data representation, there’s less room for error when an LLM generates graph database queries. This is because you only need to tell the LLM about the nodes, relationships, and properties in your graph database. Contrast this with relational databases where the LLM must navigate and retain knowledge of the table schemas and foreign key relationships throughout your database, leaving more room for error in SQL generation.\n",
    "\n",
    "<b>Next, you’ll begin working with graph databases by setting up a Neo4j AuraDB instance. After that, you’ll move the hospital system into your Neo4j instance and learn how to query it.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1a2ae-bd6e-4532-8489-073796b503de",
   "metadata": {},
   "source": [
    "<a id='step3_2'></a>\n",
    "### <b><font color='darkgreen'>Create a Neo4j Account and AuraDB Instance</font></b> ([back](#step3))\n",
    "To get started using Neo4j, you can create a free Neo4j AuraDB account. The landing page should look something like this:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-10_at_9.52.26_AM.13dfb78c613b.png)\n",
    "<center>Neo4j Aura getting started screen</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Click the Start Free button and create an account. Once you’re signed in, you should see the Neo4j Aura console:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-10_at_9.53.58_AM.c9d5252982fc.png)\n",
    "<center>Create a new Aura instance</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "After you click <b>Download and Continue</b>, your instance should be created and a text file containing the Neo4j database credentials should download. Once the instance is created, you’ll see its status is Running. There should be no nodes or relationships yet:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-10_at_10.00.34_AM.0ca76879f1fc.png)\n",
    "<center>Aura running instance</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, open the text file you downloaded with your Neo4j credentials and copy the `NEO4J_URI`, `NEO4J_USERNAME`, and `NEO4J_PASSWORD` into your `.env` file:\n",
    "* **`.env`**\n",
    "\n",
    "```shell\n",
    "OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>\n",
    "\n",
    "NEO4J_URI=<YOUR_NEO4J_URI>\n",
    "NEO4J_USERNAME=<YOUR_NEO4J_URI>\n",
    "NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD>\n",
    "```\n",
    "\n",
    "You’ll use these environment variables to connect to your Neo4j instance in Python so that your chatbot can execute queries.\n",
    "\n",
    "You now have everything in place to interact with your Neo4j instance. <b>Next up, you’ll design the hospital system graph database. This will tell you how the hospital entities are related, and it will inform the kinds of queries you can run</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f606d-075d-4d5e-a89f-46c307b4bb2a",
   "metadata": {},
   "source": [
    "<a id='step3_3'></a>\n",
    "### <b><font color='darkgreen'>Design the Hospital System Graph Database</font></b> ([back](#step3))\n",
    "<b><font size='3ptx'>Now that you have a running Neo4j AuraDB instance, you need to decide which nodes, relationships, and properties you want to store.</font></b>\n",
    "\n",
    "One of the most popular ways to represent this is with a flowchart. Based on your understanding of the hospital system data, you come up with the following design:\n",
    "![flowchart](https://files.realpython.com/media/Screenshot_2024-01-11_at_9.25.30_AM.16896d00ee08.png)\n",
    "<center>Hospital system graph database design</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "This diagram shows you all of the nodes and relationships in the hospital system data. One useful way to think about this flowchart is to start with the <b>Patient</b> node and follow the relationships. A <b>Patient</b> has a visit at a hospital, and the hospital employs a physician to treat the visit which is covered by an insurance payer.\n",
    "\n",
    "Here are the properties stored in each node:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-17_at_8.28.33_AM.e784ec79aa41.png)\n",
    "<center>Hospital system node properties</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "The majority of these properties come directly from the fields you explored in [**step 2**](#step2). One notable difference is that <b>Review</b> nodes have an `embedding` property, which is a vector representation of the `patient_name`, `physician_name`, and `text` properties. This allows you to do vector searches over review nodes like you did with ChromaDB.\n",
    "\n",
    "Here are the relationship properties:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-17_at_9.07.16_AM.de07d986e379.png)\n",
    "<center>Hospital system relationship properties</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "As you can see, `COVERED_BY` is the only relationship with more than an `id` property. The `service_date` is the date the patient was discharged from a visit, and `billing_amount` is the amount charged to the payer for the visit.\n",
    "\n",
    "<b>Now that you have an overview of the hospital system design you’ll use, it’s time to move your data into Neo4j!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fd320-071f-417e-8ff8-73f29248e87e",
   "metadata": {},
   "source": [
    "<a id='step3_4'></a>\n",
    "### <b><font color='darkgreen'>Upload Data to Neo4j</font></b> ([back](#step3))\n",
    "<b><font size='3ptx'>With a running Neo4j instance and an understanding of the nodes, properties, and relationships you want to store, you can move the hospital system data into Neo4j</font></b>\n",
    "\n",
    "For this, you’ll create a folder called hospital_neo4j_etl with a few empty files. You’ll also want to create a <font color='olive'>docker-compose.yml</font> file in your project’s root directory:\n",
    "```\n",
    "./\n",
    "│\n",
    "├── hospital_neo4j_etl/\n",
    "│   │\n",
    "│   ├── src/\n",
    "│   │   ├── entrypoint.sh\n",
    "│   │   └── hospital_bulk_csv_write.py\n",
    "│   │\n",
    "│   ├── Dockerfile\n",
    "│   └── pyproject.toml\n",
    "│\n",
    "├── .env\n",
    "└── docker-compose.yml\n",
    "```\n",
    "\n",
    "Your <font color='olive'>.env</font> file should have the following environment variables:\n",
    "* `.env.py`\n",
    "\n",
    "```shell\n",
    "OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>\n",
    "\n",
    "NEO4J_URI=<YOUR_NEO4J_URI>\n",
    "NEO4J_USERNAME=<YOUR_NEO4J_URI>\n",
    "NEO4J_PASSWORD=<YOUR_NEO4J_PASSWORD>\n",
    "\n",
    "HOSPITALS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/hospitals.csv\n",
    "PAYERS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/payers.csv\n",
    "PHYSICIANS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/physicians.csv\n",
    "PATIENTS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/patients.csv\n",
    "VISITS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/visits.csv\n",
    "REVIEWS_CSV_PATH=https://raw.githubusercontent.com/hfhoffman1144/langchain_neo4j_rag_app/main/data/reviews.csv\n",
    "```\n",
    "\n",
    "<b>Notice that you’ve stored all of the CSV files in a public location on GitHub. Because your Neo4j AuraDB instance is running in the cloud, it can’t access files on your local machine, and you have to use HTTP or upload the files directly to your instance</b>. For this example, you can either use the link above, or upload the data to another location.\n",
    "\n",
    "Once you have your <font color='olive'>.env</font> file populated, open <font color='olive'>pyproject.toml</font>, which provides configuration, metadata, and dependencies defined in the [**TOML**](https://realpython.com/python-toml/) format:\n",
    "* `hospital_neo4j_etl/pyproject.toml`\n",
    "\n",
    "```toml\n",
    "[project]\n",
    "name = \"hospital_neo4j_etl\"\n",
    "version = \"0.1\"\n",
    "dependencies = [\n",
    "   \"neo4j==5.14.1\",\n",
    "   \"retry==0.9.2\"\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\"black\", \"flake8\"]\n",
    "```\n",
    "\n",
    "This project is a bare bones [extract, transform, load (ETL) process](https://en.wikipedia.org/wiki/Extract,_transform,_load) that moves data into Neo4j, so it’s only dependencies are [neo4j](https://pypi.org/project/neo4j/) and [retry](https://pypi.org/project/retry/). The main script for the ETL is <font color='olive'>hospital_neo4j_etl/src/hospital_bulk_csv_write.py</font> ([download](https://drive.google.com/file/d/1j9bWOX9qtUTuGZkCHDRzxRkzuC0RdREA/view?usp=drive_link)). It’s too long to include the full script here, but you’ll get a feel for the main steps. <font color='olive'>hospital_neo4j_etl/src/hospital_bulk_csv_write.py</font> executes. You can copy the full script from the materials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b96932-1522-40a4-8ccf-730bd5c36abf",
   "metadata": {},
   "source": [
    "First, you import dependencies, load environment variables, and configure logging:\n",
    "* `hospital_neo4j_etl/src/hospital_bulk_csv_write.py`\n",
    "\n",
    "```python\n",
    "import os\n",
    "import logging\n",
    "from retry import retry\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "HOSPITALS_CSV_PATH = os.getenv(\"HOSPITALS_CSV_PATH\")\n",
    "PAYERS_CSV_PATH = os.getenv(\"PAYERS_CSV_PATH\")\n",
    "PHYSICIANS_CSV_PATH = os.getenv(\"PHYSICIANS_CSV_PATH\")\n",
    "PATIENTS_CSV_PATH = os.getenv(\"PATIENTS_CSV_PATH\")\n",
    "VISITS_CSV_PATH = os.getenv(\"VISITS_CSV_PATH\")\n",
    "REVIEWS_CSV_PATH = os.getenv(\"REVIEWS_CSV_PATH\")\n",
    "\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s]: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abc582-f9e9-4e9d-a4df-e5f267b3260d",
   "metadata": {},
   "source": [
    "You import the GraphDatabase class from neo4j to connect to your running instance. Notice here that <b>you’re no longer using Python-dotenv to load environment variables. Instead, you’ll pass environment variables into the Docker container that runs your script. Next, you’ll define functions to move hospital data into Neo4j following your design</b>:\n",
    "* `hospital_neo4j_etl/src/hospital_bulk_csv_write.py`\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "NODES = [\"Hospital\", \"Payer\", \"Physician\", \"Patient\", \"Visit\", \"Review\"]\n",
    "\n",
    "def _set_uniqueness_constraints(tx, node):\n",
    "    query = f\"\"\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{node})\n",
    "        REQUIRE n.id IS UNIQUE;\"\"\"\n",
    "    _ = tx.run(query, {})\n",
    "\n",
    "\n",
    "@retry(tries=100, delay=10)\n",
    "def load_hospital_graph_from_csv() -> None:\n",
    "    \"\"\"Load structured hospital CSV data following\n",
    "    a specific ontology into Neo4j\"\"\"\n",
    "\n",
    "    driver = GraphDatabase.driver(\n",
    "        NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "    )\n",
    "\n",
    "    LOGGER.info(\"Setting uniqueness constraints on nodes\")\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        for node in NODES:\n",
    "            session.execute_write(_set_uniqueness_constraints, node)\n",
    "    # ...\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d4663-397b-409f-b79d-3a3ebebe8b1a",
   "metadata": {},
   "source": [
    "First, you define a helper function, <font color='blue'>_set_uniqueness_constraints()</font>, that creates and runs queries enforcing each node to have a unique ID. In <font color='blue'>load_hospital_graph_from_csv()</font>, you instantiate a driver that connects to your Neo4j instance and set uniqueness constraints for each hospital system node.\n",
    "\n",
    "Notice the <b><font color='orange'>@retry</font></b> decorator attached <font color='blue'>to load_hospital_graph_from_csv()</font>. If <font color='blue'>to load_hospital_graph_from_csv()</font> fails for any reason, this decorator will rerun it one hundred times with a ten second delay in between tries. This comes in handy when there are intermittent connection issues to Neo4j that are usually resolved by recreating a connection. However, be sure to check the script logs to see if an error reoccurs more than a few times.\n",
    "\n",
    "Next, <font color='blue'>load_hospital_graph_from_csv()</font> loads data for each node and relationship:\n",
    "* `hospital_neo4j_etl/src/hospital_bulk_csv_write.py`\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "@retry(tries=100, delay=10)\n",
    "def load_hospital_graph_from_csv() -> None:\n",
    "    \"\"\"Load structured hospital CSV data following\n",
    "    a specific ontology into Neo4j\"\"\"\n",
    "\n",
    "    # ...\n",
    "\n",
    "    LOGGER.info(\"Loading hospital nodes\")\n",
    "    with driver.session(database=\"neo4j\") as session:\n",
    "        query = f\"\"\"\n",
    "        LOAD CSV WITH HEADERS\n",
    "        FROM '{HOSPITALS_CSV_PATH}' AS hospitals\n",
    "        MERGE (h:Hospital {{id: toInteger(hospitals.hospital_id),\n",
    "                            name: hospitals.hospital_name,\n",
    "                            state_name: hospitals.hospital_state}});\n",
    "        \"\"\"\n",
    "        _ = session.run(query, {})\n",
    "\n",
    "   # ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_hospital_graph_from_csv()\n",
    "```\n",
    "\n",
    "Each node and relationship is loaded from their respective csv files and written to Neo4j according to your graph database design. At the end of the script, you call <font color='blue'>load_hospital_graph_from_csv()</font> in the [name-main idiom](https://realpython.com/if-name-main-python/), and all of the data should populate in your Neo4j instance.\n",
    "\n",
    "After writing <font color='olive'>hospital_neo4j_etl/src/hospital_bulk_csv_write.py</font>, you can define an <font color='olive'>entrypoint.sh</font> file that will run when your Docker container starts:\n",
    "* `hospital_neo4j_etl/src/entrypoint.sh`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "# Run any setup steps or pre-processing tasks here\n",
    "echo \"Running ETL to move hospital data from csvs to Neo4j...\"\n",
    "\n",
    "# Run the ETL script\n",
    "python hospital_bulk_csv_write.py\n",
    "```\n",
    "\n",
    "This entrypoint file isn’t technically necessary for this project, but <b>it’s a good practice when building containers because it allows you to execute necessary shell commands before running your main script</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eedbf3-ca25-44a1-86b4-f022449106cb",
   "metadata": {},
   "source": [
    "The last file to write for your ETL is the Docker file. It looks like this:\n",
    "* `hospital_neo4j_etl/Dockerfile`\n",
    "\n",
    "```yaml\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY ./src/ /app\n",
    "\n",
    "COPY ./pyproject.toml /code/pyproject.toml\n",
    "RUN pip install /code/.\n",
    "\n",
    "CMD [\"sh\", \"entrypoint.sh\"]\n",
    "```\n",
    "\n",
    "\n",
    "This <font color='olive'>Dockerfile</font> tells your container to use the `python:3.11-slim distribution`, copy the contents from <font color='olive'>hospital_neo4j_etl/src/</font> into the <font color='olive'>/app</font> directory within the container, install the dependencies from <font color='olive'>pyproject.toml</font>, and run <font color='olive'>entrypoint.sh</font>.\n",
    "\n",
    "You can now add this project to <font color='olive'>docker-compose.yml</font>:\n",
    "\n",
    "* `docker-compose.yml`:\n",
    "```yaml\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  hospital_neo4j_etl:\n",
    "    build:\n",
    "      context: ./hospital_neo4j_etl\n",
    "    env_file:\n",
    "      - .env\n",
    "```\n",
    "\n",
    "The ETL will run as a service called `hospital_neo4j_etl`, and it will run the <font color='olive'>Dockerfile</font> in <font color='olive'>./hospital_neo4j_etl</font> using environment variables from <font color='olive'>.env</font>. Since you only have one container, you don’t need docker-compose yet. However, you’ll add more containers to orchestrate with your ETL in the next section, so it’s helpful to get started on <font color='olive'>docker-compose.yml</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a96241-9ac7-4dcd-a61f-b10e59719c04",
   "metadata": {},
   "source": [
    "To run your ETL, open a terminal and run:\n",
    "```shell\n",
    "$ docker-compose up --build\n",
    "```\n",
    "\n",
    "Once the ETL finishes running, return to your Aura console:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-10_at_10.00.34_AM.0ca76879f1fc.png)\n",
    "<center>Aura console</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "Click Open and you’ll be prompted to enter your Neo4j password. After successfully logging into the instance, you should see a screen similar to this:\n",
    "![ui](https://files.realpython.com/media/Screenshot_2024-01-12_at_8.14.38_AM.72233e36a1e0.png)\n",
    "<center>Neo4j Aura instance with hospital system data loaded</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "As you can see under **Database** Information, all of the nodes, relationships, and properties were loaded. There are 21,187 nodes and 48,259 relationships. You’re ready to start writing queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9004-d944-4113-b6cc-4c6ee3497b7f",
   "metadata": {},
   "source": [
    "<a id='step3_5'></a>\n",
    "### <b><font color='darkgreen'>Query the Hospital System Graph</font></b> ([back](#step3))\n",
    "<b><font size='3ptx'>The last thing you need to do before building your chatbot is get familiar with [Cypher](https://neo4j.com/docs/getting-started/cypher-intro/) syntax.</font></b>\n",
    "\n",
    "Cypher is Neo4j’s query language, and it’s fairly intuitive to learn, especially if you’re familiar with SQL. This section will cover the basics, and that’s all you need to build the chatbot. You can check out [Neo4j’s documentation](https://neo4j.com/docs/getting-started/cypher-intro/) for a more comprehensive Cypher overview.\n",
    "\n",
    "TBR (To Be Read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ad49a-c6ef-4695-b0f8-39d3abf87e52",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## <b><font color='darkblue'>Step 4: Build a Graph RAG Chatbot in LangChain</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>After all the preparatory design and data work you’ve done so far, you’re finally ready to build your chatbot! </font></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
