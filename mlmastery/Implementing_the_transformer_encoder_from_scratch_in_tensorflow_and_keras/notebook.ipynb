{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc603ab6",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras)) Having seen how to implement the [**scaled dot-product attention**](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras), and integrate it within the [**multi-head attention**](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras) of the Transformer model, we may progress one step further towards implementing a complete Transformer model by implementing its encoder. Our end goal remains the application of the complete model to Natural Language Processing (<font color='brown'>NLP</font>).\n",
    "\n",
    "In this tutorial, you will discover how to implement the Transformer encoder from scratch in TensorFlow and Keras. After completing this tutorial, you will know:\n",
    "* The layers that form part of the Transformer encoder.\n",
    "* How to implement the Transformer encoder from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b62fca7",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "* [**Recap of the Transformer Architecture**](#sect1)\n",
    "    * [The Transformer Encoder](#sect1_1)\n",
    "* [**Implementing the Transformer Encoder From Scratch**](#sect2)\n",
    "    * [The Fully Connected Feed-Forward Neural Network and Layer Normalization](#sect2_1)\n",
    "    * [The Encoder Layer](#sect2_2)\n",
    "    * [The Transformer Encoder](#sect2_3)\n",
    "* [**Testing Out the Code**](#sect3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0beb09",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Prerequisites</font>\n",
    "For this tutorial, we assume that you are already familiar with:\n",
    "* [The Transformer model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "* [The scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)\n",
    "* [The multi-head attention](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)\n",
    "* [The Transformer positional encoding](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c712db8c",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Recap of the Transformer Architecture</font>\n",
    "[Recall](https://machinelearningmastery.com/the-transformer-model/) having seen that the Transformer architecture follows an encoder-decoder structure: the encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step, to generate an output sequence.\n",
    "![Transformer architecture](images/1.PNG)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In generating an output sequence, the Transformer does not rely on recurrence and convolutions.\n",
    "\n",
    "We had seen that the decoder part of the Transformer shares many similarities in its architecture with the encoder. <b>In this tutorial, we will be focusing on the components that form part of the Transformer encoder</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d02b2",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <font color='darkgreen'>The Transformer Encoder</font>\n",
    "The Transformer encoder consists of a stack of  identical layers, where each layer further consists of two main sub-layers:\n",
    "* **The first sub-layer comprises a multi-head attention mechanism** that receives the queries, keys and values as inputs.\n",
    "* **A second sub-layer that comprises a fully-connected feed-forward network**. \n",
    "\n",
    "<br/>\n",
    "\n",
    "Following each of these two sub-layers is layer normalisation, into which the sub-layer input (<font color='brown'>through a residual connection</font>) and output are fed. The output of each layer normalization step is the following:\n",
    "> LayerNorm(Sublayer Input + Sublayer Output)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In order to facilitate such an operation, which involves an addition between the sublayer input and output, Vaswani et al. designed all sub-layers and embedding layers in the model to produce outputs of dimension, $d_{model} = 512$\n",
    "\n",
    "[Recall](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras) as well the queries, keys and values as the inputs to the Transformer encoder.\n",
    "\n",
    "Here, the queries, keys and values carry the same input sequence after this has been embedded and augmented by positional information, where the queries and keys are of dimensionality, $d_{k}$, whereas the dimensionality of the values is $d_{v}$.\n",
    "\n",
    "Furthermore, Vaswani et al. also introduce regularization into the model by applying dropout to the output of each sub-layer (<font color='brown'>before the layer normalization step</font>), as well as to the positional encodings before these are fed into the encoder. \n",
    "\n",
    "Let’s now see how to implement the Transformer encoder from scratch in TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78cb1d",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Implementing the Transformer Encoder From Scratch</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12f709",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>The Fully Connected Feed-Forward Neural Network and Layer Normalization</font>\n",
    "<b><font size='3ptx'>We shall begin by creating classes for the Feed Forward and Add & Norm layers that are shown in the diagram above.</font></b>\n",
    "\n",
    "Vaswani et al. tell us that the fully connected feed-forward network consists of two linear transformations with a ReLU activation in between. The first linear transformation produces an output of dimensionality, $d_{ff} = 2048$, while the second linear transformation produces an output of dimensionality, $d_{model} = 512$.\n",
    "\n",
    "For this purpose, let’s first create the class, <b><font color='blue'>FeedForward</font></b> that inherits form the [**Layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) base class in Keras, and initialize the dense layers and the ReLU activation:\n",
    "```python\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "We will add to it the class method, <font color='blue'>call()</font>, that receives an input and passes it through the two fully connected layers with ReLU activation, returning an output of dimensionality equal to 512:\n",
    "```python\n",
    "...\n",
    "def call(self, x):\n",
    "    # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "    x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "    return self.fully_connected2(self.activation(x_fc1))\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The next step is to create another class, <font color='blue'><b>AddNormalization</b></font>, that also inherits form the [**Layer**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) base class in Keras, and initialize a Layer normalization layer:\n",
    "```python\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "In it, we will include the following class method that <b>sums its sub-layer’s input and output, which it receives as inputs, and applies layer normalization to the result</b>:\n",
    "```python\n",
    "...\n",
    "def call(self, x, sublayer_x):\n",
    "    # The sublayer input and output need to be of the same shape to be summed\n",
    "    add = x + sublayer_x\n",
    "\n",
    "    # Apply layer normalization to the sum\n",
    "    return self.layer_norm(add)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e613cad",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <font color='darkgreen'>The Encoder Layer</font>\n",
    "<b><font size='3ptx'>Next, we will implement the encoder layer, which the Transformer encoder will replicate identically `N` times. </font></b>\n",
    "\n",
    "For this purpose, let’s create the class, <font color='blue'><b>EncoderLayer</b></font>, and initialize all of the sub-layers that it consists of:\n",
    "```python\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Here you may notice that we have initialized instances of the <b><font color='blue'>FeedForward</font></b> and <b><font color='blue'>AddNormalization</font></b> classes, which we have just created in the previous section, and assigned their output to the respective variables, `feed_forward` and `add_norm` (<font color='brown'>1 and 2</font>). The [**Dropout**](https://keras.io/api/layers/regularization_layers/dropout/) layer is self-explanatory, where rate defines the frequency at which the input units are set to 0. We had created the <b><font color='blue'>MultiHeadAttention</font></b> class in a [previous tutorial](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras), and if you had saved the code into a separate Python script, then do not forget to import it. I saved mine in a Python script named, `multihead_attention.py`, and for this reason I need to include the line of code.\n",
    "\n",
    "Let’s now proceed to create the class method, <font color='blue'>call()</font>, that implements all of the encoder sub-layers:\n",
    "```python\n",
    "...\n",
    "def call(self, x, padding_mask, training):\n",
    "    # Multi-head attention layer\n",
    "    multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Add in a dropout layer\n",
    "    multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "    # Followed by an Add & Norm layer\n",
    "    addnorm_output = self.add_norm1(x, multihead_output)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Followed by a fully connected layer\n",
    "    feedforward_output = self.feed_forward(addnorm_output)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Add in another dropout layer\n",
    "    feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "    # Followed by another Add & Norm layer\n",
    "    return self.add_norm2(addnorm_output, feedforward_output)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "In addition to the input data, the <font color='blue'>call()</font> method can also receive a `padding mask`. As a brief reminder of what we had said in a [previous tutorial](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras), <b>the `padding mask` is necessary to suppress the zero padding in the input sequence from being processed along with the actual input values</b>. \n",
    "\n",
    "The same class method can receive a <font color='violet'>training</font> flag which, when set to True, will only apply the [**Dropout**](https://keras.io/api/layers/regularization_layers/dropout/) layers during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b21af9",
   "metadata": {},
   "source": [
    "<a id='sect2_3'></a>\n",
    "### <font color='darkgreen'>The Transformer Encoder</font>\n",
    "The last step is to create a class for the Transformer encoder, which we shall be naming <b><font color='blue'>Encoder</font></b>:\n",
    "```python\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The Transformer encoder receives an input sequence after this would have undergone a process of word embedding and positional encoding. In order to compute the positional encoding, we will make use of the <b><font color='blue'>PositionEmbeddingFixedWeights</font></b> class described by Mehreen Saeed in [this tutorial](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/). \n",
    "\n",
    "As we have similarly done in the previous sections, here we will also create a class method, <font color='blue'>call()</font>, that applies word embedding and positional encoding to the input sequence, and feeds the result to `N` encoder layers:\n",
    "```python\n",
    "...\n",
    "def call(self, input_sentence, padding_mask, training):\n",
    "    # Generate the positional encoding\n",
    "    pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Add in a dropout layer\n",
    "    x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "    # Pass on the positional encoded values to each encoder layer\n",
    "    for i, layer in enumerate(self.encoder_layer):\n",
    "        x = layer(x, padding_mask, training)\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The code listing for the full Transformer encoder is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ee1f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "      \n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "      \n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d2075",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Testing Out the Code</font>\n",
    "We will be working with the parameter values specified in the paper, [Attention Is All You Need](https://arxiv.org/abs/1706.03762), by Vaswani et al. (2017):\n",
    "```python\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "As for the input sequence we will be working with dummy data for the time being until we arrive to the stage of [training the complete Transformer model](https://machinelearningmastery.com/training-the-transformer-model) in a separate tutorial, at which point we will be using actual sentences:\n",
    "```python\n",
    "...\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, we will create a new instance of the <b><font color='blue'>Encoder</font></b> class, assigning its output to the `encoder` variable, and subsequently feeding in the input arguments and printing the result. We will be setting the padding mask argument to `None` for the time being, but we shall return to this when we implement the complete Transformer model:\n",
    "```python\n",
    "...\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Tying everything together produces the following code listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65cb17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 19:39:48.589721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-15 19:39:48.590485: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-15 19:39:48.590616: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-10-15 19:39:48.593622: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.55818576  0.7659338  -1.5065082  ...  1.544514    2.3999965\n",
      "    1.2609208 ]\n",
      "  [ 0.7113264   0.180385   -0.42705274 ...  0.4240638   2.4677043\n",
      "    2.0552928 ]\n",
      "  [ 0.91591203  0.34180903 -1.0722297  ...  0.9968933   2.822958\n",
      "    2.5977676 ]\n",
      "  [ 0.55982953  0.08902823 -0.49637258 ...  1.6795886   2.3831027\n",
      "    1.269558  ]\n",
      "  [ 1.100009    0.22682343  0.07365907 ...  1.1378539   2.7020812\n",
      "    1.6387043 ]]\n",
      "\n",
      " [[ 0.7001849   1.4314605  -0.90819865 ...  0.56616014  1.197535\n",
      "    2.247654  ]\n",
      "  [ 0.6184092   1.5629302  -0.85190463 ...  0.9729141   2.1737888\n",
      "    1.0227102 ]\n",
      "  [ 1.107866    1.4481907  -0.60490626 ...  0.11705832  1.7010908\n",
      "    1.5384034 ]\n",
      "  [ 1.0612534   1.7052234  -0.94907767 ...  1.3872211   1.9995008\n",
      "    1.2116961 ]\n",
      "  [ 0.4205389   1.2733024  -0.60228366 ...  0.90708643  1.1303109\n",
      "    1.4997379 ]]\n",
      "\n",
      " [[-0.30979607  0.09501119 -2.1249964  ...  1.8940775   1.7723631\n",
      "    1.9347492 ]\n",
      "  [ 0.3617029   0.33774242 -0.8522819  ...  2.098174    2.254145\n",
      "    2.1955688 ]\n",
      "  [ 0.45732135 -0.49355173 -0.86009187 ...  1.475035    2.1730256\n",
      "    1.3692477 ]\n",
      "  [ 0.3983909   0.42249826 -0.13618745 ...  2.3937504   2.007609\n",
      "    1.6914539 ]\n",
      "  [ 1.1287411   0.3640838  -1.370296   ...  0.6316128   2.3123946\n",
      "    1.8350788 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.054498    1.687373   -1.2216005  ...  1.6735715   1.810355\n",
      "    1.5712821 ]\n",
      "  [ 0.44331402  1.3357575  -0.23721555 ...  1.8578154   0.6559343\n",
      "    1.5649999 ]\n",
      "  [ 1.3873794   1.0879698  -0.6411762  ...  1.5764316   1.0912489\n",
      "    0.8943808 ]\n",
      "  [ 0.39695722  1.3074441   0.44260746 ...  1.3638469   1.8475256\n",
      "    0.18073446]\n",
      "  [ 1.2793202   0.98078924 -0.6391253  ...  1.1421535   1.7443819\n",
      "    0.89149135]]\n",
      "\n",
      " [[-0.14514746  0.8196607  -1.116268   ...  1.2476847   1.4195602\n",
      "    1.9307822 ]\n",
      "  [ 0.3975922   0.95002294 -0.6977847  ...  2.2006822   1.4220139\n",
      "    0.9033996 ]\n",
      "  [ 1.1111596   0.9736049  -0.27019882 ...  1.3823891   1.8063846\n",
      "    1.5476708 ]\n",
      "  [ 0.08889437  0.598298   -1.2935741  ...  1.0701207   1.5168847\n",
      "    1.3345922 ]\n",
      "  [ 0.5682256   0.5628639  -0.4883073  ...  2.256435    0.8106155\n",
      "    1.4424943 ]]\n",
      "\n",
      " [[ 0.54660153  1.3897685  -1.0183291  ...  1.561873    1.5684334\n",
      "    0.9927205 ]\n",
      "  [ 0.22518513  1.2486476  -1.7466505  ...  1.2376179   1.6758369\n",
      "    1.691335  ]\n",
      "  [ 0.30459303  1.3822768  -1.384917   ...  1.4173994   1.5911179\n",
      "    1.5871587 ]\n",
      "  [-0.5064656   1.1514705  -1.1280965  ...  1.431838    1.901881\n",
      "    1.7581242 ]\n",
      "  [ 0.06487929  0.35053545 -0.95652664 ...  0.07849313  1.9960599\n",
      "    0.99821854]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a55e34d",
   "metadata": {},
   "source": [
    "Running this code produces an output of shape, `(batch size, sequence length, model dimensionality)`. Note that you will likely see a different output due to the random initialization of the input sequence, and the parameter values of the Dense layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbfb041",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>Further Reading</font>\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "* **Books**\n",
    "    * [Advanced Deep Learning with Python, 2019.](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)\n",
    "    * [Transformers for Natural Language Processing, 2021. ](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)\n",
    "* **Papers**\n",
    "    * [Attention Is All You Need, 2017.](https://arxiv.org/abs/1706.03762)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
