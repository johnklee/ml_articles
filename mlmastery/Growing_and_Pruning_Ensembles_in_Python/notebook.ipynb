{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/growing-and-pruning-ensembles-in-python/)) <font size='3ptx'>**Ensemble member selection refers to algorithms that optimize the composition of an ensemble.**</font>\n",
    "\n",
    "This may involve growing an ensemble from available models or pruning members from a fully defined ensemble.\n",
    "\n",
    "The goal is often to reduce the model or computational complexity of an ensemble with little or no effect on the performance of an ensemble, and in some cases find a combination of ensemble members that results in better performance than blindly using all contributing models directly.\n",
    "\n",
    "**In this tutorial, you will discover how to develop ensemble selection algorithms from scratch.**\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "* Ensemble selection involves choosing a subset of ensemble members that results in lower complexity than using all members and sometimes better performance.\n",
    "* How to develop and evaluate a greedy ensemble pruning algorithm for classification.\n",
    "* How to develop and evaluate an algorithm for greedily growing an ensemble from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into four parts; they are:\n",
    "* <font size='3ptx'>[**Ensemble Member Selection**](#sect1)</font>\n",
    "* <font size='3ptx'>[**Baseline Models and Voting**](#sect2)</font>\n",
    "* <font size='3ptx'>[**Ensemble Pruning Example**](#sect3)</font>\n",
    "* <font size='3ptx'>[**Ensemble Growing Example**](#sect4)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Ensemble Member Selection</font>\n",
    "<font size='3ptx'>**[Voting](https://machinelearningmastery.com/voting-ensembles-with-python/) and [stacking](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) ensembles typically combine the predictions from a heterogeneous group of model types.**</font>\n",
    "\n",
    "Although the ensemble may have a large number of ensemble members, it is hard to know that the best combination of members is being used by the ensemble. For example, **instead of simply using all members, it is possible that better results could be achieved by adding one more different model type or removing one or more models**.\n",
    "\n",
    "This can be addressed using a weighted average ensemble and using an optimization algorithm to find an appropriate weighting for each member, allowing some members to have a zero weight, which effectively removes them from the ensemble. <font color='darkred'>**The problem with a weighted average ensemble is that all models remain part of the ensemble, perhaps requiring an ensemble of greater complexity than is required to be developed and maintained.**</font>\n",
    "\n",
    "**An alternative approach is to optimize the composition of the ensemble itself**. The general approach of automatically choosing or optimizing the members of ensembles is referred to as ensemble selection.\n",
    "\n",
    "Two common approaches include <font color='darkblue'>**ensemble growing**</font> and <font color='darkblue'>**ensemble pruning**</font>:\n",
    "* **Ensemble Growing:** Add members to the ensemble until no further improvement is observed.\n",
    "* **Ensemble Pruning:** Remove members from the ensemble until no further improvement is observed.\n",
    "\n",
    "**<font color='darkblue'>Ensemble growing</font> is a technique where the model starts with no members and involves adding new members until no further improvement is observed.** This could be performed in a greedy manner where members are added one at a time only if they result in an improvement in model performance.\n",
    "\n",
    "**<font color='darkblue'>Ensemble pruning</font> is a technique where the model starts with all possible members that are being considered and removes members from the ensemble until no further improvement is observed.** This could be performed in a greedy manner where members are removed one at a time and only if their removal results in a lift in the performance of the overall ensemble.\n",
    "> Given a set of trained individual learners, rather than combining all of them, ensemble pruning tries to select a subset of individual learners to comprise the ensemble. <br/><br/>\n",
    "> [**— Page 119, Ensemble Methods: Foundations and Algorithms, 2012.**](https://amzn.to/2TavTcy)\n",
    "\n",
    "**An advantage of ensemble pruning and growing is that it may result in an ensemble with a smaller size** (<font color='brown'>lower complexity</font>) **and/or an ensemble with better predictive performance**. Sometimes a small drop in performance is desirable if it can be achieved in a large drop in model complexity and resulting maintenance burden. Alternately, on some projects, predictive skill is more important than all other concerns, and ensemble selection provides one more strategy to try and get the most out of the contributing models.\n",
    "> There are two main reasons for reducing the ensemble size: a) Reducing computational overhead: Smaller ensembles require less computational overhead and b) Improving Accuracy: Some members in the ensemble may reduce the predictive performance of the whole. <br/><br/>\n",
    "> [**— Page 119, Pattern Classification Using Ensemble Methods, 2010.**](https://amzn.to/2zxc0F7)\n",
    "\n",
    "Ensemble growing might be preferred for computational efficiency reasons in cases where a small number of ensemble members are expected to perform better, whereas ensemble pruning would be more efficient in cases where a large number of ensemble members may be expected to perform better.\n",
    "\n",
    "Simple greedy ensemble growing and pruning have a lot in common with stepwise feature selection techniques, such as those used in regression (<font color='brown'>e.g. so-called stepwise regression</font>).\n",
    "\n",
    "More sophisticated techniques may be used, such as selecting members for addition to or removal from the ensemble based on their standalone performance on the dataset, or even through the use of a global search procedure that attempts to find a combination of ensemble members that results in the best overall performance.\n",
    "> … one can perform a heuristic search in the space of the possible different ensemble subsets while evaluating the collective merit of a candidate subset. <br/><br/>\n",
    "> [**— Page 123, Pattern Classification Using Ensemble Methods, 2010.**](https://amzn.to/2zxc0F7)\n",
    "\n",
    "Now that we are familiar with ensemble selection methods, let’s explore how we might implement ensemble pruning and ensemble growing in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Baseline Models and Voting</font>\n",
    "<font size='3ptx'>**Before we dive into developing growing and pruning ensembles, let’s first establish a dataset and baseline.**</font>\n",
    "\n",
    "We will use a synthetic binary classification problem as the basis for this investigation, defined by the [make_classification() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) with **5,000 examples and 20 numerical input features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 20) (5000,)\n"
     ]
    }
   ],
   "source": [
    "# test classification dataset\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_informative=10, n_redundant=10, random_state=1)\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can choose some candidate models that will provide the basis for our ensemble.\n",
    "\n",
    "We will use five standard machine learning models, including logistic regression ([**LogisticRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), naive Bayes ([**GaussianNB**](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)), decision tree ([**DecisionTreeClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#)), support vector machine ([**SVC**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)), and a k-nearest neighbor algorithm ([**KNeighborsClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)).\n",
    "\n",
    "First, we can define a function that will create each model with default hyperparameters. Each model will be defined as a tuple with a name and the model object, then added to a list. This is a helpful structure both for enumerating the models with their names for standalone evaluation and for later use in an ensemble.\n",
    "\n",
    "The <font color='blue'>get_models()</font> function below implements this and returns the list of models to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = list()\n",
    "    models.append(('lr', LogisticRegression()))\n",
    "    models.append(('knn', KNeighborsClassifier()))\n",
    "    models.append(('tree', DecisionTreeClassifier()))\n",
    "    models.append(('nb', GaussianNB()))\n",
    "    models.append(('svm', SVC(probability=True)))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a function that takes a single model and the dataset and evaluates the performance of the model on the dataset. We will evaluate a model using [repeated stratified k-fold cross-validation](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/) with 10 folds and three repeats, a good practice in machine learning.\n",
    "\n",
    "The <font color='blue'>evaluate_model()</font> function below implements this and returns a list of scores across all folds and repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # define the model evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create the list of models and enumerate them, reporting the performance of each on the synthetic dataset in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr 0.856 (0.014)\n",
      ">knn 0.953 (0.008)\n",
      ">tree 0.865 (0.016)\n",
      ">nb 0.847 (0.021)\n",
      ">svm 0.953 (0.010)\n"
     ]
    }
   ],
   "source": [
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "\n",
    "for name, model in models:\n",
    "    # evaluate model\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    \n",
    "    # store results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    \n",
    "    # summarize result\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model performance for comparison\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we need to establish a baseline ensemble that uses all models**. This will provide a point of comparison with growing and pruning methods that seek better performance with a smaller subset of models.\n",
    "\n",
    "In this case, we will use a [**voting ensemble**](https://machinelearningmastery.com/voting-ensembles-with-python/) with soft voting. This means that each model will predict probabilities and the probabilities will be summed by the ensemble model to choose a final output prediction for each input sample.\n",
    "\n",
    "This can be achieved using the [**VotingClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) class where the members are set via the “<font color='violet'>estimators</font>” argument, which expects a list of models where each model is a tuple with a name and configured model object, just as we defined in the previous section.\n",
    "\n",
    "We can then set the type of voting to perform via the “voting” argument, which in this case is set to “<font color='violet'>soft</font>”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.930 (0.012)\n"
     ]
    }
   ],
   "source": [
    "# create the ensemble\n",
    "ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the ensemble\n",
    "scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# summarize the result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the **voting ensemble achieved a mean accuracy of about 93.0 percent. This is lower than SVM and KNN models used alone that achieved an accuracy of about 95.3 percent.**\n",
    "\n",
    "<font color='darkred'>**This result highlights that a simple voting ensemble of all models results in a model with higher complexity and worse performance in this case**</font>. Perhaps we can find a subset of members that performs better than any single model and has lower complexity than simply using all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Ensemble Pruning Example</font> ([back](#sect0))\n",
    "<font size='3ptx'>**In this section, we will explore how to develop a greedy ensemble pruning algorithm from scratch.**</font>\n",
    "\n",
    "We will use a greedy algorithm in this case, which is straightforward to implement. This involves removing one member from the ensemble and evaluating the performance and repeating this for each member in the ensemble. The member that, if removed, results in the best improvement in performance is then permanently removed from the ensemble and the process repeats. This continues until no further improvements can be achieved.\n",
    "\n",
    "**It is referred to as a “greedy” algorithm because it seeks the best improvement at each step. It is possible that the best combination of members is not on the path of greedy improvements**, in which case the greedy algorithm will not find it and a global optimization algorithm could be used instead.\n",
    "\n",
    "First, we can define a function to evaluate a candidate list of models. This function will take the list of models and the dataset and construct a voting ensemble from the list of models and evaluate its performance using [repeated stratified k-fold cross-validation](https://machinelearningmastery.com/repeated-k-fold-cross-validation-with-python/), returning the mean classification accuracy.\n",
    "\n",
    "This function can be used to evaluate each candidate’s removal from the ensemble. The <font color='blue'>evaluate_ensemble()</font> function below implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a list of models\n",
    "def evaluate_ensemble(models, X, y):\n",
    "    # check for no models    \n",
    "    if len(models) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # create the ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='soft')\n",
    "    \n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    \n",
    "    # evaluate the ensemble\n",
    "    scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "    # return mean score\n",
    "    return mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define a function that performs a single round of pruning.\n",
    "\n",
    "First, a baseline in performance is established with all models that are currently in the ensemble. Then the list of models is enumerated and each is removed in turn, and the effect of removing the model is evaluated on the ensemble. **If the removal results in an improvement in performance, the new score and specific model that was removed is recorded.**\n",
    "\n",
    "Importantly, the trial removal is performed on a copy of the list of models, not on the main list of models itself. This is to ensure we only remove an ensemble member from the list once we know it will result in the best possible improvement from all the members that could potentially be removed at the current step.\n",
    "\n",
    "The <font color='blue'>prune_round()</font> function below implements this given the list of current models in the ensemble and dataset, and returns the improvement in score (<font color='brown'>if any</font>) and the best model to remove to achieve that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a single round of pruning the ensemble\n",
    "def prune_round(models_in, X, y):\n",
    "    # establish a baseline\n",
    "    baseline = evaluate_ensemble(models_in, X, y)\n",
    "    best_score, removed = baseline, None\n",
    "    # enumerate removing each candidate and see if we can improve performance\n",
    "    for mi in range(len(models_in)):\n",
    "        # model to be removed\n",
    "        m = models_in[mi]\n",
    "        \n",
    "        # evaluate new ensemble\n",
    "        result = evaluate_ensemble(models_in[:mi] + models_in[mi+1:], X, y)\n",
    "\n",
    "        # check for new best\n",
    "        if result > best_score:\n",
    "            # store the new best\n",
    "            best_score, removed = result, m\n",
    "\n",
    "    return best_score, removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to drive the pruning process.\n",
    "\n",
    "This involves running a round of pruning until no further improvement in accuracy is achieved by calling the <font color='blue'>prune_round()</font> function repeatedly.\n",
    "\n",
    "If the function returns None for the model to be removed, we know that no single greedy improvement is possible and we can return the final list of models. Otherwise, the chosen model is removed from the ensemble and the process continues.\n",
    "\n",
    "The <font color='blue'>prune_ensemble()</font> function below implements this and returns the models to use in the final ensemble and the score that it achieved via our evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune an ensemble from scratch\n",
    "def prune_ensemble(models, X, y):\n",
    "    best_score = 0.0\n",
    "    # prune ensemble until no further improvement\n",
    "    while True:\n",
    "        # remove one model to the ensemble\n",
    "        score, removed = prune_round(models, X, y)\n",
    "\n",
    "        # check for no improvement\n",
    "        if removed is None:\n",
    "            print('>no further improvement')\n",
    "            break\n",
    "\n",
    "        # keep track of best score\n",
    "        best_score = score\n",
    "\n",
    "        # remove model from the list\n",
    "        models.remove(removed)\n",
    "\n",
    "        # report results along the way\n",
    "        print('>%.3f (removed: %s)' % (score, removed[0]))\n",
    "        \n",
    "    return best_score, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how `ensemble pruning` goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.940 (removed: nb)\n",
      ">0.949 (removed: tree)\n",
      ">0.956 (removed: lr)\n",
      ">no further improvement\n",
      "Models: knn,svm\n",
      "Final Mean Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# prune the ensemble\n",
    "score, model_list = prune_ensemble(models, X, y)\n",
    "names = ','.join([n for n,_ in model_list])\n",
    "print('Models: %s' % names)\n",
    "print('Final Mean Accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that three rounds of pruning were performed, **removing the naive Bayes, decision tree, and logistic regression algorithms, leaving only the SVM and KNN algorithms that achieved a mean classification accuracy of about 95.6 percent. This is better than the 95.3 percent achieved by SVM and KNN used in a standalone manner**, and clearly better than combining all models together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>Ensemble Growing Example</font> ([back](#sect0))\n",
    "<font size='3ptx'>**In this section, we will explore how to develop a greedy ensemble growing algorithm from scratch.**</font>\n",
    "\n",
    "The structure of greedily growing an ensemble is much like the greedy pruning of members, although in reverse. We start with an ensemble with no models and add a single model that has the best performance. Models are then added one by one only if they result in a lift in performance over the ensemble before the model was added.\n",
    "\n",
    "**Much of the code is the same as the pruning case so we can focus on the differences.**\n",
    "\n",
    "First, we must define a function to perform one round of growing the ensemble. This involves enumerating all candidate models that could be added and evaluating the effect of adding each in turn to the ensemble. The single model that results in the biggest improvement is then returned by the function, along with its score.\n",
    "\n",
    "The <font color='blue'>grow_round()</font> function below implements this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a single round of growing the ensemble\n",
    "def grow_round(models_in, models_candidate, X, y):\n",
    "    # establish a baseline\n",
    "    baseline = evaluate_ensemble(models_in, X, y)\n",
    "    best_score, addition = baseline, None\n",
    "    \n",
    "    # enumerate adding each candidate and see if we can improve performance\n",
    "    for mi in range(len(models_candidate)):\n",
    "        # model to be evaluated by adding into ensemble\n",
    "        m = models_candidate[mi]\n",
    "        \n",
    "        # add the candidate\n",
    "        dup = models_in.copy()\n",
    "        dup.append(m)\n",
    "        \n",
    "        # evaluate new ensemble\n",
    "        result = evaluate_ensemble(dup, X, y)\n",
    "        \n",
    "        # check for new best\n",
    "        if result > best_score:\n",
    "            # store the new best\n",
    "            best_score, addition = result, m\n",
    "            \n",
    "    return best_score, addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function to drive the growing procedure.\n",
    "\n",
    "This involves a loop that runs rounds of growing until no further additions can be made resulting in an improvement in model performance. For each addition that can be made, the main list of models in the ensemble is updated and the list of models currently in the ensemble is reported along with the performance.\n",
    "\n",
    "The <font color='blue'>grow_ensemble()</font> function implements this and returns the list of models greedily determined to result in the best performance along with the expected mean accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grow an ensemble from scratch\n",
    "def grow_ensemble(models, X, y):\n",
    "    best_score, best_list = 0.0, list()\n",
    "    \n",
    "    # grow ensemble until no further improvement\n",
    "    while True:\n",
    "        # add one model to the ensemble\n",
    "        score, addition = grow_round(best_list, models, X, y)\n",
    "        \n",
    "        # check for no improvement\n",
    "        if addition is None:\n",
    "            print('>no further improvement')\n",
    "            break\n",
    "            \n",
    "        # keep track of best score\n",
    "        best_score = score\n",
    "        \n",
    "        # remove new model from the list of candidates\n",
    "        models.remove(addition)\n",
    "\n",
    "        # add new model to the list of models in the ensemble\n",
    "        best_list.append(addition)\n",
    "\n",
    "        # report results along the way\n",
    "        names = ','.join([n for n,_ in best_list])\n",
    "        print('>%.3f (%s)' % (score, names))\n",
    "        \n",
    "    return best_score, best_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how `ensemble growing` goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.953 (svm)\n",
      ">0.956 (svm,knn)\n",
      ">no further improvement\n",
      "Models: svm,knn\n",
      "Final Mean Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# grow the ensemble\n",
    "score, model_list = grow_ensemble(models, X, y)\n",
    "names = ','.join([n for n,_ in model_list])\n",
    "print('Models: %s' % names)\n",
    "print('Final Mean Accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that **ensemble growing found the same solution as greedy ensemble pruning where an ensemble of SVM and KNN achieved a mean classification accuracy of about 95.6 percent, an improvement over any single standalone model and over combining all models**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
