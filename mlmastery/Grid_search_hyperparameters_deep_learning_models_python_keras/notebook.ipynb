{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428692a2",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)) <font size='3ptx'><b>Hyperparameter optimization is a big part of deep learning.</b> The reason is that neural networks are notoriously difficult to configure and there are a lot of parameters that need to be set. On top of that, individual models can be very slow to train.</font>\n",
    "\n",
    "In this post you will discover how you can use the grid search capability from the scikit-learn python machine learning library to tune the hyperparameters of Keras deep learning models. After reading this post you will know:\n",
    "* How to wrap Keras models for use in scikit-learn and how to use grid search.\n",
    "* How to grid search common neural network parameters such as learning rate, dropout rate, epochs and number of neurons.\n",
    "* How to define your own hyperparameter tuning experiments on your own projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a37e2",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "### <font color='darkgreen'>Overview</font>\n",
    "In this post, I want to show you both how you can use the scikit-learn grid search capability and give you a suite of examples that you can copy-and-paste into your own project as a starting point.\n",
    "\n",
    "Below is a list of the topics we are going to cover:\n",
    "1. <font size='3ptx'><b><a href='#sect1'>How to use Keras models in scikit-learn.</a></b></font>\n",
    "2. <font size='3ptx'><b><a href='#sect2'>How to use grid search in scikit-learn.</a></b></font>\n",
    "3. <font size='3ptx'><b><a href='#sect3'>How to tune batch size and training epochs.</a></b></font>\n",
    "4. <font size='3ptx'><b><a href='#sect4'>How to tune optimization algorithms.</a></b></font>\n",
    "5. <font size='3ptx'><b><a href='#sect5'>How to tune learning rate and momentum.</a></b></font>\n",
    "6. <font size='3ptx'><b><a href='#sect6'>How to tune network weight initialization.</a></b></font>\n",
    "7. <font size='3ptx'><b><a href='#sect7'>How to tune activation functions.</a></b></font>\n",
    "8. <font size='3ptx'><b><a href='#sect8'>How to tune dropout regularization.</a></b></font>\n",
    "9. <font size='3ptx'><b><a href='#sect9'>How to tune the number of neurons in the hidden layer.</a></b></font>\n",
    "10. <font size='3ptx'><b><a href='#sect10'>Tips for Hyperparameter Optimization</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d832263",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>How to Use Keras Models in scikit-learn</font>\n",
    "<font size='3ptx'><b>Keras models can be used in scikit-learn by wrapping them with the <font color='blue'>KerasClassifier</font> or <font color='blue'>KerasRegressor</font> class from the module [SciKeras](https://pypi.org/project/scikeras/)</b>. You may need to run the command <font color='blue'>pip install scikeras</font> first to install the module.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fc214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3476cf",
   "metadata": {},
   "source": [
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the model argument when constructing the <b><font color='blue'>KerasClassifier</font></b> class. For example:\n",
    "```python\n",
    "def create_model():\n",
    "\t...\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(model=create_model)\n",
    "```\n",
    "<br/>\n",
    "\n",
    "The constructor for the <b><font color='blue'>KerasClassifier</font></b> class can also take new arguments that can be passed to your custom <font color='blue'>create_model()</font> function. These new arguments must also be defined in the signature of your <font color='blue'>create_model()</font> function with default parameters:\n",
    "```python\n",
    "def create_model(dropout_rate=0.0):\n",
    "\t...\n",
    "\treturn model\n",
    "\n",
    "model = KerasClassifier(model=create_model, dropout_rate=0.2)\n",
    "```\n",
    "<br/>\n",
    "\n",
    "You can learn more about these from the [SciKeras documentation](https://www.adriangb.com/scikeras/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9080122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "np.seterr(all=\"ignore\")\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d9533",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>How to Use Grid Search in scikit-learn</font>\n",
    "<font size='3ptx'><b>Grid search is a model hyperparameter optimization technique.</b> In scikit-learn this technique is provided in the [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class.</font>\n",
    "\n",
    "When constructing this class you must provide a dictionary of hyperparameters to evaluate in the <font color='violet'>param_grid</font> argument. This is a map of the model parameter name and an array of values to try.\n",
    "\n",
    "By default, accuracy is the score that is optimized, but other scores can be specified in the <font color='violet'>score</font> argument of the [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) constructor.\n",
    "\n",
    "<b>By default, the grid search will only use one thread. By setting the <font color='violet'>n_jobs</font> argument in the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) constructor to -1, the process will use all cores on your machine</b>. However, sometimes this may interfere with the main neural network training process.\n",
    "\n",
    "The [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) process will then construct and evaluate one model for each combination of parameters. <b>Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used</b>, although this can be overridden by specifying the <font color='violet'>c</font>v argument to the [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) constructor.\n",
    "\n",
    "Below is an example of defining a simple grid search:\n",
    "```python\n",
    "param_grid = dict(epochs=[10,20,30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "```\n",
    "<br/>\n",
    "\n",
    "Once completed, you can access the outcome of the grid search in the result object returned from <font color='blue'>grid.fit()</font>. The `best_score_` member provides access to the best score observed during the optimization procedure and the `best_params_` describes the combination of parameters that achieved the best results.\n",
    "\n",
    "You can learn more about the [GridSearchCV class in the scikit-learn API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202db732",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Problem Description</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>Now that we know how to use Keras models with scikit-learn and how to use grid search in scikit-learn, let’s look at a bunch of examples.</b> All examples will be demonstrated on a small standard machine learning dataset called the [Pima Indians onset of diabetes classification dataset](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes). This is a small dataset with all numerical attributes that is easy to work with.</font> \n",
    "\n",
    "[Download the dataset](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) and place it in path <font color='olive'>datas/kaggle_pima-indians-diabetes-database/diabetes.csv</font>.\n",
    "\n",
    "<b>As we proceed through the examples in this post, we will aggregate the best parameters</b>. This is not the best way to grid search because parameters can interact, but it is good for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460a164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(\"../../datas/kaggle_pima-indians-diabetes-database/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54a209e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.190</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2</td>\n",
       "      <td>96</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.647</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "178            5      143             78              0        0  45.0   \n",
       "26             7      147             76              0        0  39.4   \n",
       "134            2       96             68             13       49  21.1   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "178                     0.190   47        0  \n",
       "26                      0.257   43        1  \n",
       "134                     0.647   26        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb84ce",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>How to Tune Batch Size and Number of Epochs</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>In this first simple example, we look at tuning the batch size and number of epochs used when fitting the network.</b></font>\n",
    "\n",
    "The **batch size** in [iterative gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method) is the number of patterns shown to the network before the weights are updated. It is also an optimization in the training of the network, defining how many patterns to read at a time and keep in memory.\n",
    "\n",
    "The **number of epochs** is the number of times that the entire training dataset is shown to the network during training. Some networks are sensitive to the batch size, such as LSTM recurrent neural networks and Convolutional Neural Networks.\n",
    "\n",
    "Here we will evaluate a suite of different mini batch sizes from 10 to 100 in steps of 20.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff69532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8144e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8367229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = df.loc[:, df.columns != 'Outcome']\n",
    "Y = df['Outcome']\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee73d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7fbdf46790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2022-07-09 16:48:54.267675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-09 16:48:54.267728: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-09 16:48:54.267759: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2022-07-09 16:48:54.268119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.99 s, sys: 4.12 s, total: 10.1 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb46667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.691406 using {'batch_size': 20, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16411911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610677 (0.023939) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.674479 (0.048824) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.688802 (0.020752) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.640625 (0.016877) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.640625 (0.011500) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.691406 (0.019918) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.626302 (0.034401) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.627604 (0.014382) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.658854 (0.032264) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.496094 (0.017758) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.656250 (0.044309) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.669271 (0.007366) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.537760 (0.084766) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.588542 (0.009207) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.679688 (0.014616) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.488281 (0.101262) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.566406 (0.044993) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.625000 (0.024080) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24687f9",
   "metadata": {},
   "source": [
    "We can see that the batch size of 20 and 100 epochs achieved the best result of about 70% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499279f",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>How to Tune the Training Optimization Algorithm</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>Keras offers a suite of different state-of-the-art optimization algorithms.</b> In this example, we tune the optimization algorithm used to train the network, each with default parameters.</font>\n",
    "\n",
    "This is an odd example, because often you will choose one approach a priori and instead focus on tuning its parameters on your problem (<font color='brown'>e.g. see the next example</font>). Here we will evaluate the [suite of optimization algorithms supported by the Keras API](http://keras.io/optimizers/).\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4450aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # return model without compile\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5edb8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(\n",
    "    model=create_model, loss=\"binary_crossentropy\", epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bcbe950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 7.54 s, total: 17.9 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d9d5d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.709635 using {'optimizer': 'Adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c09a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.649740 (0.003683) with: {'optimizer': 'SGD'}\n",
      "0.671875 (0.017758) with: {'optimizer': 'RMSprop'}\n",
      "0.567708 (0.025582) with: {'optimizer': 'Adagrad'}\n",
      "0.436198 (0.094847) with: {'optimizer': 'Adadelta'}\n",
      "0.709635 (0.020256) with: {'optimizer': 'Adam'}\n",
      "0.658854 (0.018688) with: {'optimizer': 'Adamax'}\n",
      "0.695312 (0.006379) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234ef7d",
   "metadata": {},
   "source": [
    "Note in the function <font color='blue'>create_model()</font> defined above do not return a compiled model like that one in the previous example. This is because <b>setting an optimizer for a Keras model is done in the <font color='blue'>compile()</font> function call</b>, hence it is better to leave it to the <b><font color='blue'>KerasClassifier</font></b> wrapper and the [**GridSearchCV model**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Also note that we specified <font color='blue'>loss=\"binary_crossentropy\"</font> in the wrapper as it should also be set during the <font color='blue'>compile()</font> function call.\n",
    "\n",
    "The <b><font color='blue'>KerasClassifier</font></b> wrapper will not compile your model again if the model is already compiled. Hence the other way to run [**GridSearchCV model**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is to set the `optimizer` as an argument to the <font color='blue'>create_model()</font> function which returns an appropriately compiled model, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9c1a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af5cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(model__optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ffe6264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 8.8 s, total: 20.6 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab77f72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.700521 using {'model__optimizer': 'Adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ba49c",
   "metadata": {},
   "source": [
    "The results suggest that the ADAM optimization algorithm is the best with a score of about 70% accuracy.\n",
    "\n",
    "Note that in the above, we have the prefix `model__` in the parameter dictionary `param_grid`. This is required for the <font color='blue'><b>KerasClassifier</b></font> in [**SciKeras**](https://pypi.org/project/scikeras/) module to make clear that the parameter need to route into the <font color='blue'>create_model()</font> function as arguments, rather than some parameter to set up in <font color='blue'>compile()</font> or <font color='blue'>fit()</font>. See also the [routed parameter section](https://www.adriangb.com/scikeras/stable/advanced.html#routed-parameters) of SciKeras documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07b661",
   "metadata": {},
   "source": [
    "<a id='sect5'></a>\n",
    "## <font color='darkblue'>How to Tune Learning Rate and Momentum</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>It is common to pre-select an optimization algorithm to train your network and tune its parameters.</b></font>\n",
    "\n",
    "By far the most common optimization algorithm is plain old [**Stochastic Gradient Descent**](http://keras.io/optimizers/#sgd) (SGD) because it is so well understood. In this example, we will look at optimizing the SGD learning rate and momentum parameters.\n",
    "\n",
    "<b><font color='darkblue'>Learning rate</font> controls how much to update the weight at the end of each batch and the <font color='darkblue'>momentum</font> controls how much to let the previous update influence the current weight update.</b>\n",
    "\n",
    "We will try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice). In Keras, the way to set the learning rate and momentum is [the following](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD):\n",
    "```python\n",
    "...\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.2)\n",
    "```\n",
    "<br/>\n",
    "\n",
    "In [**SciKeras**](https://pypi.org/project/scikeras/) wrapper, we will route the parameters to the optimizer with the prefix `optimizer__`.\n",
    "\n",
    "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (<font color='brown'>learning rate</font>), the number of updates per epoch (<font color='brown'>batch size</font>) and the number of epochs.\n",
    "\n",
    "The full code listing is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67712bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93ce5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(\n",
    "    model=create_model, loss=\"binary_crossentropy\", optimizer=\"SGD\",\n",
    "    epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(\n",
    "    optimizer__learning_rate=learn_rate,\n",
    "    optimizer__momentum=momentum)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d58b468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 13.7 s, total: 31.5 s\n",
      "Wall time: 5min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca75743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.677083 using {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a27ef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675781 (0.019918) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.677083 (0.040637) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.2}\n",
      "0.673177 (0.025780) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.4}\n",
      "0.675781 (0.022097) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.6}\n",
      "0.670573 (0.006639) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
      "0.673177 (0.027126) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.9}\n",
      "0.652344 (0.019137) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.615885 (0.076966) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.2}\n",
      "0.651042 (0.003683) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.4}\n",
      "0.649740 (0.003683) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.6}\n",
      "0.652344 (0.003189) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.8}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.9}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.0}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.2}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.4}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.6}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.8}\n",
      "0.652344 (0.003189) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.9}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.0}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.2}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.4}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.6}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.8}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.9}\n",
      "0.652344 (0.000000) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.0}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.2}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.4}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.6}\n",
      "0.651042 (0.001841) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.8}\n",
      "0.549479 (0.142719) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6cade5",
   "metadata": {},
   "source": [
    "We can see that relatively SGD is not very good on this problem, nevertheless best results were achieved using a learning rate of 0.001 and a momentum of 0.2 with an accuracy of about 68%."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b209ac5d",
   "metadata": {},
   "source": [
    "<a id='sect6'></a>\n",
    "## <font color='darkblue'>How to Tune Network Weight Initialization</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>Neural network weight initialization used to be simple: use small random values.</b></font>\n",
    "\n",
    "Now there is a suite of different techniques to choose from. [Keras provides a laundry list](https://keras.io/api/layers/initializers/).\n",
    "\n",
    "<b>In this example, we will look at tuning the selection of network weight initialization by evaluating all of the available techniques.</b>\n",
    "\n",
    "<b>We will use the same weight initialization method on each layer. Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer.</b> In the example below we use rectifier for the hidden layer. We use sigmoid for the output layer because the predictions are binary. The weight initialization is now an argument to <font color='blue'>create_model()</font> function, which we need to use the `model__` prefix to ask <font color='blue'><b>KerasClassifier</b></font> to route the parameter to the model creation function.\n",
    "\n",
    "The full code listing is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07a01702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(init_mode='uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11950ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "init_mode = [\n",
    "    'uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal',\n",
    "    'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(model__init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a343c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.52 s, sys: 7.36 s, total: 16.9 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9aa758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.727865 using {'model__init_mode': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37cae865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727865 (0.018136) with: {'model__init_mode': 'uniform'}\n",
      "0.683594 (0.035084) with: {'model__init_mode': 'lecun_uniform'}\n",
      "0.726562 (0.020915) with: {'model__init_mode': 'normal'}\n",
      "0.651042 (0.001841) with: {'model__init_mode': 'zero'}\n",
      "0.707031 (0.008438) with: {'model__init_mode': 'glorot_normal'}\n",
      "0.688802 (0.024774) with: {'model__init_mode': 'glorot_uniform'}\n",
      "0.687500 (0.019401) with: {'model__init_mode': 'he_normal'}\n",
      "0.703125 (0.006379) with: {'model__init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af292dda",
   "metadata": {},
   "source": [
    "We can see that the best results were achieved with a uniform weight initialization scheme achieving a performance of about 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d55f1",
   "metadata": {},
   "source": [
    "<a id='sect7'></a>\n",
    "## <font color='darkblue'>How to Tune the Neuron Activation Function</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>The activation function controls the non-linearity of individual neurons and when to fire.</b> Generally, the [rectifier activation function](https://keras.io/api/layers/activations/#relu-function) is the most popular, but it used to be the sigmoid and the tanh functions and these functions may still be more suitable for different problems.</font>\n",
    "\n",
    "<b>In this example, we will evaluate the [suite of different activation functions available in Keras](https://keras.io/api/layers/activations/)</b>. We will only use these functions in the hidden layer, as we require a sigmoid activation function in the output for the binary classification problem. Similar to the previous example, this is an argument to the <font color='blue'>create_model()</font> function and we will use the `model__` prefix for the <font color='blue'><b>GridSearchCV</b></font> parameter grid.\n",
    "\n",
    "Generally, it is a good idea to prepare data to the range of the different transfer functions, which we will not do in this case.\n",
    "\n",
    "The full code listing is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59403324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68a742d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(model__activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ed31f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 9.29 s, total: 20.7 s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cf13671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.739583 using {'model__activation': 'softplus'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbb74033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656250 (0.005524) with: {'model__activation': 'softmax'}\n",
      "0.739583 (0.028764) with: {'model__activation': 'softplus'}\n",
      "0.680990 (0.008027) with: {'model__activation': 'softsign'}\n",
      "0.692708 (0.038051) with: {'model__activation': 'relu'}\n",
      "0.682292 (0.030314) with: {'model__activation': 'tanh'}\n",
      "0.683594 (0.028705) with: {'model__activation': 'sigmoid'}\n",
      "0.678385 (0.028940) with: {'model__activation': 'hard_sigmoid'}\n",
      "0.705729 (0.007366) with: {'model__activation': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/root/Github/ml_courses/env/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53178b",
   "metadata": {},
   "source": [
    "Surprisingly (to me at least), the ‘softplus’ activation function achieved the best results with an accuracy of about 73%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a98ee7",
   "metadata": {},
   "source": [
    "<a id='sect8'></a>\n",
    "## <font color='darkblue'>How to Tune Dropout Regularization</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>In this example, we will look at tuning the [dropout rate for regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) in an effort to limit overfitting and improve the model’s ability to generalize.</b></font>\n",
    "\n",
    "To get good results, dropout is best combined with a weight constraint such as the max norm constraint. For more on using dropout in deep learning models with Keras see the post:\n",
    "* [Dropout Regularization in Deep Learning Models With Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)\n",
    "\n",
    "This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between 0.0 and 0.9 (<font color='darkbrown'>1.0 does not make sense</font>) and maxnorm weight constraint values between 0 and 5.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50968835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate, weight_constraint):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,),\n",
    "                    kernel_initializer='uniform',\n",
    "                    activation='linear',\n",
    "                    kernel_constraint=MaxNorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38cb4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(\n",
    "    model__dropout_rate=dropout_rate,\n",
    "    model__weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1dd26b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 7.24 s, total: 18.3 s\n",
      "Wall time: 10min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6e5a49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.725260 using {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "903e3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723958 (0.013279) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
      "0.725260 (0.012075) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
      "0.713542 (0.012075) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
      "0.695312 (0.005524) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
      "0.717448 (0.009207) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
      "0.703125 (0.006379) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
      "0.705729 (0.014731) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
      "0.694010 (0.013279) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
      "0.716146 (0.009744) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
      "0.722656 (0.019137) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
      "0.709635 (0.014382) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
      "0.716146 (0.012890) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
      "0.709635 (0.012075) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
      "0.707031 (0.019918) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
      "0.694010 (0.025582) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
      "0.720052 (0.016367) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
      "0.708333 (0.017566) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
      "0.701823 (0.012075) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
      "0.700521 (0.007366) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
      "0.725260 (0.012075) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
      "0.710938 (0.014616) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
      "0.722656 (0.011500) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
      "0.700521 (0.003683) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
      "0.696615 (0.004872) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
      "0.705729 (0.010253) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
      "0.695312 (0.008438) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
      "0.708333 (0.001841) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
      "0.705729 (0.010253) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
      "0.700521 (0.004872) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
      "0.710938 (0.013902) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
      "0.687500 (0.011500) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
      "0.687500 (0.003189) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
      "0.694010 (0.004872) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
      "0.700521 (0.006639) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
      "0.695312 (0.005524) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
      "0.703125 (0.011500) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
      "0.699219 (0.011500) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
      "0.691406 (0.008438) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
      "0.692708 (0.020256) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
      "0.690104 (0.009744) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
      "0.695312 (0.013902) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
      "0.691406 (0.015947) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
      "0.699219 (0.009568) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
      "0.683594 (0.008438) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
      "0.695312 (0.016877) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
      "0.674479 (0.015733) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
      "0.656250 (0.012758) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
      "0.673177 (0.009744) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
      "0.666667 (0.015073) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
      "0.664062 (0.012758) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f6adb",
   "metadata": {},
   "source": [
    "We can see that the dropout rate of 0% and the MaxNorm weight constraint of 2 resulted in the best accuracy of about 72%. You may notice some of the result is nan. Probably it is due to the issue that the input is not normalized and you may run into a degenerated model by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83490294",
   "metadata": {},
   "source": [
    "<a id='sect9'></a>\n",
    "## <font color='darkblue'>How to Tune the Number of Neurons in the Hidden Layer</font> ([back](#sect0))\n",
    "<font size='3ptx'><b>The number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity of the network, at least at that point in the topology.</b></font>\n",
    "\n",
    "Also, generally, a large enough single layer network can approximate any other neural network, [at least in theory](https://en.wikipedia.org/wiki/Universal_approximation_theorem).\n",
    "\n",
    "<b>In this example, we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5</b>\n",
    "\n",
    "A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons.\n",
    "\n",
    "The full code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88ab096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(\n",
    "            neurons, input_shape=(8,), kernel_initializer='uniform',\n",
    "            activation='softplus', kernel_constraint=MaxNorm(2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6286699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = range(15, 40, 5)\n",
    "param_grid = dict(model__neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2153470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 7.79 s, total: 18.4 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "874c2c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.753906 using {'model__neurons': 35}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7086cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729167 (0.016367) with: {'model__neurons': 15}\n",
      "0.722656 (0.027251) with: {'model__neurons': 20}\n",
      "0.736979 (0.029635) with: {'model__neurons': 25}\n",
      "0.740885 (0.032264) with: {'model__neurons': 30}\n",
      "0.753906 (0.033754) with: {'model__neurons': 35}\n"
     ]
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dbf29",
   "metadata": {},
   "source": [
    "We can see that the best results were achieved with a network with 35 neurons in the hidden layer with an accuracy of about 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80866fa2",
   "metadata": {},
   "source": [
    "<a id='sect10'></a>\n",
    "## <font color='darkblue'>Tips for Hyperparameter Optimization</font> ([back](#sect0))\n",
    "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
    "* <font size='3ptx'>**k-fold Cross Validation**</font>. You can see that the results from the examples in this post show some variance. A default cross-validation of 3 was used, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
    "* <font size='3ptx'>**Review the Whole Grid**</font>. Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
    "* <font size='3ptx'>**Parallelize**</font>. Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider spinning up a lot of [**AWS instances**](https://machinelearningmastery.com/develop-evaluate-large-deep-learning-models-keras-amazon-web-services/).\n",
    "* <font size='3ptx'>**Use a Sample of Your Dataset**</font>. Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
    "* <font size='3ptx'>**Start with Coarse Grids**</font>. Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
    "* <font size='3ptx'>**Do not Transfer Results**</font>. Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
    "* <font size='3ptx'>**Reproducibility is a Problem**</font>. Although we set the seed for the random number generator in NumPy, the results are not 100% reproducible. There is more to reproducibility when grid searching wrapped Keras models than is presented in this post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ce45e",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Supplement</font>\n",
    "* [Stackoverflow - Disable Tensorflow debugging information](https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information)\n",
    "* [Stackoverflow - Why can't I suppress numpy warnings](https://stackoverflow.com/questions/29347987/why-cant-i-suppress-numpy-warnings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
