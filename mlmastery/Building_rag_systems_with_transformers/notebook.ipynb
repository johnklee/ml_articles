{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1313388c-7358-4c80-8fb1-edbde7e178de",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "<font size='3ptx'><b>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the capabilities of large language models</b>. By combining the strengths of retrieval systems with generative models, RAG systems can produce more accurate, factual, and contextually relevant responses. This approach is particularly valuable when dealing with domain-specific knowledge or when up-to-date information is required.</font>\n",
    "\n",
    "<b>In this post, you will explore how to build a basic RAG system using models from the Hugging Face library</b>. You’ll build each system component, from document indexing to retrieval and generation, and implement a complete end-to-end solution. Specifically, you will learn:\n",
    "* The RAG architecture and its components\n",
    "* How to build a document indexing and retrieval system\n",
    "* How to implement a transformer-based generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27859dab-4477-4691-84ff-949d4607adbe",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Overview</font></b>\n",
    "This post is divided into five parts:\n",
    "* Understanding the RAG architecture\n",
    "* Building the Document Indexing System\n",
    "* Implementing the Retrieval System\n",
    "* Implementing the Generator\n",
    "* Building the Complete RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6696b-eb00-4a0a-9169-14189ef1b1d1",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Build RAG</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78421cf4-72f7-4c3a-bc15-6153d74a6c74",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Understanding the RAG Architecture</font></b>\n",
    "<b><font size='3ptx'>An RAG system consists of two main components:</font></b>\n",
    "1. <b>Retriever</b>: Responsible for finding relevant documents or passages from a knowledge base given a query.\n",
    "2. <b>Generator</b>: Uses the retrieved documents and the original query to generate a coherent and informative response.\n",
    "\n",
    "Each of these components has many fine details. <b>You need RAG because the generator alone</b> (i.e., the language model) <b>cannot generate accurate and contextually relevant responses, which are known as hallucinations</b>. Therefore, you need the retriever to provide hints to help the generator.\n",
    "\n",
    "This approach combines generative models’ broad language understanding capabilities with the ability to access specific information from a knowledge base. This results in responses that are both fluent and factually accurate.\n",
    "\n",
    "<b>Let’s implement each component of a RAG system step by step.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce2361-2bc5-4301-82c9-3d025af0e56d",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Building the Document Indexing System</font></b>\n",
    "<font size='3ptx'><b>The first step in creating a RAG system is to build a document indexing system. This system must encode documents into dense vector representations and store them in a database.</b> Then, we can retrieve the documents based on contextual similarity. This means you need to be able to search by vector similarity metrics, not exact matches. This is a key point – not all database systems can be used to build a document indexing system.</font>\n",
    "\n",
    "<b>Of course, you could collect documents, encode them into vector representations, and keep them in memory. When retrieval is requested, you could compute the similarity one by one to find the closest match</b>. However, checking each vector in a loop is inefficient and not scalable.\n",
    "\n",
    "**[FAISS](https://pypi.org/project/faiss-cpu/) is a library that is optimized for this task**. To install FAISS, you can compile it from source or use the pre-compiled version from PyPI:\n",
    "```python\n",
    "$ pip install faiss-cpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72220381-39e1-4820-9786-1f40d0a59b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu==1.10.0\n",
      "torch==2.7.0\n",
      "torchvision==0.22.0\n",
      "transformers==4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -P '(faiss|torch|transformers)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5e1d7-60b4-49aa-b95d-3ee2236f31fb",
   "metadata": {},
   "source": [
    "In the following, you’ll create a language model to encode documents into dense vector representations and store them in a FAISS index for efficient retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99931d25-2f56-4d72-8654-c7ce0c0cb9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with 10 documents\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def generate_embedding(docs, model, tokenizer):\n",
    "    # Tokenize each text and convert to PyTorch tensors\n",
    "    inputs = tokenizer(docs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Embedding defined as mean pooling of all tokens\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    expanded_mask = attention_mask.unsqueeze(-1).expand(embeddings.shape).float()\n",
    "    sum_embeddings = torch.sum(embeddings * expanded_mask, axis=1)\n",
    "    sum_mask = torch.clamp(expanded_mask.sum(axis=1), min=1e-9)\n",
    "    mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    # Convert to numpy array\n",
    "    return mean_embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "# Sample document collection\n",
    "documents = [\n",
    "    \"Transformers are a type of deep learning model introduced in the paper 'Attention \"\n",
    "        \"Is All You Need'.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is a \"\n",
    "        \"transformer-based model designed to understand the context of a word based on \"\n",
    "        \"its surroundings.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is a transformer-based model designed for \"\n",
    "        \"natural language generation tasks.\",\n",
    "    \"T5 (Text-to-Text Transfer Transformer) treats every NLP problem as a text-to-text \"\n",
    "        \"problem, where both the input and output are text strings.\",\n",
    "    \"RoBERTa is an optimized version of BERT with improved training methodology and more \"\n",
    "        \"training data.\",\n",
    "    \"DistilBERT is a smaller, faster version of BERT that retains 97% of its language \"\n",
    "        \"understanding capabilities.\",\n",
    "    \"ALBERT reduces the parameters of BERT by sharing parameters across layers and using \"\n",
    "        \"embedding factorization.\",\n",
    "    \"XLNet is a generalized autoregressive pretraining method that overcomes the \"\n",
    "        \"limitations of BERT by using permutation language modeling.\",\n",
    "    \"ELECTRA uses a generator-discriminator architecture for more efficient pretraining.\",\n",
    "    \"DeBERTa enhances BERT with disentangled attention and an enhanced mask decoder.\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all documents, then create FAISS index for efficient similarity search\n",
    "document_embeddings = generate_embedding(documents, model, tokenizer)\n",
    "dimension = document_embeddings.shape[1]   # Dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dimension)       # Using L2 (Euclidean) distance\n",
    "index.add(document_embeddings)             # Add embeddings to the index\n",
    "print(f\"Created index with {index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa31d75e-e4ed-495b-8051-f958b68ce8ce",
   "metadata": {},
   "source": [
    "<b>The key part of this code is the <font color='blue'>generate_embedding()</font> function. It takes a list of documents, encodes them through the model, and returns a dense vector representation using mean pooling over all token embeddings from each document</b>. The document does not need to be long and complete. A sentence or paragraph is expected because the models have a context window limit.\n",
    "\n",
    "Moreover, <b>you will see later in another example that a very long document is not ideal for RAG.</b>\n",
    "\n",
    "You used a pre-trained Sentence Transformer model, `sentence-transformers/all-MiniLM-L6-v2`, which is specifically designed for generating sentence embeddings. You do not keep the original document in the FAISS index; you only keep the embedding vectors. You pre-build the L2 distance index among these vectors for efficient similarity search.\n",
    "\n",
    "You may modify this code for different implementations of the RAG system. For example, the dense vector representation is obtained by mean pooling. Still, you can just use the first token since the tokenizer prepends the `[CLS]` token to each sentence, and the model is supposed to produce the context embedding over this special token. Moreover, <b>[L2 distance](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances#metric_l2) is used here because you declared the FAISS index intending to use it with the L2 metric. There is no cosine similarity metric in FAISS, but L2 and cosine distance are similar. Note that, with normalized vectors</b>,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1d7e5-f4e6-4a5e-b1ab-1e4dbb85ccdd",
   "metadata": {},
   "source": [
    "<b>Therefore, L2 distance is equivalent to cosine distance when the vectors are normalized<b> (<font color='brown'>as long as you remember that when dissimilarity increases, L2 runs from 0 to infinity, but cosine distance decreases from +1 to -1</font>). If you intended to use cosine distance, you should modify the code to become:\n",
    "```python\n",
    "...\n",
    "document_embeddings = generate_embedding(documents, model, tokenizer)\n",
    "normalized = document_embeddings / np.linalg.norm(document_embeddings, axis=1, keepdims=True)\n",
    "index.add(normalized)\n",
    "```\n",
    "\n",
    "\n",
    "Essentially, you scaled each embedding vector to make it unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94208eeb-728f-41a7-a0ea-2665e6aea108",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Implementing the Retrieval System</font></b>\n",
    "With the documents indexed, let’s see how you can retrieve some of the most relevant documents for a given query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98d236fc-4503-4c0f-a86c-adb119aed04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is BERT?\n",
      "\n",
      "Document 1 (Distance: 23.7060):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "\n",
      "Document 2 (Distance: 28.0793):\n",
      "RoBERTa is an optimized version of BERT with improved training methodology and more training data.\n",
      "\n",
      "Document 3 (Distance: 29.5908):\n",
      "DistilBERT is a smaller, faster version of BERT that retains 97% of its language understanding capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, index, documents, k=3):\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = generate_embedding(query, model, tokenizer)   # 1xD matrix\n",
    "    # Search the index for similar documents\n",
    "    distances, indices = index.search(query_embedding, k)  # 1xk matrices\n",
    "    # Return the retrieved documents and their distances\n",
    "    retrieved_docs = [(documents[idx], float(distances[0][i])) for i, idx in enumerate(indices[0])]\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "# Example query\n",
    "query = \"What is BERT?\"\n",
    "retrieved_docs = retrieve_documents(query, index, documents)\n",
    "\n",
    "\n",
    "# Print the retrieved documents\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, (doc, distance) in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1} (Distance: {distance:.4f}):\")\n",
    "    print(doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17961f-5512-4634-b109-d76c8b89a500",
   "metadata": {},
   "source": [
    "In the function <font color='blue'>retrieve_documents()</font>, you provide the query string, the FAISS index, and the document collection. You then generate the embedding for the query just like you did for the documents. Then, you leverage the <font color='blue'>search()</font> method of the FAISS index to find the `k` most similar documents to the query embedding. The <font color='blue'>search()</font> method returns two arrays:\n",
    "* **`distances`**: The `distances` between the query embedding and the indexed embeddings. Since this is how you defined the index, these are the L2 distances.\n",
    "* **`indices`**: The `indices` of the indexed embeddings that are most similar to the query embedding, matching the distances array.\n",
    "\n",
    "<br/>\n",
    "\n",
    "You can use these arrays to retrieve the most similar documents from the original collection. Here, you use the indices to get the documents from the list. Afterward, you print the retrieved documents along with their distances from the query in the embedding space in descending order of relevance or increasing distance.\n",
    "\n",
    "<b>Note that the document’s context vector is supposed to represent the entire document. Therefore, the distance between the query and the document may be large if the document contains a lot of information.</b> Ideally, you want the documents to be focused and concise. If you have a long text, you may want to split it into multiple documents to make the RAG system more accurate.\n",
    "\n",
    "This retrieval system forms the first component of our RAG architecture. Given a user query, it allows us to find relevant information from our knowledge base. There are many other ways to implement the same functionality, but this highlights the key idea of vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc01f9-908b-45a6-aab5-66a4ccf23948",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Implementing the Generator</font></b>\n",
    "<b><font size='3ptx'>Next, let’s implement the generator component of our RAG system.</font></b>\n",
    "\n",
    "<b><font size='3ptx'>It is a prompt engineering problem</font></b>. While the user provides a query, you first retrieve the most relevant documents from the retriever and create a new prompt that includes the user’s query and the retrieved documents as context. Then, you use a pre-trained language model to generate a response based on the new prompt. Here is how you can implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb54dbb-f72c-4bcb-9bbb-e7d18c3a0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "def generate_response(query, retrieved_docs, max_length=150):\n",
    "    # Combine the query and retrieved documents into a single prompt\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "\n",
    "    # Generate a response\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = gen_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Generate a response for the example query\n",
    "response = generate_response(query, [doc for doc, score in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2cf48c-be8b-4238-a30d-9b44dd99ade0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is BERT?\n",
      "Generated Response: Bidirectional Encoder Representations from Transformers\n"
     ]
    }
   ],
   "source": [
    "print(f'Query: {query}')\n",
    "print(f\"Generated Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251a059-593b-4909-8350-a7e54322ebc3",
   "metadata": {},
   "source": [
    "<b>This is the generator component of our RAG system. You instantiate the pre-trained T5 model `t5-small`</b> (<font color='brown'>small version, but you can pick a larger one or a different model that fits to run on your system</font>). This model is a sequence-to-sequence model that generates a new sequence from a given sequence. If you use a different model, such as the “causal LM” model, you may need to change the prompt to make it work more efficiently.\n",
    "\n",
    "In the <font color='blue'>generate_response()</font> function, you combine the query and the retrieved documents into a single prompt. Then, you use the T5 model to generate a response. You can adjust the generation parameters to make it work better. In the above, only beam search is used for simplicity. The model’s output is then decoded to a text string as the response. <b>Since you combined multiple documents into a single prompt, you need to be careful that the prompt does not exceed the context window of the model</b>.\n",
    "\n",
    "The generator leverages the information from the retrieved documents to produce a fluent and factually accurate response. The model behaves vastly differently when you just pose the query without context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f202982-f652-4114-b623-99f7ff34ae69",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Building the Complete RAG System</font></b>\n",
    "<b><font size='3ptx'>That’s all you need to build a basic RAG system. Let’s create a function to wrap up the retrieval and generation components</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6eb581-88b2-4fa2-b10a-e6885af36003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query, documents, retriever_k=3, max_length=150):\n",
    "    retrieved_docs_info = retrieve_documents(query, index, documents, k=retriever_k)\n",
    "    retrieved_docs = [info[0] for info in retrieved_docs_info]\n",
    "    response = generate_response(query, retrieved_docs, max_length=max_length)\n",
    "    return response, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd737e-ff08-4d23-8c8f-543b5adb59d5",
   "metadata": {},
   "source": [
    "Then you can use the RAG pipeline in a loop to generate responses for a set of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866eed6b-f86a-4c2e-b18c-699a16aca005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is BERT?\n",
      "\n",
      "Retrieved Documents:\n",
      "Document 1 (Distance: 29.5908):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "Document 2 (Distance: 29.5908):\n",
      "RoBERTa is an optimized version of BERT with improved training methodology and more training data.\n",
      "Document 3 (Distance: 29.5908):\n",
      "DistilBERT is a smaller, faster version of BERT that retains 97% of its language understanding capabilities.\n",
      "\n",
      "Generated Response:\n",
      "Bidirectional Encoder Representations from Transformers\n",
      "--------------------\n",
      "Query: How does GPT work?\n",
      "\n",
      "Retrieved Documents:\n",
      "Document 1 (Distance: 29.5908):\n",
      "GPT (Generative Pre-trained Transformer) is a transformer-based model designed for natural language generation tasks.\n",
      "Document 2 (Distance: 29.5908):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "Document 3 (Distance: 29.5908):\n",
      "Transformers are a type of deep learning model introduced in the paper 'Attention Is All You Need'.\n",
      "\n",
      "Generated Response:\n",
      "natural language generation tasks\n",
      "--------------------\n",
      "Query: What is the difference between BERT and GPT?\n",
      "\n",
      "Retrieved Documents:\n",
      "Document 1 (Distance: 29.5908):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "Document 2 (Distance: 29.5908):\n",
      "GPT (Generative Pre-trained Transformer) is a transformer-based model designed for natural language generation tasks.\n",
      "Document 3 (Distance: 29.5908):\n",
      "DistilBERT is a smaller, faster version of BERT that retains 97% of its language understanding capabilities.\n",
      "\n",
      "Generated Response:\n",
      "can understand the context of a word based on its surroundings\n",
      "--------------------\n",
      "Query: What is a smaller version of BERT?\n",
      "\n",
      "Retrieved Documents:\n",
      "Document 1 (Distance: 29.5908):\n",
      "DistilBERT is a smaller, faster version of BERT that retains 97% of its language understanding capabilities.\n",
      "Document 2 (Distance: 29.5908):\n",
      "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed to understand the context of a word based on its surroundings.\n",
      "Document 3 (Distance: 29.5908):\n",
      "RoBERTa is an optimized version of BERT with improved training methodology and more training data.\n",
      "\n",
      "Generated Response:\n",
      "DistilBERT\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Example queries\n",
    "queries = [\n",
    "    \"What is BERT?\",\n",
    "    \"How does GPT work?\",\n",
    "    \"What is the difference between BERT and GPT?\",\n",
    "    \"What is a smaller version of BERT?\"\n",
    "]\n",
    "# Run the RAG pipeline for each query\n",
    "for query in queries:\n",
    "    response, retrieved_docs = rag_pipeline(query, documents)\n",
    "    print(f\"Query: {query}\")\n",
    "    print()\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"Document {i+1} (Distance: {distance:.4f}):\")\n",
    "        print(doc)\n",
    "    print()\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bafcb-688f-4594-acea-acf1a3e15479",
   "metadata": {},
   "source": [
    "You can see that the queries are answered one by one in a loop. The set of documents, however, is prepared in advance and reused for all queries. This is how an RAG system typically works.\n",
    "\n",
    "This code is self-contained. All the documents and queries are defined in the code. This is a starting point, and you may extend it for new features, such as saving the indexed documents in a file that you can load later without re-indexing every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae25515-ffcc-433d-811c-2d82062c79c2",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Further Readings</font></b>\n",
    "Below are some further readings that you may find useful:\n",
    "* [The FAISS library documentation](https://github.com/facebookresearch/faiss/wiki)\n",
    "* [The FAISS library paper](https://arxiv.org/abs/2401.08281)\n",
    "* [Billion-scale similarity search with GPUs](https://arxiv.org/abs/1702.08734)_\n",
    "* [What is RAG (Retrieval-Agumented Generation)?](https://aws.amazon.com/what-is/retrieval-augmented-generation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd77ff-5550-4ff5-9e0a-e1203bef873d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Summary</font></b>\n",
    "This post explored building a Retrieval-Augmented Generation (RAG) system using transformer models from the Hugging Face library. We’ve implemented each system component, from document indexing to retrieval and generation, and combined them into a complete end-to-end solution.\n",
    "\n",
    "RAG systems represent a powerful approach to enhancing the capabilities of language models by grounding them in external knowledge. RAG systems can produce more accurate, factual, and contextually relevant responses by retrieving relevant information and incorporating it into the generation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
