{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fca62db",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([course link](https://machinelearningmastery.com/using-cnn-for-financial-time-series-prediction/)) <b><font size='3ptx'>Convolutional neural networks have their roots in image processing. It was first published in LeNet to recognize the MNIST handwritten digits. However, convolutional neural networks are not limited to handling images.</font></b>\n",
    "\n",
    "<b>In this tutorial, we are going to look at an example of using CNN for time series prediction with an application from financial markets.</b> By way of this example, we are going to explore some techniques in using Keras for model training as well.\n",
    "\n",
    "After completing this tutorial, you will know\n",
    "* What a typical multidimensional financial data series looks like?\n",
    "* How can CNN applied to time series in a classification problem\n",
    "* How to use generators to feed data to train a Keras model\n",
    "* How to provide a custom metric for evaluating a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bdb46c",
   "metadata": {},
   "source": [
    "<a id='sect_0'></a>\n",
    "### <font color='darkgreen'>Tutorial overview</font>\n",
    "This tutorial is divided into 7 parts; they are:\n",
    "* [**Background of the idea**](#sect_1)\n",
    "* [**Preprocessing of data**](#sect_2)\n",
    "* [**Data generator**](#sect_3)\n",
    "* [**The model**](#sect_4)\n",
    "* Training, validation, and test\n",
    "* Extensions\n",
    "* Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99becab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DataType(enum.Enum):\n",
    "  TRAIN = 0\n",
    "  VALID = 1\n",
    "\n",
    "  \n",
    "DATADIR = '../../datas/CNNpred_data'\n",
    "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
    "TRAIN_VALID_RATIO = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9949b",
   "metadata": {},
   "source": [
    "<a id='sect_1'></a>\n",
    "## <font color='darkblue'>Background of the idea</font>\n",
    "In this tutorial we are following the paper titled “CNNpred: CNN-based stock market prediction using a iverse set of variables” by Ehsan Hoseinzade and Saman Haratizadeh. The data file and sample code from the author are available in [github](https://github.com/hoseinzadeehsan/CNNpred-Keras).\n",
    "\n",
    "<b>The goal of the paper is simple: To predict the next day’s direction of the stock market</b> (<font color='brown'>i.e., up or down compared to today</font>), hence it is a binary classification problem. However, it is interesting to see how this problem are formulated and solved.\n",
    "\n",
    "We have seen the examples on using CNN for sequence prediction. If we consider [Dow Jones Industrial Average](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) (<font color='brown'>DJIA</font>) as an example, we may build a CNN with 1D convolution for prediction. This makes sense because a 1D convolution on a time series is roughly computing its moving average or using digital signal processing terms, applying a filter to the time series. It should provide some clues about the trend.\n",
    "\n",
    "However, <b>when we look at financial time series, it is quite a common sense that some derived signals are useful for predictions too. For example, price and volume together can provide a better clue. Also some other technical indicators such as the moving average of different window size are useful too</b>. If we put all these align together, we will have a table of data, which each time instance has multiple features, and the goal is still to predict the direction of one time series.\n",
    "\n",
    "In the CNNpred paper, 82 such features are prepared for the DJIA time series:\n",
    "![features](images/1.PNG)\n",
    "\n",
    "<b>Unlike LSTM, which there is an explicit concept of time steps applied, we present data as a matrix in CNN models</b>. As shown in the table below, the features across multiple time steps are presented as a 2D array.\n",
    "![features](images/2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6a4e4",
   "metadata": {},
   "source": [
    "<a id='sect_2'></a>\n",
    "## <font color='darkblue'>Preprocessing of data</font>\n",
    "<b><font size='3ptx'>In the following, we try to implement the idea of the CNNpred from scratch using Tensorflow’s keras API. While there is a reference implementation from the author in the github link above, we reimplement it differently to illustrate some Keras techniques.</font></b>\n",
    "\n",
    "Firstly the data are five CSV files, each for a different market index, under the Dataset directory from github repository above, or we can also get a copy here ([CNNpred-data.zip](https://machinelearningmastery.com/?attachment_id=13057)). The input data has a date column and a name column to identify the ticker symbol for the market index. We can leave the date column as time index and remove the name column. The rest are all numerical.\n",
    "\n",
    "For five data file in the directory, we read each of them as a separate pandas DataFrame and keep them in a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb446566",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "for filename in os.listdir(DATADIR):\n",
    "    if not filename.lower().endswith(\".csv\"):\n",
    "        continue # read only the CSV files\n",
    "    filepath = os.path.join(DATADIR, filename)\n",
    "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
    "    # basic preprocessing: get the name, the classification\n",
    "    # Save the target variable as a column in dataframe for easier dropna()\n",
    "    name = X[\"Name\"][0]\n",
    "    del X[\"Name\"]\n",
    "    cols = X.columns\n",
    "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    X.dropna(inplace=True)\n",
    "    # Fit the standard scaler using the training dataset\n",
    "    index = X.index[X.index > TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
    "    # Save scale transformed dataframe\n",
    "    X[cols] = scaler.transform(X[cols])\n",
    "    data[name] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2f730",
   "metadata": {},
   "source": [
    "The result of the above code is a DataFrame for each index, which the classification label is the column “Target” while all other columns are input features. We also normalize the data with a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e0e5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RUT', 'DJI', 'S&P', 'NYA', 'NASDAQ'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8180b53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-10-19    1\n",
       "2010-10-20    0\n",
       "2010-10-21    1\n",
       "2010-10-26    0\n",
       "2010-10-27    0\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['RUT']['Target'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf091636",
   "metadata": {},
   "source": [
    "As we are going to predict the market direction, we first try to create the classification label. The market direction is defined as the closing index of tomorrow compared to today. If we have read the data into a pandas DataFrame, we can use `X[\"Close\"].pct_change()` to find the percentage change, which a positive change for the market goes up. So we can shift this to one time step back as our label. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfee83c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-11-08   -8.335279\n",
       "2010-11-09   -8.386844\n",
       "2010-11-10   -8.339116\n",
       "2010-11-11   -8.409379\n",
       "2010-11-12   -8.522084\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"Close\"][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e5ede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-11-08   -0.000388\n",
       "2010-11-09    0.006186\n",
       "2010-11-10   -0.005691\n",
       "2010-11-11    0.008426\n",
       "2010-11-12    0.013402\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"Close\"].pct_change()[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d15f6b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-11-08    1\n",
       "2010-11-09    0\n",
       "2010-11-10    1\n",
       "2010-11-11    1\n",
       "2010-11-12    1\n",
       "Name: Close, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X[\"Close\"].pct_change().shift(-1) > 0).astype(int)[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872d9cb",
   "metadata": {},
   "source": [
    "<b>In time series problems, it is generally reasonable not to split the data into training and test sets randomly, but to set up a cutoff point in which the data before the cutoff is training set while that afterwards is the test set</b>. The scaling above are based on the training set but applied to the entire dataset:\n",
    "```python\n",
    "    index = X.index[X.index > TRAIN_TEST_CUTOFF]\n",
    "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa278456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1114 points as training data\n"
     ]
    }
   ],
   "source": [
    "print(f'We have {X.shape[0]} points as training data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6c5b7",
   "metadata": {},
   "source": [
    "<a id='sect_3'></a>\n",
    "## <font color='darkblue'>Data generator</font> ([back](#sect_0))\n",
    "<b><font size='3ptx'>We are not going to use all time steps at once, but instead, we use a fixed length of `N` time steps to predict the market direction at step `N+1`. In this design, the window of N time steps can start from anywhere</font></b>\n",
    "\n",
    "We can just create a large number of DataFrames with large amount of overlaps with one another. <b>To save memory, we are going to build a data generator for training and validation, as follows</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d16afb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(data, seq_len, batch_size, targetcol, kind: DataType):\n",
    "    \"\"\"As a generator to produce samples for Keras model.\n",
    "    \n",
    "    Args:\n",
    "      data: Raw data to produce fixed length of data from.\n",
    "      seq_len: The desired sequence length.\n",
    "      batch_size: Parameter used in learning.\n",
    "      targetcol: The target column to make prediction at.\n",
    "      kind: ('train'|'valid')\n",
    "      \n",
    "    Returns:\n",
    "      Fixed length of sequence dataset.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    while True:\n",
    "        # Pick one dataframe from the pool\n",
    "        key = random.choice(list(data.keys()))\n",
    "        df = data[key]\n",
    "        input_cols = [c for c in df.columns if c != targetcol]\n",
    "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
    "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
    "        if kind == DataType.TRAIN:\n",
    "            index = index[:split]   # range for the training set\n",
    "        elif kind == DataType.VALID:\n",
    "            index = index[split:]   # range for the validation set\n",
    "            \n",
    "        # Pick one position, then clip a sequence length\n",
    "        while True:\n",
    "            t = random.choice(index)      # pick one time step\n",
    "            n = (df.index == t).argmax()  # find its position in the dataframe\n",
    "            if n-seq_len+1 < 0:\n",
    "                continue # can't get enough data for one sequence length\n",
    "            frame = df.iloc[n-seq_len+1:n+1]\n",
    "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
    "            break\n",
    "            \n",
    "        # if we get enough for a batch, dispatch\n",
    "        if len(batch) == batch_size:\n",
    "            X, y = zip(*batch)\n",
    "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
    "            yield X, y\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe268f",
   "metadata": {},
   "source": [
    "[**Generator**](https://docs.python.org/3/c-api/gen.html) is a special function in Python that does not return a value but to yield in iterations, such that a sequence of data are produced from it. **For a generator to be used in Keras training, it is expected to yield a batch of input data and target. This generator supposed to run indefinitely. Hence the generator function above is created with an infinite loop starts with `while True`**.\n",
    "\n",
    "In each iteration, it randomly pick one DataFrame from the Python dictionary, then within the range of time steps of the training set (<font color='brown'>i.e., the beginning portion</font>), we start from a random point and take `N` time steps using the pandas `iloc[start:end]` syntax to create a input under the variable frame. This DataFrame will be a 2D array. The target label is that of the last time step. The input data and the label are then appended to the list batch. Until we accumulated for one batch’s size, we dispatch it from the generator.\n",
    "\n",
    "The last four lines at the code snippet above is to dispatch a batch for training or validation. We collect the list of input data (<font color='brown'>each a 2D array</font>) as well as a list of target label into variables `X` and `y`, then convert them into numpy array so it can work with our Keras model. We need to add one more dimension to the numpy array `X` using <font color='blue'>np.expand_dims()</font> because of the design of the network model, as explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d3f53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([[[1, 2, 3], 0], [[4, 5, 6], 1], [[7, 8, 9], 2]], dtype=object)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4489d78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[list([1, 2, 3]), 0],\n",
       "       [list([4, 5, 6]), 1],\n",
       "       [list([7, 8, 9]), 2]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "750e9bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[list([1, 2, 3])],\n",
       "        [0]],\n",
       "\n",
       "       [[list([4, 5, 6])],\n",
       "        [1]],\n",
       "\n",
       "       [[list([7, 8, 9])],\n",
       "        [2]]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data = np.expand_dims(test_data, 2)\n",
    "new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b45193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b44ce466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[list([1, 2, 3]), 0],\n",
       "        [list([4, 5, 6]), 1],\n",
       "        [list([7, 8, 9]), 2]]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data = np.expand_dims(test_data, 0)\n",
    "new_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5daddbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c82a7",
   "metadata": {},
   "source": [
    "<a id='sect_4'></a>\n",
    "## <font color='darkblue'>The Model</font> ([back](#sect_0))\n",
    "<font size='3ptx'><b>The 2D CNN model presented in the original paper accepts an input tensor of shape \n",
    "$N*m*1$ for `N` the number of time steps and `m` the number of features in each time step. The paper assumes \n",
    "$N=60$ and $m=82$.</b></font>\n",
    "\n",
    "The model comprises of three convolutional layers, as described as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51738b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e5bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), drop_rate=0.1):\n",
    "    \"\"\"2D-CNNpred model according to the paper\n",
    "    \n",
    "    Args:\n",
    "      seq_len: Length of sequence.\n",
    "      n_features: Number of features.\n",
    "      n_filters: CNN hyper paramemter.\n",
    "      drop_rate: Keras hyper parameter.\n",
    "      \n",
    "    Returns:\n",
    "      CNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_len, n_features, 1)),\n",
    "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
    "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
    "        MaxPool2D(pool_size=(2,1)),\n",
    "        Flatten(),\n",
    "        Dropout(drop_rate),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553a195",
   "metadata": {},
   "source": [
    "and the model is presented by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48e39829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 60, 1, 8)          664       \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 58, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 29, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 27, 1, 8)          200       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 1, 8)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 104)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 104)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,169\n",
      "Trainable params: 1,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = cnnpred_2d()\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8caa2d2",
   "metadata": {},
   "source": [
    "The first convolutional layer has 8 units, and is applied across all features in each time step. It is followed by a second convolutional layer to consider three consecutive days at once, for it is a common belief that three days can make a trend in the stock market. It is then applied to a max pooling layer and another convolutional layer before it is flattened into a one-dimensional array and applied to a fully-connected layer with sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4017a",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Training, validation, and test</font>\n",
    "That’s it for the model. The paper used [**MAE**](https://en.wikipedia.org/wiki/Mean_absolute_error) as the loss metric and also monitor for accuracy and F1 score to determine the quality of the model. We should point out that F1 score depends on precision and recall ratios, which are both considering the positive classification. The paper, however, consider the average of the F1 from positive and negative classification. Explicitly, it is the F1-macro metric:\n",
    "![F1-macro metric](images/3.PNG)\n",
    "\n",
    "The first term in the big parenthesis above is the normal F1 metric that considered positive classifications. And the second term is the reverse, which considered the negative classifications.\n",
    "\n",
    "While this metric is available in scikit-learn as [**sklearn.metrics**.f1_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) there is no equivalent in Keras. Hence we would create our own by borrowing code from [this stackexchange question](https://datascience.stackexchange.com/questions/45165/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db5dae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall)/(precision + recall + K.epsilon()))\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    f_pos = f1_m(y_true, y_pred)\n",
    "    # negative version of the data and prediction\n",
    "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
    "    return (f_pos + f_neg) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1d96",
   "metadata": {},
   "source": [
    "The training process can take hours to complete. Hence we want to save the model in the middle of the training so that we may interrupt and resume it. We can make use of checkpoint features in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48ab3b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "      checkpoint_path,\n",
    "      monitor='val_f1macro', mode=\"max\", verbose=0,\n",
    "      save_best_only=True, save_weights_only=False, save_freq=\"epoch\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb830c",
   "metadata": {},
   "source": [
    "We set up a filename template `checkpoint_path` and ask Keras to fill in the epoch number as well as validation F1 score into the filename. We save it by monitoring the validation’s F1 metric, and this metric is supposed to increase when the model gets better. Hence we pass in the `mode=\"max\"` to it.\n",
    "\n",
    "It should now be trivial to train our model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fe39b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "400/400 [==============================] - 58s 142ms/step - loss: 0.4460 - acc: 0.5563 - f1macro: 0.3844 - val_loss: 0.3691 - val_acc: 0.6453 - val_f1macro: 0.5329\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 56s 139ms/step - loss: 0.3441 - acc: 0.6758 - f1macro: 0.6466 - val_loss: 0.2614 - val_acc: 0.7688 - val_f1macro: 0.7538\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 56s 140ms/step - loss: 0.2752 - acc: 0.7419 - f1macro: 0.7285 - val_loss: 0.2404 - val_acc: 0.7750 - val_f1macro: 0.7622\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 57s 142ms/step - loss: 0.2493 - acc: 0.7648 - f1macro: 0.7548 - val_loss: 0.2188 - val_acc: 0.7945 - val_f1macro: 0.7834\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.2251 - acc: 0.7876 - f1macro: 0.7787 - val_loss: 0.1936 - val_acc: 0.8172 - val_f1macro: 0.8086\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 54s 135ms/step - loss: 0.2133 - acc: 0.7968 - f1macro: 0.7892 - val_loss: 0.1836 - val_acc: 0.8258 - val_f1macro: 0.8213\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 53s 134ms/step - loss: 0.2008 - acc: 0.8085 - f1macro: 0.8016 - val_loss: 0.1516 - val_acc: 0.8594 - val_f1macro: 0.8533\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 55s 138ms/step - loss: 0.1971 - acc: 0.8111 - f1macro: 0.8041 - val_loss: 0.1640 - val_acc: 0.8398 - val_f1macro: 0.8320\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.1926 - acc: 0.8149 - f1macro: 0.8084 - val_loss: 0.1721 - val_acc: 0.8328 - val_f1macro: 0.8260\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.1882 - acc: 0.8182 - f1macro: 0.8122 - val_loss: 0.1463 - val_acc: 0.8594 - val_f1macro: 0.8528\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.1829 - acc: 0.8233 - f1macro: 0.8174 - val_loss: 0.1478 - val_acc: 0.8562 - val_f1macro: 0.8527\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 56s 139ms/step - loss: 0.1796 - acc: 0.8261 - f1macro: 0.8205 - val_loss: 0.1497 - val_acc: 0.8516 - val_f1macro: 0.8465\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 56s 139ms/step - loss: 0.1762 - acc: 0.8288 - f1macro: 0.8235 - val_loss: 0.1493 - val_acc: 0.8531 - val_f1macro: 0.8483\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 55s 138ms/step - loss: 0.1761 - acc: 0.8287 - f1macro: 0.8232 - val_loss: 0.1508 - val_acc: 0.8508 - val_f1macro: 0.8463\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 57s 143ms/step - loss: 0.1738 - acc: 0.8306 - f1macro: 0.8253 - val_loss: 0.1445 - val_acc: 0.8602 - val_f1macro: 0.8549\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 55s 139ms/step - loss: 0.1717 - acc: 0.8328 - f1macro: 0.8279 - val_loss: 0.1364 - val_acc: 0.8641 - val_f1macro: 0.8585\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 55s 139ms/step - loss: 0.1666 - acc: 0.8377 - f1macro: 0.8325 - val_loss: 0.1342 - val_acc: 0.8641 - val_f1macro: 0.8593\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 56s 140ms/step - loss: 0.1660 - acc: 0.8381 - f1macro: 0.8332 - val_loss: 0.1423 - val_acc: 0.8609 - val_f1macro: 0.8544\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 56s 141ms/step - loss: 0.1661 - acc: 0.8373 - f1macro: 0.8322 - val_loss: 0.1315 - val_acc: 0.8695 - val_f1macro: 0.8657\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 55s 137ms/step - loss: 0.1642 - acc: 0.8390 - f1macro: 0.8337 - val_loss: 0.1408 - val_acc: 0.8594 - val_f1macro: 0.8568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76485dfac0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len    = 60\n",
    "batch_size = 128\n",
    "n_epochs   = 20\n",
    "n_features = 82\n",
    "\n",
    "model = cnnpred_2d(seq_len, n_features)\n",
    "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
    "history = model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
    "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
    "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037aaab",
   "metadata": {},
   "source": [
    "Two points to note in the above snippets. We supplied \"acc\" as the accuracy as well as the function `f1macro` defined above as the metrics parameter to the <font color='blue'>compile()</font> function. Hence these two metrics will be monitored during training. Because the function is named `f1macro`, we refer to this metric in the checkpoint’s monitor parameter as `val_f1macro`.\n",
    "\n",
    "Separately, in the <font color='blue'>fit()</font> function, we provided the input data through the <font color='blue'>datagen()</font> generator as defined above. Calling this function will produce a generator, which during the training loop, batches are fetched from it one after another. Similarly, validation data are also provided by the generator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
