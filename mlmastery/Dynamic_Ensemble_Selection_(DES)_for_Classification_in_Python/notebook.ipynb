{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/dynamic-ensemble-selection-in-python/)) <font size='3ptx'>**Dynamic ensemble selection is an ensemble learning technique that automatically selects a subset of ensemble members just-in-time when making a prediction.**</font>\n",
    "\n",
    "The technique involves fitting multiple machine learning models on the training dataset, then selecting the models that are expected to perform best when making a prediction for a specific new example, based on the details of the example to be predicted.\n",
    "\n",
    "**This can be achieved using a k-nearest neighbor model to locate examples in the training dataset that are closest to the new example to be predicted, evaluating all models in the pool on this neighborhood and using the models that perform the best on the neighborhood to make a prediction for the new example.**\n",
    "\n",
    "As such, the dynamic ensemble selection can often perform better than any single model in the pool and better than averaging all members of the pool, so-called static ensemble selection.\n",
    "\n",
    "In this tutorial, you will discover how to develop dynamic ensemble selection models in Python. After completing this tutorial, you will know:\n",
    "* Dynamic ensemble selection algorithms automatically choose ensemble members when making a prediction on new data.\n",
    "* How to develop and evaluate dynamic ensemble selection models for classification tasks using the scikit-learn API.\n",
    "* How to explore the effect of dynamic ensemble selection model hyperparameters on classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "* <font size='3ptx'>[**Dynamic Ensemble Selection**](#sect1)</font>\n",
    "* <font size='3ptx'>[**k-Nearest Neighbor Oracle (KNORA) With Scikit-Learn**](#sect2)</font>\n",
    "  * [KNORA-Eliminate (KNORA-E)](#sect2_1)\n",
    "  * [KNORA-Union (KNORA-U)](#sect2_2)\n",
    "* <font size='3ptx'>[**Hyperparameter Tuning for KNORA**](#sect3)</font>\n",
    "  * [Explore k in k-Nearest Neighbor](#sect3_1)\n",
    "  * [Explore Algorithms for Classifier Pool](#sect3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import make_classification\n",
    "from deslib.des.knora_e import KNORAE\n",
    "from deslib.des.knora_u import KNORAU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Dynamic Ensemble Selection</font> ([back](#sect0))\n",
    "<font size='3ptx'>**Multiple Classifier Systems refer to a field of machine learning algorithms that use multiple models to address classification predictive modeling problems.**</font>\n",
    "\n",
    "The first class of multiple classifier systems to find success is referred to as [**Dynamic Classifier Selection**](https://www.researchgate.net/publication/319632614_Dynamic_classifier_selection_Recent_advances_and_perspectives), or DCS for short.\n",
    "* **Dynamic Classifier Selection:** Algorithms that dynamically choose one from among many trained models to make a prediction based on the specific details of the input.\n",
    "\n",
    "**Dynamic Classifier Selection algorithms generally involve partitioning the input feature space in some way and assigning specific models to be responsible for making predictions for each partition.** There are a variety of different DCS algorithms and research efforts are mainly focused on how to evaluate and assign classifiers to specific regions of the input space.\n",
    "> After training multiple individual learners, DCS dynamically selects one learner for each test instance. […] DCS makes predictions by using one individual learner. <br/><br/>\n",
    "> [**— Page 93, Ensemble Methods: Foundations and Algorithms, 2012.**](https://amzn.to/32L1yWD)\n",
    "<br/>\n",
    "\n",
    "A natural extension to DCS is algorithms that select one or more models dynamically in order to make a prediction. That is, selecting a subset or ensemble of classifiers dynamically. These techniques are referred to as <font color='darkblue'>**dynamic ensemble selection, or DES**</font>.\n",
    "* **Dynamic Ensemble Selection**: Algorithms that dynamically choose a subset of trained models to make a prediction based on the specific details of the input.\n",
    "\n",
    "**Dynamic Ensemble Selection algorithms operate much like DCS algorithms, except predictions are made using votes from multiple classifier models instead of a single best model.** In effect, each region of the input feature space is owned by a subset of models that perform best in that region.\n",
    "> … given the fact that selecting only one classifier can be highly error-prone, some researchers decided to select a subset of the pool of classifiers rather than just a single base classifier. All base classifiers that obtained a certain competence level are used to compose the EoC, and their outputs are aggregated to predict the label … <br/><br/>\n",
    "> [**— Dynamic Classifier Selection: Recent Advances And Perspectives, 2018.**](https://www.sciencedirect.com/science/article/pii/S1566253517304074)\n",
    "\n",
    "Perhaps the canonical approach to dynamic ensemble selection is the k-Nearest Neighbor Oracle, or [**KNORA**](https://arxiv.org/abs/1804.06943), algorithm as it is a natural extension of the canonical dynamic classifier selection algorithm “<font color='darkblue'>**Dynamic Classifier Selection Local Accuracy**</font>,” or DCS-LA.\n",
    "\n",
    "DCS-LA involves selecting the k-nearest neighbors from the training or validation dataset for a given new input pattern, then selecting the single best classifier based on its performance in that neighborhood of k examples to make a prediction on the new example.\n",
    "\n",
    "KNORA was described by Albert Ko, et al. in their 2008 paper titled “[**From Dynamic Classifier Selection To Dynamic Ensemble Selection**](https://dollar.biz.uiowa.edu/~street/ko07.pdf).” It is an extension of DCS-LA that selects multiple models that perform well on the neighborhood and whose predictions are then combined using majority voting to make a final output prediction.\n",
    "> For any test data point, KNORA simply finds its nearest K neighbors in the validation set, figures out which classifiers correctly classify those neighbors in the validation set and uses them as the ensemble for classifying the given pattern in that test set. <br/><br/>\n",
    "> [**— From Dynamic Classifier Selection To Dynamic Ensemble Selection, 2008.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320307004499)\n",
    "\n",
    "**The selected classifier models are referred to as “<font color='darkblue'>oracles</font>“, hence the use of oracle in the name of the method.**\n",
    "\n",
    "The ensemble is considered dynamic because the members are chosen just-in-time conditional on the specific input pattern requiring a prediction. This is opposed to static, where ensemble members are chosen once, such as averaging predictions from all classifiers in the model.\n",
    "> This is done through a dynamic fashion, since different patterns might require different ensembles of classifiers. Thus, we call our method a dynamic ensemble selection. <br/><br/>\n",
    "> [**— From Dynamic Classifier Selection To Dynamic Ensemble Selection, 2008.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320307004499)\n",
    "\n",
    "Two versions of KNORA are described, including <font color='darkblue'>**KNORA-Eliminate**</font> and <font color='darkblue'>**KNORA-Union**</font>.\n",
    "* **KNORA-Eliminate (KNORA-E)**: Ensemble of classifiers that achieves perfect accuracy on the neighborhood of the new example, with a reducing neighborhood size until at least one perfect classifier is located.\n",
    "* **KNORA-Union (KNORA-U)**: Ensemble of all classifiers that makes at least one correct prediction on the neighborhood with weighted voting and votes proportional to accuracy on the neighborhood.\n",
    "\n",
    "<font color='darkblue'>**KNORA-Eliminate**</font>, or <font color='darkblue'>**KNORA-E**</font> for short, involves selecting all classifiers that achieve perfect predictions on the neighborhood of k examples in the neighborhood. If no classifier achieves 100 percent accuracy, the neighborhood size is reduced by one and the models are re-evaluated. This process is repeated until one or more models are discovered that has perfect performance, and then used to make a prediction for the new example.\n",
    "> In the case where no classifier can correctly classify all the K-nearest neighbors of the test pattern, then we simply decrease the value of K until at least one classifier correctly classifies its neighbors <br/><br/>\n",
    "> [**— From Dynamic Classifier Selection To Dynamic Ensemble Selection, 2008.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320307004499)\n",
    "\n",
    "<font color='darkblue'>**KNORA-Union**</font>, or <font color='darkblue'>**KNORA-U**</font> for short, involves selecting all classifiers that make at least one correct prediction in the neighborhood. The predictions from each classifier are then combined using a weighted average, where the number of correct predictions in the neighborhood indicates the number of votes assigned to each classifier.\n",
    "> The more neighbors a classifier classifies correctly, the more votes this classifier will have for a test pattern <br/><br/>\n",
    "> [**— From Dynamic Classifier Selection To Dynamic Ensemble Selection, 2008.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320307004499)\n",
    "\n",
    "Now that we are familiar with DES and the KNORA algorithm, let’s look at how we can use it on our own classification predictive modeling projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>k-Nearest Neighbor Oracle (KNORA) With Scikit-Learn</font> ([back](#sect0))\n",
    "<font size='3ptx'>**The Dynamic Ensemble Library, or [DESlib](https://github.com/scikit-learn-contrib/DESlib) for short, is a Python machine learning library that provides an implementation of many different dynamic classifiers and dynamic ensemble selection algorithms.**</font>\n",
    "\n",
    "[**DESlib**](https://github.com/scikit-learn-contrib/DESlib) is an easy-to-use ensemble learning library focused on the implementation of the state-of-the-art techniques for dynamic classifier and ensemble selection. First, we can install the DESlib library using the pip package manager, if it is not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can then check that the library was installed correctly and is ready to be used by loading the library and printing the installed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.5\n"
     ]
    }
   ],
   "source": [
    "# check deslib version\n",
    "import deslib\n",
    "print(deslib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the script will print your version of the DESlib library you have installed. Your version should be the same or higher than `0.3` in our case. If not, you must upgrade your version of the DESlib library.\n",
    "\n",
    "The [**DESlib**](https://github.com/scikit-learn-contrib/DESlib) provides an implementation of the KNORA algorithm with each dynamic ensemble selection technique via the [**KNORAE**](https://deslib.readthedocs.io/en/latest/modules/des/knora_e.html) and [**KNORAU**](https://deslib.readthedocs.io/en/latest/modules/des/knora_u.html) classes respectively. Each class can be used as a scikit-learn model directly, allowing the full suite of scikit-learn data preparation, modeling pipelines, and model evaluation techniques to be used directly.\n",
    "\n",
    "Both classes use a k-nearest neighbor algorithm to select the neighbor with a default value of `k=7`.\n",
    "\n",
    "A [**bootstrap aggregation**](https://machinelearningmastery.com/bagging-ensemble-with-python/) (<font color='brown'>bagging</font>) ensemble of decision trees is used as the pool of classifier models considered for each classification that is made by default, although this can be changed by setting “<font color='violet'>pool_classifiers</font>” to a list of models.\n",
    "\n",
    "We can use the [make_classification()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to create **a synthetic binary classification problem with 10,000 examples and 20 input features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# synthetic binary classification dataset\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with the [**DESlib API**](https://github.com/scikit-learn-contrib/DESlib), let’s look at how to use each KNORA algorithm on our synthetic classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_1'></a>\n",
    "### <font color='darkgreen'>KNORA-Eliminate (KNORA-E)</font>\n",
    "**We can evaluate a KNORA-Eliminate dynamic ensemble selection algorithm on the synthetic dataset.**\n",
    "\n",
    "In this case, we will use default model hyperparameters, including bagged decision trees as the pool of classifier models and a `k=7` for the selection of the local neighborhood when making a prediction.\n",
    "\n",
    "We will evaluate the model using repeated stratified k-fold cross-validation with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.913 (0.011)\n"
     ]
    }
   ],
   "source": [
    "# evaluate dynamic KNORA-E dynamic ensemble selection for binary classification\n",
    "\n",
    "# define the model\n",
    "model = KNORAE()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10,\n",
    "    n_repeats=3,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the KNORA-E ensemble and default hyperparameters achieve a classification accuracy of about 91.3 percent.\n",
    "\n",
    "We can also use the KNORA-E ensemble as a final model and make predictions for classification. First, the model is fit on all available data, then the <font color='blue'>predict()</font> function can be called to make predictions on new data.\n",
    "\n",
    "The example below demonstrates this on our binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=10000, \n",
    "    n_features=20, \n",
    "    n_informative=15,\n",
    "    n_redundant=5, \n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "# define the model\n",
    "model = KNORAE()\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [\n",
    "    0.2929949, -4.21223056, -1.288332, -2.17849815, -0.64527665,\n",
    "    2.58097719, 0.28422388, -7.1827928,-1.91211104, 2.73729512,\n",
    "    0.81395695, 3.96973717, -2.66939799,3.34692332, 4.19791821,\n",
    "    0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808\n",
    "]\n",
    "\n",
    "yhat = model.predict([row])\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example fits the KNORA-E dynamic ensemble selection algorithm on the entire dataset and is then used to make a prediction on a new row of data, as we might when using the model in an application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <font color='darkgreen'>KNORA-Union (KNORA-U)</font>\n",
    "**We can evaluate a KNORA-Union model on the synthetic dataset.**\n",
    "\n",
    "In this case, we will use default model hyperparameters, including bagged decision trees as the pool of classifier models and a `k=7` for the selection of the local neighborhood when making a prediction.\n",
    "\n",
    "We will evaluate the model using repeated stratified k-fold cross-validation with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.934 (0.010)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = KNORAU()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, \n",
    "    n_repeats=3, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the KNORA-U dynamic ensemble selection model and default hyperparameters achieve a classification accuracy of about 93.3 percent. Now that we are familiar with using the scikit-learn API to evaluate and use KNORA models, let’s look at configuring the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Hyperparameter Tuning for KNORA</font>\n",
    "<font size='3ptx'>**In this section, we will take a closer look at some of the hyperparameters you should consider tuning for the KNORA model and their effect on model performance.**</font>\n",
    "\n",
    "There are many hyperparameters we can look at for KNORA, although **in this case, we will look at the value of `k` in the k-nearest neighbor model used in the local evaluation of the models, and how to use a custom pool of classifiers.**\n",
    "\n",
    "We will use the KNORA-Union as the basis for these experiments, although the choice of the specific method is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_1'></a>\n",
    "### <font color='darkgreen'>Explore k in k-Nearest Neighbors</font>\n",
    "**The configuration of the k-nearest neighbors algorithm is critical to the KNORA model as it defines the scope of the neighborhood in which each ensemble is considered for selection.**\n",
    "\n",
    "The `k` value controls the size of the neighborhood and it is important to set it to a value that is appropriate for your dataset, specifically the density of samples in the feature space. A value too small will mean that relevant examples in the training set might be excluded from the neighborhood, whereas values too large may mean that the signal is being washed out by too many examples.\n",
    "\n",
    "The code example below explores the classification accuracy of the KNORA-U algorithm with k values from 2 to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2 0.935 (0.008)\n",
      ">3 0.937 (0.008)\n",
      ">4 0.934 (0.009)\n",
      ">5 0.934 (0.008)\n",
      ">6 0.935 (0.009)\n",
      ">7 0.936 (0.008)\n",
      ">8 0.934 (0.007)\n",
      ">9 0.934 (0.008)\n",
      ">10 0.935 (0.008)\n",
      ">11 0.938 (0.009)\n",
      ">12 0.938 (0.009)\n",
      ">13 0.936 (0.007)\n",
      ">14 0.935 (0.008)\n",
      ">15 0.935 (0.008)\n",
      ">16 0.934 (0.009)\n",
      ">17 0.934 (0.010)\n",
      ">18 0.935 (0.007)\n",
      ">19 0.936 (0.010)\n",
      ">20 0.933 (0.010)\n",
      ">21 0.935 (0.009)\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(\n",
    "        n_samples=10000, \n",
    "        n_features=20, \n",
    "        n_informative=15, \n",
    "        n_redundant=5,\n",
    "        random_state=7\n",
    "    )\n",
    "    return X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for n in range(2,22):\n",
    "        models[str(n)] = KNORAU(k=n)\n",
    "        \n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(\n",
    "        n_splits=10,\n",
    "        n_repeats=3,\n",
    "        random_state=1\n",
    "    )\n",
    "    \n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEwCAYAAACE62RIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAkIElEQVR4nO3df5Ac9Xnn8c+HZUEOYFtIimMjfiWHXavacsDecM5FNiyOc5BLQVASGyXnC8mmKCdhL7/IFdy4fA7UFEmMc0kprqhIloudHwO2YmPi8xkIXuJsVeywAgSCtbACjpGwYR0giU0Ei/TcH9Mrj4bZ1eyqvz3TPe9X1dbOdE/v0z07/e2nv/30dxwRAgAAQL6O6/UKAAAAVBFJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACXSVZti+2vcf2XtvXdph/pu17bD9k+17bG1vmnWH7Lttzth+1fVaO6w8AANCXfLRxsmwPSXpM0rsk7ZN0n6StEfFoy2s+IekzEfFR2xdJ+rmIeG82715J9Yi42/bJkg5FxAtLxVu/fn2cddZZx7ZVAAAABdi5c+c3I2JDp3nHd7H8+ZL2RsTjkmT7VkmXSXq05TWbJP169nha0u3ZazdJOj4i7pakiPjW0YKdddZZmp2d7WK1AAAAesv2Py01r5vLhadJerLl+b5sWqtdkrZkjy+XdIrtdZLeKOl525+0/YDtD2U9YwAAAJWWV+H7NZIusP2ApAsk7Zd0UM2esrdn839A0vdKurJ9YdtX2Z61PTs/P5/TKgEAAPRON0nWfkmntzzfmE07LCKeiogtEXGepFo27Xk1e70ejIjHI+JlNS8jvqU9QETcHBFjETG2YUPHy5oAAACl0k2SdZ+kc2yfbfsESVdIuqP1BbbX2178W9dJuqVl2dfaXsycLtKRtVwAAACVdNQkK+uBulrSnZLmJH08Ih6xfb3tS7OXXShpj+3HJL1OUj1b9qCalwrvsf2wJEv649y3AgAAoM8cdQiHoo2NjQV3FwIAgDKwvTMixjrNY8R3AACABEiygAppNBoaHR3V0NCQRkdH1Wg0er1KADCwuhmMFEAJNBoN1Wo1TU1NafPmzZqZmdHExIQkaevWrT1eOwAYPNRkARUxOjqqbdu2aXx8/PC06elpTU5Oavfu3T1cMwCoruVqskiygIoYGhrSgQMHNDw8fHjawsKC1qxZo4MHD/ZwzQCguih8BwbAyMiIZmZmjpg2MzOjkZGRHq0RAAw2kiygImq1miYmJjQ9Pa2FhQVNT09rYmJCtVqt16sGAAOJwnegIhaL2ycnJzU3N6eRkRHV63WK3gGgR6jJAgAAWCVqsgAAAApGkgUAAJAASRYAAEACJFkAAAAJkGQBAAAkwBAOAACgI9vLzu+3EQr6DT1ZANBBo9HQ6OiohoaGNDo6qkaj0etVAgoXEUf8tE/D8ujJAoA2jUZDtVpNU1NT2rx5s2ZmZjQxMSFJDO4KoGv0ZAFAm3q9rqmpKY2Pj2t4eFjj4+OamppSvV7v9aoBKBFGfAeANkNDQzpw4ICGh4cPT1tYWNCaNWt08ODBHq4Z0Fu2k14mLGMNGCO+A8AKjIyMaGZm5ohpMzMzGhkZ6dEaAYOhajVgJFkA0KZWq2liYkLT09NaWFjQ9PS0JiYmVKvVer1qAEqEwncAaLNY3D45Oam5uTmNjIyoXq9T9A5gRajJAgAAXUldk9XreKtBTRYAAEDBSLIAAAASIMkCgD7ACPNA9VD4DgA9xgjzQDXRkwUAPcYI80A1cXchAPQYI8yjLLi78JW4uxAA+hgjzGOlqOErB5IsAOgxRpjHSizW8G3btk0HDhzQtm3bVKvVSLT6EJcLAaAPNBoN1ev1wyPM12o1it7R0ejoqLZt26bx8fHD06anpzU5Oandu3cnjc3lwlda7nIhSRZQAWX85vqVqPr2ASvRyxq+qiVZebQt1GQBFVe1b65vV/XtA1aCGr78pG5bSLIAACgRavjKg8FIAQAokcVavcnJycM1fPV6nRq+PkRNFlBBZSgWPRZV3z6gX1WtJiuPeMvVZNGTtQJFF99Wvdh3ue0r+7ZV3SB/NqXybx+AYnRVk2X7Ytt7bO+1fW2H+Wfavsf2Q7bvtb2xZd5B2w9mP3fkufJFW644LkWjW3S8olHIXF5VL0Sv+r4HoBhH7cmyPSTpI5LeJWmfpPts3xERj7a87CZJH4uIj9q+SNKNkt6bzfv3iDg339UGAADob930ZJ0vaW9EPB4RL0m6VdJlba/ZJOnz2ePpDvMBAAAGSjdJ1mmSnmx5vi+b1mqXpC3Z48slnWJ7XfZ8je1Z21+0/ePHsrIAAABlkdc4WddIusD2A5IukLRf0uKws2dmVfc/Len3bX9f+8K2r8oSsdn5+fmcVgn4DtvL/gCoDvZ19Itukqz9kk5veb4xm3ZYRDwVEVsi4jxJtWza89nv/dnvxyXdK+m89gARcXNEjEXE2IYNG1axGcDyql6oDeA72NfRL7pJsu6TdI7ts22fIOkKSUfcJWh7ve3Fv3WdpFuy6Wttn7j4Gkk/JKm1YB4AAKCSjppkRcTLkq6WdKekOUkfj4hHbF9v+9LsZRdK2mP7MUmvk1TPpo9ImrW9S82C+N9uuysRAACgkhjxfZXKMArtamIsJ/U3oVft/eylqr+fRcar+melCIPUtlRdlff11cZjxHd0pfWDRcMEIC+0LRhUed1dCAAAgBYkWQAAAAmQZAEAACRQ6pqsXhZTAkBeaMvKi/8dllPqJItiSgBVQFtWXu3/K/5/aMXlQgAAgARIsgAAABIgyQIAAEig1DVZQL+iGBb9jM9nefG/KxeSLCABimHRzyi0Ly/alnLhciEAAEACJFkAAAAJkGQBAAAkQJIFAACQAEkWAABAAiRZAAAACZBkAQAAJECSBQAAkABJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACJFnLOPXUU2W744+kJeedeuqpPV5zAADQa8f3egX62XPPPaeIWPFyi0kYAAAYXPRkAQCAw7iKkx96sgD0pVNPPVXPPffckvM79RivXbtWzz77bMrVAiqPqzj5oScLQF9abOhX8rNcUrYcztyBwbGa/X21+zo9WQAGHmfuwOBYzf6+2n2dniwAAIAESLIAAAASIMkCAGAFqOFDt6jJAgCU2mruRJVWfzcqNXzoFkkWACBXJD1AE0kWACBXJD1AEzVZAAAACXSVZNm+2PYe23ttX9th/pm277H9kO17bW9sm/9q2/ts/2FeK15FFFMCg4F9HRgMR02ybA9J+oikSyRtkrTV9qa2l90k6WMR8WZJ10u6sW3+DZK+cOyrW22rGeH6WEa5LlrVDyxV3z7kp+h9nc8m0Bvd1GSdL2lvRDwuSbZvlXSZpEdbXrNJ0q9nj6cl3b44w/ZbJb1O0uckjR37KqOsql6nUfXtQ3nx2QR6o5vLhadJerLl+b5sWqtdkrZkjy+XdIrtdbaPk/RhSdcc64oCAACUSV53F14j6Q9tX6nmZcH9kg5K+iVJn42IfcudEdm+StJVknTGGWfktEo4mqJvsy5a1bevaEW/n/G/Xi198DUrXwYA+kQ3SdZ+Sae3PN+YTTssIp5S1pNl+2RJPxERz9v+QUlvt/1Lkk6WdILtb0XEtW3L3yzpZkkaGxtbsk+bg2a+qn4JoerbV7Si30//1r+u6ktc44OrCgcAuesmybpP0jm2z1YzubpC0k+3vsD2eknPRsQhSddJukWSIuJnWl5zpaSx9gRrJThoAgCAsjhqTVZEvCzpakl3SpqT9PGIeMT29bYvzV52oaQ9th9Ts8i9nmh9AQAASqGrmqyI+Kykz7ZN+0DL4x2Sdhzlb/yppD9d8RoCAACUECO+AyXF2EcAqqDKbRnfXQiUFDWKAKqgym0ZPVkAsIT5F+Z15eeu1Df//Zu9XhUAJUSSBaBUikx8tj+0Xfc/fb+279qePBaA6iHJAlAqRSU+8y/M69N7P61Q6Pa9t9ObBWDFqMkCUBrtic/7vv99Wv+q9cf8dzuNLr993VodOvlk6Tjr0MIBbf+TMb3/n5975XIAsASSLAClsf2h7ToUhyRJh+KQtu/arve/7f3H/HfbR5eff2Fen/7kJVo4+KIkaeE46/a16/W+X5g9IqljhHkUgW87KS+SLAClsNiLtXBoQZK0cGgh196sVq3J3KI8k7rVfC/j4eUwcKp8913VkWStwvwL8/rNL/ymbrrgptwbdwCdpU58Wu16ZtfhZG7RwqEFPfjMg7n8/dV8L6NEzxlQNiRZy1jqbHP7urW6/5STO9ZoHF4OQK5SJz6tdly67BdYAEBXSLKW0elsc7FWIw6+2LFGQ+JsE0hhEBIfesmBamEIhxXqVHgLAHlgXC70MwbnXTl6slagyMLbKqLYF1haquEpgLy0ngTkXQdZVSRZK1Bk4W0V9UuxL5dk0I9SDU8B5IGTgNUhyVqBIgtvkQ5nY6tDT2Q69JKXyyDuC5wErI5X07OQ0tjYWMzOznacZ7tjT8jReiaWWu5oWK56y82/MK9LPnmJXjz4ok4cOlGf+4nPdb5xoaTb14vl+mn/65f3ZKXL3fDFG/Spr3zqiJO44eOGteWcLUccyHq9niw3mMu1tpuLOrWfvV7PlMstt4ztnREx1mleqXqyGFIhX/1yNlbk5TvOxvJHz+Cxo5cc/WxQSmVSHItKlWQxpEK++qVGqqiDNJdk8kedRj4GYXgKiXrIshqUk4AUx6LSD+HAkArl1n6QTnlr8HJnY1VS5G3W7H/lVvQt+UUPUVH1IQeK2r4dl+7Qwz/78Ct+qnRykOpYVOqarEG5Tlx0zUuR8VprUTrVoBxLvPZLoT/5hu/RnhNPeMXL3vTiS9rx1Dfalv2XFYfrh/dTar6nn9jzCb37Te/ueDZW5TqNY4m1Gqv9At6i25alygJuWLdWnzjlZL37377VsdSiuWw++0LKesh+2D6pOvv6aspIvrNsfu9nkcsd7Vi02pqsUidZg1IsWtRBs+h4/XiQTrFckf+/Igv7+3H/W/VBpSR/s6yfFSntCVU/bJ9U3X296st1cywaiML3doNwnbjompci4w1CMWXR/78iC/sHYf+rspSflfabauaHjtOnN75BC8c1K1QWDi3o9rmG3nf3h7X+4KEjl8tJ0Te5VHlfT63om7Da421ft1aHTj5ZOu47PdqHFg4ccTPdamOVOsmq0vXgpRS9I3GQzleR72fRhf1F7H8rvYy3du3aRGtSLak/K+031Wz/4g069JVPSS37+6HjT9T2d/3GK3s9P3jM4Xtyk0uV9/XUir4Jqz3erjt+UgvP7TniNQvHWQ+eOSZN7jimWKVOsqqu6B2pigfpXir6/axaz+ByjW6KS3iDpOjPStEnVEVvX+p9fTU9L4eXw1GlPBaRZPWxohuKqh2ke63qBzKUV9GflaJPqKqW1K2m50Uq//BFVRjygySrjxXdUHCQzlfVD2Qor6p/Vqqe1FX9/7eoCgMdl/ruwm6VZblBvG22Ssvx/8t3uaL/ZpGxyvI/YDmW69Vy/Xb35EDeXVg1/TICO1aH/x8A5KMqd0+SZAEl1S/fPYnVWc0AqNw9iUFQpbsnSbKAkqLnrLy4cxJYWpVuwir9dxcCAIDqqNJNWKXryaKLHQCA6qrS3ZOlSrLoYgcAAGXB5UIAAIAESLIAAAASIMkCAABIgCQLAAAggVIVvgMAMIi4s76cuurJsn2x7T2299q+tsP8M23fY/sh2/fa3tgy/X7bD9p+xPb78t4AAACqLCKW/Flu/rPPPtvjNcdRkyzbQ5I+IukSSZskbbW9qe1lN0n6WES8WdL1km7Mpn9d0g9GxLmS/qOka22/Iad1BwAA6Fvd9GSdL2lvRDweES9JulXSZW2v2STp89nj6cX5EfFSRLyYTT+xy3gAAACl103Sc5qkJ1ue78umtdolaUv2+HJJp9heJ0m2T7f9UPY3ficinjq2VS6W7RX/cB0cVcX+AADdy6vw/RpJf2j7SklfkLRf0kFJiognJb05u0x4u+0dEfF068K2r5J0lSSdccYZOa3SsWOEeeA72B/KjcJpoHjd9GTtl3R6y/ON2bTDIuKpiNgSEedJqmXTnm9/jaTdkt7eHiAibo6IsYgY27Bhw8q2AACwLAqngd7oJsm6T9I5ts+2fYKkKyTd0foC2+ttL/6t6yTdkk3faPtV2eO1kjZL2pPXygMAAPSro14ujIiXbV8t6U5JQ5JuiYhHbF8vaTYi7pB0oaQbbYealwt/OVt8RNKHs+mWdFNEPJxgO4COuEQCACtH25mPrmqyIuKzkj7bNu0DLY93SNrRYbm7Jb35GNcRWBVqiABg5Wg788OI7wOOs5V8Ff1+8v9Dv+KzCZBkDTTOVvJV9PvJ/w/9is8m0MTgoAAAAAnQkwUAKD0uT6IfkWT1GRoKAFgZLk+iX5Fk9REaCgAAqoOaLAAAgAToyUKhuBwKABgUJFkoDJdDAQD9YKUn/Ks92SfJAgBgheiVz1eR72eRJ/wkWQAArAC98vmq8vtJ4TsAAEACJFkAAAAJkGQBAAAkQJIFAACQAEkWAABAAiRZAAAACZBkAQAAJMA4WSvQPlha+/Myj+UB9LNOAxW2Tst732NfB5AHkqwVoGEFeqPofY99HUAeuFwIAACQAEkWAABAAiRZAAAACZBkAQAAJECSBQAAkABJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACJFkAAAAJkGQBAAAkwBdEAwnYXnYaX0CMVu2fl/bnfF6wiLalXEiygARo6LASfF7QLT4r5cLlQgCl0Wg0NDo6qqGhIY2OjqrRaPR6lQBgSfRkASiFRqOhWq2mqakpbd68WTMzM5qYmJAkbd26tcdrBwCvRE8WgFKo1+uamprS+Pi4hoeHNT4+rqmpKdXr9V6vGgB05H67vjs2Nhazs7NdvbZTAWCrftu2Y2G70O0hXrn0el8o4v0cGhrSgQMHNDw8fHjawsKC1qxZo4MHDyaNXWVV3/eqHq9oVX8/VxPP9s6IGOs0r6ueLNsX295je6/tazvMP9P2PbYfsn2v7Y3Z9HNt/73tR7J571nRmh9FRCz7AwyKQdgXRkZGNDMzc8S0mZkZjYyM9GiNAGB5R63Jsj0k6SOS3iVpn6T7bN8REY+2vOwmSR+LiI/avkjSjZLeK+kFSf8tIr5i+w2Sdtq+MyKez3tDAFRbrVbTe97zHp100kn62te+pjPOOEPf/va39Qd/8Ae9XjUA6KibnqzzJe2NiMcj4iVJt0q6rO01myR9Pns8vTg/Ih6LiK9kj5+S9IykDXmsOIDBVZXeOQDV1k2SdZqkJ1ue78umtdolaUv2+HJJp9he1/oC2+dLOkHSP65uVQEMsnq9rttuu01PPPGEDh06pCeeeEK33XYbhe8A+lZedxdeI+kC2w9IukDSfkmHK1Ftv17Sn0n6uYg41L6w7atsz9qenZ+fz2mVAFTJ3NycNm/efMS0zZs3a25urkdrBADL6ybJ2i/p9JbnG7Nph0XEUxGxJSLOk1TLpj0vSbZfLen/SqpFxBc7BYiImyNiLCLGNmzgaiKAV6LwHUDZdJNk3SfpHNtn2z5B0hWS7mh9ge31thf/1nWSbsmmnyDpU2oWxe/Ib7UBDJparaaJiQlNT09rYWFB09PTmpiYUK1W6/WqAUBHR727MCJetn21pDslDUm6JSIesX29pNmIuEPShZJutB2SviDpl7PF3y3pHZLW2b4ym3ZlRDyY61YAqLzFUd0nJyc1NzenkZER1et1RnsH0LdKPRjpICnDgGzEG1y8n+VS9OC1vRwsl7YlX1V/P/MejJTvLgSAAVN0ElDlpANYTum/u7DRaGh0dFRDQ0MaHR1Vo9Ho9SoBwIrRlgHVU+qerEajoVqtpqmpKW3evFkzMzOamJiQJOo0AJQGbRlQTaWuyRodHdW2bds0Pj5+eNr09LQmJye1e/fuVKvYE2W4Lk28wcX7eWwGqS0rGm1Lvqr+fuZdk1XqJGtoaEgHDhzQ8PDw4WkLCwtas2aNDh48uMyS5VOGD1o/x+tl4W0V8X7ma5DasqJVrS3rtaq/n3knWaWuyWJwQnQrIpb9wcrwfuaLtgyoplInWQxOCKAKaMuAaip14TuDEwKoAtoyoJpKXZM1SMpwXbpM8QAMBtqyfFX9/WQwUgAAUIhON7m0TqtyQpkHkiwAANARSdSxKXXhOwAAQL8iyQIAAEiAy4UAAGAgpa45I8kCAAADKXXNWekvF/LN9QAAoB+VuieLb64HAAD9qtQ9WfV6XVNTUxofH9fw8LDGx8c1NTWler3e61UDAAADrtQjvg/SN9eXYdTbMsUDMBhoy8qtDO/nciO+l7oni2+uBwYLNZgAyqTUSRbfXA8MjsUazG3btunAgQPatm2barUaiRaAvlXqy4VSs+Gt1+uHv7m+VqtVsui96l3eZegSRm+Njo5q27ZtGh8fPzxtenpak5OT2r17dw/XDP2MtqzcyvB+Lne5sPRJ1qAo4oPWaVC2Vinjl2FHQm8NUg0m8kOSVS69PA6tVmVrspCviFj2B+glajCB6qvacYgkC0ApUIOZL24iANIr9WCkAAbHYq3l5OTk4RrMer1eyRrM1BjIGSgGNVklUfXr/FXfPqCfDNJNBNRkITUK3yugijvucgWOVdtWoJ8M0k0EJFlIjcJ39KUqFTcCZcJNBEAxSLIAYMBwEwFQDArfAWDAcBMBUAxqskqC6/wAsHLUZCG15Wqy6MkCAFRK+001rc9JgFAkkiwAQKWQSKFfUPgOAACQAEkWAABAAiRZAAAACXSVZNm+2PYe23ttX9th/pm277H9kO17bW9smfc528/b/kyeKw4AANDPjppk2R6S9BFJl0jaJGmr7U1tL7tJ0sci4s2Srpd0Y8u8D0l6bz6rCwAAUA7d9GSdL2lvRDweES9JulXSZW2v2STp89nj6db5EXGPpH/LYV0BAABKo5sk6zRJT7Y835dNa7VL0pbs8eWSTrG97thXDwAAoJzyKny/RtIFth+QdIGk/ZK6/ip321fZnrU9Oz8/n9MqAQAA9E43SdZ+Sae3PN+YTTssIp6KiC0RcZ6kWjbt+W5XIiJujoixiBjbsGFDt4sBAAD0rW6SrPsknWP7bNsnSLpC0h2tL7C93vbi37pO0i35riYAAEC5HDXJioiXJV0t6U5Jc5I+HhGP2L7e9qXZyy6UtMf2Y5JeJ6m+uLztv5P0CUnvtL3P9n/OeRsAAAD6jvvtO57GxsZidna216vRd/hmdwDof7TVg8f2zogY6zSPL4juY8t9k7zEl6ACANDPSLL6GEkUAADlxXcXAgAAJECSBQAAkABJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACJFkAAAAJkGQBAAAkQJIFAB00Gg2Njo5qaGhIo6OjajQavV4lACXD1+oAQJtGo6FaraapqSlt3rxZMzMzmpiYkCRt3bq1x2sHoCzoyQKANvV6XVNTUxofH9fw8LDGx8c1NTWler3e61UDUCLuty8hHhsbi9nZ2V6vBoABNjQ0pAMHDmh4ePjwtIWFBa1Zs0YHDx7s4Zqh39lWvx1XkZbtnREx1mkePVkA0GZkZEQzMzNHTJuZmdHIyEiP1ghAGZFkAUCbWq2miYkJTU9Pa2FhQdPT05qYmFCtVuv1qgEoEQrfAaDNYnH75OSk5ubmNDIyonq9TtE7gBWhJgsAgFWyvez8fjvGIn/L1WTRkwUAwCqRRGE51GQBAAAkQJIFAACQAEkWAABAAiRZAAAACZBkAQAAJECSBQAAkABJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACffcF0bbnJf3TKhZdL+mbOa8O8YhHPOJVeduIRzziHXu8MyNiQ6cZfZdkrZbt2aW+BZt4xCMe8coQi3jEI1614nG5EAAAIAGSLAAAgASqlGTdTDziEY94JY9FPOIRr0LxKlOTBQAA0E+q1JMFAADQN0qfZNk+3fa07UdtP2L7VxLHW2P7H2zvyuL9Vsp4LXGHbD9g+zMFxPqq7YdtP2h7toB4r7W9w/aXbc/Z/sGEsd6Ubdfiz7/a/tWE8X4t+5zstt2wvSZVrCzer2SxHkmxXbZvsf2M7d0t0061fbftr2S/1yaO91PZ9h2ynetdR0vE+1D22XzI9qdsvzZxvBuyWA/avsv2G1LGa5n3G7bD9vqU8Wx/0Pb+ln3wR1PGy6ZPZv/DR2z/bsp4tm9r2bav2n4wcbxzbX9xsb22fX7CWN9v+++z48Nf2351HrGyv93xWJ6qfVkmXr7tS0SU+kfS6yW9JXt8iqTHJG1KGM+STs4eD0v6kqS3FbCdvy7pLyV9poBYX5W0vsD/4Ucl/UL2+ARJry0o7pCkb6g5xkmKv3+apCckvSp7/nFJVybcnlFJuyV9l6TjJf2NpP+Qc4x3SHqLpN0t035X0rXZ42sl/U7ieCOS3iTpXkljBWzfj0g6Pnv8OwVs36tbHv93SdtTxsumny7pTjXHKMxt319i+z4o6Zo8/29HiTee7QsnZs+/O/X72TL/w5I+kHj77pJ0Sfb4RyXdmzDWfZIuyB7/vKQbcty2jsfyVO3LMvFybV9K35MVEV+PiPuzx/8maU7Ng1uqeBER38qeDmc/SQvbbG+U9F8k/UnKOL1g+zVq7sxTkhQRL0XE8wWFf6ekf4yI1Qx+263jJb3K9vFqJj9PJYw1IulLEfFCRLws6W8lbckzQER8QdKzbZMvUzNRVvb7x1PGi4i5iNiTV4wu4t2VvZ+S9EVJGxPH+9eWpycpx/Zlif+fJP1vSf8jz1hHiZfEEvF+UdJvR8SL2WueSRxPkmTbkt4tqZE4Xkha7FF6jXJqY5aI9UZJX8ge3y3pJ/KIlcVb6liepH1ZKl7e7Uvpk6xWts+SdJ6avUsp4wxlXcDPSLo7IpLGk/T7ajaAhxLHWRSS7rK90/ZViWOdLWle0v9x83Lon9g+KXHMRVcoxwawXUTsl3STpK9J+rqkf4mIu1LFU7MX6+2219n+LjXPak9PGG/R6yLi69njb0h6XQExe+XnJf2/1EFs120/KelnJH0gcazLJO2PiF0p47S5Orskekuel5eX8EY194sv2f5b2z+QON6it0t6OiK+kjjOr0r6UPZ5uUnSdQljPaJm0iNJP6VE7UvbsTx5+5Iyd6hMkmX7ZEl/JelX284EcxcRByPiXDXPaM+3PZoqlu0fk/RMROxMFaODzRHxFkmXSPpl2+9IGOt4Nbuk/ygizpP0bTW7hJOyfYKkSyV9ImGMtWo2SGdLeoOkk2z/11TxImJOzctZd0n6nKQHJR1MFW+JdQgl7tntFds1SS9L+ovUsSKiFhGnZ7GuThUnS8b/pxIncm3+SNL3STpXzZOPDyeOd7ykUyW9TdJvSvp41suU2lYlPIlr8YuSfi37vPyasqsCify8pF+yvVPNS2wv5R1guWN5ivYlde5QiSTL9rCab9JfRMQni4qbXdaalnRxwjA/JOlS21+VdKuki2z/ecJ4iz0wi93qn5KUSyHlEvZJ2tfSG7hDzaQrtUsk3R8RTyeM8cOSnoiI+YhYkPRJSf8pYTxFxFREvDUi3iHpOTXrDFJ72vbrJSn7ndvlmH5h+0pJPybpZ7KGvih/oRwvyXTwfWqeBOzK2piNku63/T2pAkbE09mJ6iFJf6y07YvUbGM+mZV6/IOaVwRyK+7vJCsP2CLptpRxMj+rZtsiNU8ak72fEfHliPiRiHirmgnkP+b595c4lidrX4rIHUqfZGVnJFOS5iLi9wqIt8HZ3UW2XyXpXZK+nCpeRFwXERsj4iw1L299PiKS9YbYPsn2KYuP1Sz6fcWdSHmJiG9IetL2m7JJ75T0aKp4LYo4y/yapLfZ/q7sc/pONa/7J2P7u7PfZ6jZyP9lyniZO9Rs6JX9/nQBMQtj+2I1L9dfGhEvFBDvnJanlylt+/JwRHx3RJyVtTH71CwG/kaqmIsHzMzlSti+ZG5Xs/hdtt+o5s01qb9w+IclfTki9iWOIzVrsC7IHl8kKdnlyZb25ThJ75e0Pce/vdSxPEn7UljucKyV873+kbRZze7Dh9S8PPKgpB9NGO/Nkh7I4u1WjneOdBH7QiW+u1DS90ralf08IqlWwHadK2k2e09vl7Q2cbyTJP2zpNcUsG2/peZBcrekP1N2h1PCeH+nZpK6S9I7E/z9hpqXeBbUPCBPSFon6R41G/e/kXRq4niXZ49flPS0pDsTx9sr6cmW9iXPu/06xfur7PPykKS/VrMYN1m8tvlfVb53F3bavj+T9HC2fXdIen3ieCdI+vPsPb1f0kWp309JfyrpfXnFOcr2bZa0M9vnvyTprQlj/YqaveOPSfptZQOa5xSv47E8VfuyTLxc2xdGfAcAAEig9JcLAQAA+hFJFgAAQAIkWQAAAAmQZAEAACRAkgUAAJAASRYAAEACJFkAAAAJkGQBAAAk8P8BtM9J4/N4oP8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model performance for comparison\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_2'></a>\n",
    "### <font color='darkgreen'>Explore Algorithms for Classifier Pool</font>\n",
    "**The choice of algorithms used in the pool for the KNORA is another important hyperparameter.**\n",
    "\n",
    "By default, [**bagged decision trees**](https://en.wikipedia.org/wiki/Bootstrap_aggregating) are used, as it has proven to be an effective approach on a range of classification tasks. Nevertheless, a custom pool of classifiers can be considered.\n",
    "> In the majority of DS publications, the pool of classifiers is generated using either well known ensemble generation methods such as Bagging, or by using heterogeneous classifiers. <br/><br/>\n",
    "> [**— Dynamic Classifier Selection: Recent Advances And Perspectives, 2018.**](https://www.sciencedirect.com/science/article/pii/S1566253517304074)\n",
    "\n",
    "**This requires first defining a list of classifier models to use and fitting each on the training dataset. Unfortunately, this means that the automatic k-fold cross-validation model evaluation methods in scikit-learn cannot be used in this case**. Instead, we will use a train-test split so that we can fit the classifier pool manually on the training dataset.\n",
    "\n",
    "The list of fit classifiers can then be specified to the <font color='darkblue'>**KNORA-Union**</font> (<font color='brown'>or</font> <font color='darkblue'>**KNORA-Eliminate**</font>) class via the “<font color='violet'>pool_classifiers</font>” argument. In this case, we will use a pool that includes logistic regression, a decision tree, and a naive Bayes classifier.\n",
    "\n",
    "The complete example of evaluating the KNORA ensemble and a custom set of classifiers on the synthetic dataset is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "# define classifiers to use in the pool\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(),\n",
    "    GradientBoostingClassifier(),\n",
    "]\n",
    "\n",
    "# fit each classifier on the training set\n",
    "for c in classifiers:\n",
    "    c.fit(X_train, y_train)\n",
    "\n",
    "# define the KNORA-U model\n",
    "model = KNORAU(pool_classifiers=classifiers)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieved an accuracy of about 96.2 percent.\n",
    "\n",
    "**In order to adopt the KNORA model, it must perform better than any contributing model**. Otherwise, we would simply use the contributing model that performs better. **We can check this by evaluating the performance of each contributing classifier on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> LogisticRegression: 0.878\n",
      "> DecisionTreeClassifier: 0.884\n",
      "> GaussianNB: 0.873\n",
      "> SVC: 0.982\n",
      "> GradientBoostingClassifier: 0.942\n"
     ]
    }
   ],
   "source": [
    "# evaluate contributing models\n",
    "for c in classifiers:\n",
    "    yhat = c.predict(X_test)\n",
    "    score = accuracy_score(y_test, yhat)\n",
    "    print('> %s: %.3f' % (c.__class__.__name__, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying a pool of classifiers, it is also possible to specify a single ensemble algorithm from the scikit-learn library and the KNORA algorithm will automatically use the internal ensemble members as classifiers.\n",
    "\n",
    "For example, we can use a random forest ensemble with 1,000 members as the base classifiers to consider within KNORA as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classifiers to use in the pool\n",
    "pool = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "# fit the classifiers on the training set\n",
    "pool.fit(X_train, y_train)\n",
    "\n",
    "# define the KNORA-U model\n",
    "model = KNORAU(pool_classifiers=pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying this together, the complete example of KNORA-U with random forest ensemble members as classifiers is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968\n",
      ">RandomForestClassifier: 0.968\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (score))\n",
    "\n",
    "# evaluate the standalone model\n",
    "yhat = pool.predict(X_test)\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('>%s: %.3f' % (pool.__class__.__name__, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Supplement</font> ([back](#sect0))\n",
    "* [A Gentle Introduction to Mixture of Experts Ensembles](https://machinelearningmastery.com/mixture-of-experts/?fbclid=IwAR29lvTUiXQ-TVQtfnTtbj9FIRLAIsor16hBOohlLox0reLV6IZcVbEieho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
