{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f9e1e3",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "<b><font size='3ptx'>In this tutorial, you will discover how to implement multi-head attention from scratch in TensorFlow and Keras.</font></b>\n",
    "\n",
    "([article source](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)) We have already familiarised ourselves with the theory behind the [**Transformer model**](https://machinelearningmastery.com/the-transformer-model/) and its [**attention mechanism**](https://machinelearningmastery.com/the-transformer-model/), and we have already started our journey of implementing a complete model by seeing how to [implement the scaled-dot product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras). **We shall now progress one step further into our journey by encapsulating the scaled-dot product attention into a multi-head attention mechanism, of which it is a core component. Our end goal remains the application of the complete model to Natural Language Processing** (NLP).\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "* The layers that form part of the multi-head attention mechanism.\n",
    "* How to implement the multi-head attention mechanism from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836645d",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "1. <font size='3ptx'><b><a href='#sect1'>Recap of the Transformer Architecture</a></b></font>\n",
    "    * <b><a href='#sect1_1'>The Transformer Multi-Head Attention</a></b>\n",
    "2. Implementing Multi-Head Attention From Scratch\n",
    "3. Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d37182",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Prerequisites</font>\n",
    "For this tutorial, we assume that you are already familiar with:\n",
    "* [The concept of attention](https://machinelearningmastery.com/what-is-attention/)\n",
    "* [The Transfomer attention mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism)\n",
    "* [The Transformer model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "* [The scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788219f",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Recap of the Transformer Architecture</font>\n",
    "<b><font size='3ptx'>Recall having seen that the Transformer architecture follows an encoder-decoder structure</font></b>\n",
    "\n",
    "The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step, to generate an output sequence ([image source: Attention is all you need](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "![Transformer articture](images/1.PNG)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In generating an output sequence, the Transformer does not rely on recurrence and convolutions.\n",
    "\n",
    "We had seen that the decoder part of the Transformer shares many similarities in its architecture with the encoder. <b>One of the core mechanisms that both the encoder and decoder share is the <font color='darkblue'>multi-head attention mechanism</font></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c90b",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <font color='darkgreen'>The Transformer Multi-Head Attention</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
