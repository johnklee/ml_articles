{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f9e1e3",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "<b><font size='3ptx'>In this tutorial, you will discover how to implement multi-head attention from scratch in TensorFlow and Keras.</font></b>\n",
    "\n",
    "([article source](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)) We have already familiarised ourselves with the theory behind the [**Transformer model**](https://machinelearningmastery.com/the-transformer-model/) and its [**attention mechanism**](https://machinelearningmastery.com/the-transformer-model/), and we have already started our journey of implementing a complete model by seeing how to [implement the scaled-dot product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras). **We shall now progress one step further into our journey by encapsulating the scaled-dot product attention into a multi-head attention mechanism, of which it is a core component. Our end goal remains the application of the complete model to Natural Language Processing** (NLP).\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "* The layers that form part of the multi-head attention mechanism.\n",
    "* How to implement the multi-head attention mechanism from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836645d",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "1. <font size='3ptx'><b><a href='#sect1'>Recap of the Transformer Architecture</a></b></font>\n",
    "    * <b><a href='#sect1_1'>The Transformer Multi-Head Attention</a></b>\n",
    "    * <b><a href='#sect1_2'>Implementing Multi-Head Attention From Scratch</a></b>\n",
    "2. <font size='3ptx'><b><a href='#sect2'>Testing Out the Code</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d37182",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Prerequisites</font>\n",
    "For this tutorial, we assume that you are already familiar with:\n",
    "* [The concept of attention](https://machinelearningmastery.com/what-is-attention/)\n",
    "* [The Transfomer attention mechanism](https://machinelearningmastery.com/the-transformer-attention-mechanism)\n",
    "* [The Transformer model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "* [The scaled dot-product attention](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788219f",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Recap of the Transformer Architecture</font>\n",
    "<b><font size='3ptx'>Recall having seen that the Transformer architecture follows an encoder-decoder structure</font></b>\n",
    "\n",
    "The encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step, to generate an output sequence ([image source: Attention is all you need](https://arxiv.org/abs/1706.03762)).\n",
    "\n",
    "![Transformer articture](images/1.PNG)\n",
    "\n",
    "<br/>\n",
    "\n",
    "In generating an output sequence, the Transformer does not rely on recurrence and convolutions.\n",
    "\n",
    "We had seen that the decoder part of the Transformer shares many similarities in its architecture with the encoder. <b>One of the core mechanisms that both the encoder and decoder share is the <font color='darkblue'>multi-head attention mechanism</font></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c90b",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <font color='darkgreen'>The Transformer Multi-Head Attention</font>\n",
    "Each multi-head attention block is made up of four consecutive levels:\n",
    "* **On the first level**, three linear (dense) layers that each receives the queries, keys or values. \n",
    "* **On the second level**, a scaled dot-product attention function. The operations performed on both first and second levels are repeated `h` times and performed in parallel, according to the number of heads composing the multi-head attention block. \n",
    "* **On the third level**, a concatenation operation that joins the outputs of the different heads.\n",
    "* **On the fourth level**, a final linear (dense) layer that produces the output. \n",
    "\n",
    "![Multi-Head Attention](images/2.PNG)\n",
    "\n",
    "[Recall](https://machinelearningmastery.com/the-transformer-attention-mechanism/) as well the important components that will serve as building blocks for our implementation of the multi-head attention:\n",
    "* The **queries**, **keys** and **values**: These are the inputs to each multi-head attention block. In the encoder stage, they each carry the same input sequence after this has been embedded and augmented by positional information. Similarly on the decoder side, the queries, keys and values fed into the first attention block represent the same target sequence, after this would have also been embedded and augmented by positional information. The second attention block of the decoder receives the encoder output in the form of keys and values, and the normalized output of the first decoder attention block as the queries. The dimensionality of the queries and keys is denoted by $d_k$, whereas the dimensionality of the values is denoted by $d_v$.\n",
    "* The **projection matrices**: When applied to the queries, keys and values, these projection matrices generate different subspace representations of each. Each attention head then works on one of these projected versions of the queries, keys and values. An additional projection matrix is also applied to the output of the multi-head attention block, after the outputs of each individual head would have been concatenated together. The projection matrices are learned during training.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Let’s now see how to implement the multi-head attention from scratch in TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4cfd9",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <font color='darkgreen'>Implementing Multi-Head Attention From Scratch</font>\n",
    "Let’s start by creating the class, <b><font color='blue'>MultiHeadAttention</font></b>, that inherits form the [**Layer**](https://keras.io/api/layers/base_layer/) base class in Keras, and initialize several instance attributes that we shall be working with (<font color='brown'>attribute descriptions may be found in the comments</font>):\n",
    "```python\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention \n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Here note that we have also created an instance of the <b><font color='blue'>DotProductAttention</font></b> class that we had implemented earlier, and assigned its output to the variable attention. Recall that we had implemented the <b><font color='blue'>DotProductAttention</font></b> class as follows:\n",
    "```python\n",
    "from tensorflow import matmul, math, cast, float32\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, we will be reshaping the linearly projected queries, keys and values in such a manner as to allow the attention heads to be computed in parallel. \n",
    "\n",
    "The queries, keys and values will be fed as input into the multi-head attention block having a shape of (<font color='brown'>batch size, sequence length, model dimensionality</font>), where <b>the `batch size` is a hyperparameter of the training process, the `sequence length` defines the maximum length of the input/output phrases, and the `model dimensionality` is the dimensionality of the outputs produced by all sub-layers of the model</b>. They are then passed through the respective dense layer to be linearly projected to a shape of `(batch size, sequence length, queries/keys/values dimensionality)`.\n",
    "\n",
    "The linearly projected queries, keys and values will be rearranged into `(batch size, number of heads, sequence length, depth)`, by first reshaping them into `(batch size, sequence length, number of heads, depth)` and then transposing the second and third dimensions. For this purpose, we will create the class method, <font color='blue'>reshape_tensor</font>, as follows:\n",
    "```python\n",
    "def reshape_tensor(self, x, heads, flag):\n",
    "    if flag:\n",
    "        # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "        x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "        x = transpose(x, perm=(0, 2, 1, 3))\n",
    "    else:\n",
    "        # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_model)\n",
    "        x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        x = reshape(x, shape=(shape(x)[0], shape(x)[1], -1))\n",
    "    return x\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The <font color='blue'>reshape_tensor</font> method receives the linearly projected queries, keys or values as input (<font color='brown'>while setting the flag to True</font>) to be rearranged as previously explained. Once the multi-head attention output has been generated, this is also fed into the same function (<font color='brown'>this time setting the flag to False</font>) to perform a reverse operation, effectively concatenating the results of all heads together. \n",
    "\n",
    "Hence, the next step is to feed the linearly projected queries, keys and values into the <font color='blue'>reshape_tensor</font> method to be rearranged, and then proceed to feed them into the scaled dot-product attention function. In order to do so, we will create another class method, <font color='blue'>call</font>, as follows:\n",
    "```python\n",
    "def call(self, queries, keys, values, mask=None):\n",
    "    # Rearrange the queries to be able to compute all heads in parallel\n",
    "    q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "    # Rearrange the keys to be able to compute all heads in parallel\n",
    "    k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "    # Rearrange the values to be able to compute all heads in parallel\n",
    "    v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "    # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "    o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "    # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "    ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Note that the <font color='blue'>reshape_tensor</font> method can also receive a mask (<font color='brown'>whose value defaults to None</font>) as input, in addition to the queries, keys and values. \n",
    "\n",
    "[Recall](https://machinelearningmastery.com/the-transformer-model/) that the Transformer model introduces a <b>look-ahead mask</b> to prevent the decoder from attending to succeeding words, such that the prediction for a particular word can only depend on known outputs for the words that come before it. Furthermore, since the word embeddings are zero-padded to a specific sequence length, a `padding mask` needs to be introduced too in order to prevent the zero values from being processed along with the input. These look-ahead and padding masks can be passed on to the scaled-dot product attention through the mask argument.\n",
    "\n",
    "Once we have generated the multi-head attention output from all the attention heads, the final steps are to concatenate back all outputs together into a tensor of shape, `(batch size, sequence length, values dimensionality)`, and passing the result through one final dense layer. For this purpose, we will be adding the next two lines of code to the <font color='blue'>call</font> method:\n",
    "```python\n",
    "...\n",
    "# Rearrange back the output into concatenated form\n",
    "output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "# Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "# Apply one final linear projection to the output to generate the multi-head attention\n",
    "# Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "return self.W_o(output)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Putting everything together, we have the following implementation of the multi-head attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42347602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389eb938",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Testing Out the Code</font>\n",
    "We will be working with the parameter values specified in the paper, [Attention Is All You Need, by Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762):\n",
    "\n",
    "```python\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "As for the sequence length, and the queries, keys and values, we will be working with dummy data for the time being until we arrive to the stage of [training the complete Transformer model](https://machinelearningmastery.com/training-the-transformer-model) in a separate tutorial, at which point we will be using actual sentences:\n",
    "\n",
    "```python\n",
    "...\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "<b>In the complete Transformer model, values for the sequence length, and the queries, keys and values will be obtained through a process of word tokenization and embedding</b>. We will be covering this in a separate tutorial. \n",
    "\n",
    "Returning back to our testing procedure, the next step is to create a new instance of the <b><font color='blue'>MultiHeadAttention</font></b> class, assigning its output to the multihead_attention variable:\n",
    "\n",
    "```python\n",
    "...\n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Since the <b><font color='blue'>MultiHeadAttention</font></b> class inherits from the [**Layer**](https://keras.io/api/layers/base_layer/) base class, the <font color='blue'>call()</font> method of the former will be automatically invoked by the magic \\_\\_call()__ method of the latter. The final step is to pass in the input arguments and printing the result:\n",
    "\n",
    "```python\n",
    "...\n",
    "print(multihead_attention(queries, keys, values))\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Tying everything together produces the following code listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36a0376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.31182265  0.19591606 -0.1745555  ... -0.0810058   0.3352888\n",
      "   -0.09868204]\n",
      "  [ 0.31025675  0.19259396 -0.17539161 ... -0.08174489  0.3322592\n",
      "   -0.09594787]\n",
      "  [ 0.31222063  0.1926596  -0.17362075 ... -0.08272956  0.3357062\n",
      "   -0.09639519]\n",
      "  [ 0.30701864  0.19324327 -0.1784897  ... -0.08069471  0.3368063\n",
      "   -0.09762746]\n",
      "  [ 0.3132949   0.19455978 -0.17680576 ... -0.08266836  0.33526513\n",
      "   -0.09668439]]\n",
      "\n",
      " [[ 0.30330384  0.31986487 -0.41438392 ... -0.18172678  0.38058582\n",
      "   -0.21451142]\n",
      "  [ 0.2987469   0.32107183 -0.4159746  ... -0.1853449   0.3806158\n",
      "   -0.2141388 ]\n",
      "  [ 0.3031524   0.31956658 -0.41565073 ... -0.18619305  0.3800994\n",
      "   -0.21355376]\n",
      "  [ 0.30263156  0.32314113 -0.41567734 ... -0.18400706  0.37728742\n",
      "   -0.21264756]\n",
      "  [ 0.30143106  0.319786   -0.41592628 ... -0.18569006  0.3804273\n",
      "   -0.21345107]]\n",
      "\n",
      " [[ 0.38233978  0.2469757  -0.2247107  ... -0.35052028  0.38104048\n",
      "   -0.20756163]\n",
      "  [ 0.38205683  0.24726364 -0.22410625 ... -0.35173076  0.38397768\n",
      "   -0.20645136]\n",
      "  [ 0.3841547   0.24818419 -0.22608115 ... -0.34932673  0.38151565\n",
      "   -0.2082776 ]\n",
      "  [ 0.38429803  0.24782309 -0.22152837 ... -0.35082823  0.3798393\n",
      "   -0.20643288]\n",
      "  [ 0.38528267  0.24785504 -0.22427496 ... -0.34887102  0.379811\n",
      "   -0.20623529]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.41801938  0.22418734 -0.27224252 ... -0.29897213  0.383297\n",
      "   -0.12252958]\n",
      "  [ 0.4180454   0.21725206 -0.27076146 ... -0.29694083  0.38683727\n",
      "   -0.12426764]\n",
      "  [ 0.42024738  0.22540872 -0.26992968 ... -0.2989318   0.38369608\n",
      "   -0.12416383]\n",
      "  [ 0.41511774  0.22644177 -0.27550918 ... -0.30035344  0.3820196\n",
      "   -0.12114803]\n",
      "  [ 0.41483414  0.23191497 -0.2772554  ... -0.30157816  0.38104203\n",
      "   -0.1182543 ]]\n",
      "\n",
      " [[ 0.35387927  0.22657068 -0.23416573 ... -0.28504685  0.3549208\n",
      "   -0.1383157 ]\n",
      "  [ 0.3547317   0.22896552 -0.23414855 ... -0.28420353  0.35496995\n",
      "   -0.1373832 ]\n",
      "  [ 0.3552403   0.22761948 -0.23265032 ... -0.28576848  0.3525528\n",
      "   -0.13713413]\n",
      "  [ 0.35518256  0.22906853 -0.23236963 ... -0.28602073  0.3532879\n",
      "   -0.13641825]\n",
      "  [ 0.35581467  0.2266352  -0.23450418 ... -0.28645486  0.35467345\n",
      "   -0.13845795]]\n",
      "\n",
      " [[ 0.41803735  0.3092435  -0.28229016 ... -0.2539584   0.31616896\n",
      "   -0.19465272]\n",
      "  [ 0.4180402   0.3061435  -0.28084314 ... -0.2541531   0.31771737\n",
      "   -0.19503102]\n",
      "  [ 0.41968817  0.3028212  -0.28218266 ... -0.25168094  0.3201986\n",
      "   -0.19379736]\n",
      "  [ 0.41981283  0.3038704  -0.28132847 ... -0.25125557  0.3202399\n",
      "   -0.19307493]\n",
      "  [ 0.42083803  0.30584535 -0.28219828 ... -0.25150505  0.32034937\n",
      "   -0.19430573]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "batch_size = 64  # Batch size from the training process\n",
    "\n",
    "queries = random.random((batch_size, input_seq_length, d_k))\n",
    "keys = random.random((batch_size, input_seq_length, d_k))\n",
    "values = random.random((batch_size, input_seq_length, d_v))\n",
    "\n",
    "multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "print(multihead_attention(queries, keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af06c2f",
   "metadata": {},
   "source": [
    "Running this code produces an output of shape, `(batch size, sequence length, model dimensionality)`. Note that you will likely see a different output due to the random initialization of the queries, keys and values, and the parameter values of the dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f215958e",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Further Reading</font>\n",
    "This section provides more resources on the topic if you are looking to go deeper.\n",
    "\n",
    "* **Books**\n",
    "    * [Advanced Deep Learning with Python, 2019.](https://www.amazon.com/Advanced-Deep-Learning-Python-next-generation/dp/178995617X)\n",
    "    * [Transformers for Natural Language Processing, 2021.](https://www.amazon.com/Transformers-Natural-Language-Processing-architectures/dp/1800565798)\n",
    "* **Papers**\n",
    "    * [Attention Is All You Need, 2017.](https://arxiv.org/abs/1706.03762)\n",
    "* **Others**\n",
    "    * [【機器學習2021】自注意力機制 (Self-attention) (上)](https://www.youtube.com/watch?v=hYdO9CscNes)\n",
    "    * [【機器學習2021】自注意力機制 (Self-attention) (下)](https://www.youtube.com/watch?v=gmsMY5kc-zw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
