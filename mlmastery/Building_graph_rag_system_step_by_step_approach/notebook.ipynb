{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef677d59-4071-43ff-8563-caae78673318",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://machinelearningmastery.com/building-graph-rag-system-step-by-step-approach/?fbclid=IwZXh0bgNhZW0CMTEAAR1la-9zv31c6hYrmyJ59_9ttnj6szllkChoiF7lhF5hD2QvGFiFRAURyxw_aem_aw-qnM-4W1WghDc9rZwcTg)) <b><font size='3ptx'>Graph RAG, Graph RAG, Graph RAG! This term has become the talk of the town, and you might have come across it as well.</font> But what exactly is Graph RAG, and what has made it so popular? In this article, we’ll explore the concept behind Graph RAG, why it’s needed, and, as a bonus, we’ll discuss how to implement it using LlamaIndex. Let’s get started!</b>\n",
    "\n",
    "<b>First, let’s address the shift from large language models (LLMs) to Retrieval-Augmented Generation (RAG) systems. <font color='darkred'>LLMs rely on static knowledge, which means they only use the data they were trained on. This limitation often makes them prone to hallucinations—generating incorrect or fabricated information</font>.</b>  To handle this, RAG systems were developed. Unlike LLMs, <b>RAG retrieves data in real-time from external knowledge bases, using this fresh context to generate more accurate and relevant responses. These traditional RAG systems work by using text embeddings to retrieve specific information. While powerful, they come with limitations</b>.\n",
    "\n",
    "If you’ve worked on RAG-related projects, you’ll probably relate to this: <b>the quality of the system’s response heavily depends on the clarity and specificity of the query. <font color='darkred'>But an even bigger challenge emerged — the inability to reason effectively across multiple documents</font></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddaa400-5e89-49da-b473-8f5420fb93ce",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Tradition RAG problem</font></b>\n",
    "Now, What does that mean? Let’s take an example. Imagine you’re asking the system:\n",
    "> “Who were the key contributors to the discovery of DNA’s double-helix structure, and what role did Rosalind Franklin play?”\n",
    "\n",
    "In a traditional RAG setup, the system might retrieve the following pieces of information:\n",
    "1. **Document 1:** “James Watson and Francis Crick proposed the double-helix structure in 1953.”\n",
    "2. **Document 2:** “Rosalind Franklin’s X-ray diffraction images were critical in identifying DNA’s helical structure.”\n",
    "3. **Document 3:** “Maurice Wilkins shared Franklin’s images with Watson and Crick, which contributed to their discovery.”\n",
    "\n",
    "The problem? **Traditional RAG systems treat these documents as <font color='darkblue'>independent units</font>.** They don’t connect the dots effectively, leading to fragmented responses like:\n",
    "> “Watson and Crick proposed the structure, and Franklin’s work was important.”\n",
    "\n",
    "This response lacks depth and misses key relationships between contributors. <b>Enter Graph RAG! By organizing the retrieved data as a graph, Graph RAG represents each document or fact as a node, and the relationships between them as edges.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6c0a7-93bf-4826-a8a2-2349287e30fe",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Basic of Graph RAG</font></b>\n",
    "Here’s how Graph RAG would handle the same query:\n",
    "* **Nodes:** Represent facts (e.g., “Watson and Crick proposed the structure,” “Franklin contributed critical X-ray images”).\n",
    "* **Edges:** Represent relationships (e.g., “Franklin’s images → shared by Wilkins → influenced Watson and Crick”).\n",
    "\n",
    "\n",
    "By reasoning across these interconnected nodes, Graph RAG can produce a complete and insightful response like:\n",
    "> “The discovery of DNA’s double-helix structure in 1953 was primarily led by James Watson and Francis Crick. However, this breakthrough heavily relied on Rosalind Franklin’s X-ray diffraction images, which were shared with them by Maurice Wilkins.”\n",
    "\n",
    "<b>This ability to combine information from multiple sources and answer broader, more complex questions is what makes Graph RAG so popular.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407b37af-7619-4cdd-814c-a7856e92fd2d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>The Graph RAG</font></b>\n",
    "<b><font size='3ptx'>We’ll now explore the Graph RAG pipeline, as presented in the paper “[From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)” by Microsoft Research.</font></b>\n",
    "\n",
    "![graph rag flow](images/1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834dde9-9f2f-4490-9021-68a0450e95de",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Pipeline</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9cc00-fe49-4d0f-8be6-c2cc07c95b23",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 1: Source Documents → Text Chunks</font></b>\n",
    "LLMs can handle only a limited amount of text at a time. To maintain accuracy and ensure that nothing important is missed, we will first <b>break down large documents into smaller, manageable “chunks” of text for processing</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c97e0f-a80c-4fcb-89a3-4d0834875552",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 2: Text Chunks → Element Instances</font></b>\n",
    "From each chunk of source text, we will <b>prompt the LLMs to identify graph nodes and edges</b>. For example, from a news article, the LLMs might detect that “NASA launched a spacecraft” and link “NASA” (`entity: node`) to “spacecraft” (`entity: node`) through “launched” (`relationship: edge`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba31eb-e4d8-4b10-b1d0-cbaed223a7a4",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 3: Element Instances → Element Summaries</font></b>\n",
    "<b>After identifying the elements, the next step is to summarize them into concise, meaningful descriptions using LLMs</b>. This process makes the data easier to understand. For example, for the node “NASA,” the summary could be: “NASA is a space agency responsible for space exploration missions.” For the edge connecting “NASA” and “spacecraft,” the summary might be: “NASA launched the spacecraft in 2023.” These summaries ensure the graph is both rich in detail and easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b95827-0982-4326-b7f8-d60453b64a29",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 4: Element Summaries → Graph Communities</font></b>\n",
    "<b>The graph created in the previous steps is often too large to analyze directly. To simplify it, the graph is divided into communities using specialized algorithms like [Leiden](https://en.wikipedia.org/wiki/Leiden_algorithm)</b>. These communities help identify clusters of closely related information. For example, one community might focus on “Space Exploration,” grouping nodes such as “NASA,” “Spacecraft,” and “Mars Rover.” Another might focus on “Environmental Science,” grouping nodes like “Climate Change,” “Carbon Emissions,” and “Sea Levels.” This step makes it easier to identify themes and connections within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd9418-9f3a-40dc-aad3-8e0a4d633ee2",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 5: Graph Communities → Community Summaries</font></b>\n",
    "<b>LLMs prioritize important details and fit them into a manageable size. Therefore, each community is summarized to give an overview of the information it contains</b>. For example: A community about “space exploration” might summarize key missions, discoveries, and organizations like NASA or SpaceX. These summaries are useful for answering general questions or exploring broad topics within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f225587-bee0-47b2-8973-1909d93293c5",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Step 6: Community Summaries → Community Answers → Global Answer</font></b>\n",
    "Finally, the community summaries are used to answer user queries. Here’s how:\n",
    "1. **Query the Data**: A user asks, “`What are the main impacts of climate change?`”\n",
    "2. **Community Analysis**: The AI reviews summaries from relevant communities.\n",
    "3. **Generate Partial Answers**: Each community provides partial answers, such as:\n",
    "   - “Rising sea levels threaten coastal cities.”\n",
    "   - “Disrupted agriculture due to unpredictable weather.”\n",
    "4. **Combine into a Global Answer**: These partial answers are combined into one comprehensive response\n",
    "   > “Climate change impacts include rising sea levels, disrupted agriculture, and an increased frequency of natural disasters.”\n",
    "\n",
    "This process ensures the final answer is detailed, accurate, and easy to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d46cb5-9e96-4e7a-8f68-0263b685e229",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Step-by-Step Implementation of GraphRAG with LlamaIndex</font></b>\n",
    "You can build your custom Python implementation or use frameworks like [**LangChain**](https://python.langchain.com/docs/introduction/) or [**LlamaIndex**](https://docs.llamaindex.ai/en/stable/). **For this article, we will use the LlamaIndex baseline code provided on their [**website**](https://docs.llamaindex.ai/en/stable/examples/cookbooks/GraphRAG_v1/); however, I will explain it in a beginner-friendly manner. Additionally, I encountered a parsing problem with the original code, which I will explain later along with how I solved it**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ad5c2-dd42-47d9-801b-7cfd66152a77",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 1: Install Dependencies</font></b>\n",
    "Install the required libraries for the pipeline:\n",
    "```shell\n",
    "$ pip install llama-index graspologic numpy==1.24.4 scipy==1.12.0\n",
    "```\n",
    "\n",
    "**<font color='orange'>Note</font>**: graspologic: Used for graph algorithms like Hierarchical Leiden for community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7788ed52-6dfa-4ad5-816e-5154c5d6ddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graspologic==3.3.0\n",
      "graspologic-native==1.2.1\n",
      "llama-index==0.12.3\n",
      "llama-index-agent-openai==0.4.0\n",
      "llama-index-cli==0.4.0\n",
      "llama-index-core==0.12.3\n",
      "llama-index-embeddings-openai==0.3.1\n",
      "llama-index-indices-managed-llama-cloud==0.6.3\n",
      "llama-index-legacy==0.9.48.post4\n",
      "llama-index-llms-openai==0.3.2\n",
      "llama-index-multi-modal-llms-openai==0.3.0\n",
      "llama-index-program-openai==0.3.1\n",
      "llama-index-question-gen-openai==0.3.0\n",
      "llama-index-readers-file==0.4.1\n",
      "llama-index-readers-llama-parse==0.4.0\n",
      "numpy==1.24.4\n",
      "scipy==1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -P '(llama-index|graspologic|numpy|scipy)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d92e1-a32f-4100-9252-1b9cf6ea41d9",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 2: Load and Preprocess Data</font></b>\n",
    "<b>Load sample news data, which will be chunked into smaller parts for easier processing</b>. For demonstration, we limit it to 50 samples. Each row (title and text) is converted into a Document object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf9e9b1-fe9f-4083-9f88-5fd5803f86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from llama_index.core import Document\n",
    "\n",
    "SOURCE_DOCS_PATH = os.path.expanduser('~/Github/bt_test_common/docs/')\n",
    "\n",
    "doc_files = [os.path.join(SOURCE_DOCS_PATH, fn) for fn in os.listdir(os.path.expanduser(SOURCE_DOCS_PATH)) if fn.endswith('.md')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32157364-775c-4a77-8c0a-986e81f1c4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/google/home/johnkclee/Github/bt_test_common/docs/contributing.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/profiles_hfp_facade.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/background_knowldge_bluetooth.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/utils_bds_broker.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/ReleaseSteps.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/wifi_utils_usages.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/bt_utils_usages.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/general_utils_usages.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/background_knowledge_mobly.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/utils_log_parser.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/code-of-conduct.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/utils_dialer_simulator.md',\n",
       " '/usr/local/google/home/johnkclee/Github/bt_test_common/docs/mc_utils_usage.md']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5083f4e4-5f1f-4b2e-a336-97561e85031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(file_path):\n",
    "    file_content = open(file_path, 'r').read()\n",
    "    return f'Source: {file_path}\\n{file_content}'\n",
    "    \n",
    "# Convert data into LlamaIndex Document objects\n",
    "documents = [\n",
    "    Document(text=read_doc(fn))\n",
    "    for fn in doc_files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f287a2f-3a9f-47e9-87a5-22a8c84ab414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='168ca918-a8a6-49aa-bf86-59c6bad5eb19', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Source: /usr/local/google/home/johnkclee/Github/bt_test_common/docs/contributing.md\\n# How to Contribute\\n\\nWe would love to accept your patches and contributions to this project.\\n\\n## Before you begin\\n\\n### Sign our Contributor License Agreement\\n\\nContributions to this project must be accompanied by a\\n[Contributor License Agreement](https://cla.developers.google.com/about) (CLA).\\nYou (or your employer) retain the copyright to your contribution; this simply\\ngives us permission to use and redistribute your contributions as part of the\\nproject.\\n\\nIf you or your current employer have already signed the Google CLA (even if it\\nwas for a different project), you probably don't need to do it again.\\n\\nVisit <https://cla.developers.google.com/> to see your current agreements or to\\nsign a new one.\\n\\n### Review our Community Guidelines\\n\\nThis project follows [Google's Open Source Community\\nGuidelines](https://opensource.google/conduct/).\\n\\n## Contribution process\\n\\n### Code Reviews\\n\\nAll submissions, including submissions by project members, require review. We\\nuse [GitHub pull requests](https://docs.github.com/articles/about-pull-requests)\\nfor this purpose.\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681f300-14d2-44a9-9d9c-4b33a9e06e89",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 3: Split Text into Nodes</font></b>\n",
    "Use [**SentenceSplitter**](https://docs.llamaindex.ai/en/v0.10.19/api/llama_index.core.node_parser.SentenceSplitter.html) to break down documents into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f146777-99ac-42ab-a845-94bae3e048d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398dff33-6163-48fe-aaa6-b497f1b80f31",
   "metadata": {},
   "source": [
    "<b><font color='orange'>Notes</font></b>: `chunk_overlap=20`: Ensures chunks overlap slightly to avoid missing information at the boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573258c8-049b-430c-ae8b-bbc705929038",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 4: Configure the LLM, Prompt, and GraphRAG Extractor</font></b>\n",
    "Set up the LLM (<font color='brown'>e.g., GPT-4</font>). This LLM will later analyze the chunks to extract entities and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4784260-7d4e-4cee-8958-86f3510f9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "a = load_dotenv(find_dotenv(os.path.expanduser('~/.env'))) # read local .env file\n",
    "llm = OpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e354e-706b-47cf-a59a-8ab22136f08e",
   "metadata": {},
   "source": [
    "<b>The <font color='blue'>GraphRAGExtractor</font> uses the above LLM, a prompt template to guide the extraction process, and a parsing function to process the LLM’s output into structured data.</b> Text chunks (<font color='brown'>called `nodes`</font>) are fed into the extractor. <b>For each chunk, the extractor sends the text to the LLM along with the prompt, which instructs the LLM to identify entities, their types, and their relationships</b>. The response is parsed by a function (**`parse_fn`**), which extracts the entities and relationships. \n",
    "\n",
    "These are then converted into <b><font color='blue'>EntityNode</font></b> objects (for entities) and <b><font color='blue'>Relation</font></b> objects (for relationships), with descriptions stored as metadata. <b>The extracted entities and relationships are saved into the text chunk’s metadata, ready for use in building knowledge graphs or performing queries</b>.\n",
    "\n",
    "<b><font color='orange'>Notes:</font></b>\n",
    "> The issue in the original implementation was that the **`parse_fn`** failed to extract entities and relationships from the LLM-generated response, resulting in empty outputs for parsed entities and relationships. This occurred due to overly complex and rigid regular expressions that did not align well with the LLM response’s actual structure, particularly regarding inconsistent formatting and line breaks in the output. To address this, I have simplified the parse_fn by replacing the original regex patterns with straightforward patterns designed to match the key-value structure of the LLM response more reliably. The updated part looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "304d85a7-e326-49af-9835-162441321964",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pattern = r'entity_name:\\s*(.+?)\\s*entity_type:\\s*(.+?)\\s*entity_description:\\s*(.+?)\\s*'\n",
    "relationship_pattern = r'source_entity:\\s*(.+?)\\s*target_entity:\\s*(.+?)\\s*relation:\\s*(.+?)\\s*relationship_description:\\s*(.+?)\\s*'\n",
    "\n",
    "def parse_fn(response_str: str) -> Any:\n",
    "    entities = re.findall(entity_pattern, response_str)\n",
    "    relationships = re.findall(relationship_pattern, response_str)\n",
    "    return entities, relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43066f-9051-47a5-bfad-5bc07d18a353",
   "metadata": {},
   "source": [
    "The prompt template and <b><font color='blue'>GraphRAGExtractor</font></b> class are kept as is, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65ba66be-3b99-4280-97b5-394574ff6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from typing import Any, List, Callable, Optional, Union, Dict\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core.async_utils import run_jobs\n",
    "from llama_index.core.indices.property_graph.utils import (\n",
    "    default_parse_triplets_fn,\n",
    ")\n",
    "from llama_index.core.graph_stores.types import (\n",
    "    EntityNode,\n",
    "    KG_NODES_KEY,\n",
    "    KG_RELATIONS_KEY,\n",
    "    Relation,\n",
    ")\n",
    "from llama_index.core.llms.llm import LLM\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.prompts.default_prompts import (\n",
    "    DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    ")\n",
    "from llama_index.core.schema import TransformComponent, BaseNode\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GraphRAGExtractor(TransformComponent):\n",
    "    \"\"\"Extract triples from a graph.\n",
    "\n",
    "    Uses an LLM and a simple prompt + output parsing to extract paths (i.e. triples) and entity, relation descriptions from text.\n",
    "\n",
    "    Args:\n",
    "        llm (LLM):\n",
    "            The language model to use.\n",
    "        extract_prompt (Union[str, PromptTemplate]):\n",
    "            The prompt to use for extracting triples.\n",
    "        parse_fn (callable):\n",
    "            A function to parse the output of the language model.\n",
    "        num_workers (int):\n",
    "            The number of workers to use for parallel processing.\n",
    "        max_paths_per_chunk (int):\n",
    "            The maximum number of paths to extract per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    llm: LLM\n",
    "    extract_prompt: PromptTemplate\n",
    "    parse_fn: Callable\n",
    "    num_workers: int\n",
    "    max_paths_per_chunk: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Optional[LLM] = None,\n",
    "        extract_prompt: Optional[Union[str, PromptTemplate]] = None,\n",
    "        parse_fn: Callable = default_parse_triplets_fn,\n",
    "        max_paths_per_chunk: int = 10,\n",
    "        num_workers: int = 4,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        from llama_index.core import Settings\n",
    "\n",
    "        if isinstance(extract_prompt, str):\n",
    "            extract_prompt = PromptTemplate(extract_prompt)\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm or Settings.llm,\n",
    "            extract_prompt=extract_prompt or DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,\n",
    "            parse_fn=parse_fn,\n",
    "            num_workers=num_workers,\n",
    "            max_paths_per_chunk=max_paths_per_chunk,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"GraphExtractor\"\n",
    "\n",
    "    def __call__(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes.\"\"\"\n",
    "        return asyncio.run(\n",
    "            self.acall(nodes, show_progress=show_progress, **kwargs)\n",
    "        )\n",
    "\n",
    "    async def _aextract(self, node: BaseNode) -> BaseNode:\n",
    "        \"\"\"Extract triples from a node.\"\"\"\n",
    "        assert hasattr(node, \"text\")\n",
    "\n",
    "        text = node.get_content(metadata_mode=\"llm\")\n",
    "        try:\n",
    "            llm_response = await self.llm.apredict(\n",
    "                self.extract_prompt,\n",
    "                text=text,\n",
    "                max_knowledge_triplets=self.max_paths_per_chunk,\n",
    "            )\n",
    "            entities, entities_relationship = self.parse_fn(llm_response)\n",
    "        except ValueError:\n",
    "            entities = []\n",
    "            entities_relationship = []\n",
    "\n",
    "        existing_nodes = node.metadata.pop(KG_NODES_KEY, [])\n",
    "        existing_relations = node.metadata.pop(KG_RELATIONS_KEY, [])\n",
    "        metadata = node.metadata.copy()\n",
    "        for entity, entity_type, description in entities:\n",
    "            metadata[\n",
    "                \"entity_description\"\n",
    "            ] = description  # Not used in the current implementation. But will be useful in future work.\n",
    "            entity_node = EntityNode(\n",
    "                name=entity, label=entity_type, properties=metadata\n",
    "            )\n",
    "            existing_nodes.append(entity_node)\n",
    "\n",
    "        metadata = node.metadata.copy()\n",
    "        for triple in entities_relationship:\n",
    "            subj, rel, obj, description = triple\n",
    "            subj_node = EntityNode(name=subj, properties=metadata)\n",
    "            obj_node = EntityNode(name=obj, properties=metadata)\n",
    "            metadata[\"relationship_description\"] = description\n",
    "            rel_node = Relation(\n",
    "                label=rel,\n",
    "                source_id=subj_node.id,\n",
    "                target_id=obj_node.id,\n",
    "                properties=metadata,\n",
    "            )\n",
    "\n",
    "            existing_nodes.extend([subj_node, obj_node])\n",
    "            existing_relations.append(rel_node)\n",
    "\n",
    "        node.metadata[KG_NODES_KEY] = existing_nodes\n",
    "        node.metadata[KG_RELATIONS_KEY] = existing_relations\n",
    "        return node\n",
    "\n",
    "    async def acall(\n",
    "        self, nodes: List[BaseNode], show_progress: bool = False, **kwargs: Any\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Extract triples from nodes async.\"\"\"\n",
    "        jobs = []\n",
    "        for node in nodes:\n",
    "            jobs.append(self._aextract(node))\n",
    "\n",
    "        return await run_jobs(\n",
    "            jobs,\n",
    "            workers=self.num_workers,\n",
    "            show_progress=show_progress,\n",
    "            desc=\"Extracting paths from text\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c319bd4-6cdf-40db-b781-c943a42b5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "-Goal-\n",
    "Given a text document, identify all entities and their entity types from the text and all relationships among the identified entities.\n",
    "Given the text, extract up to {max_knowledge_triplets} entity-relation triplets.\n",
    "\n",
    "-Steps-\n",
    "1. Identify all entities. For each identified entity, extract the following information:\n",
    "- entity_name: Name of the entity, capitalized\n",
    "- entity_type: Type of the entity\n",
    "- entity_description: Comprehensive description of the entity's attributes and activities\n",
    "Format each entity as (\"entity\")\n",
    "\n",
    "2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\n",
    "For each pair of related entities, extract the following information:\n",
    "- source_entity: name of the source entity, as identified in step 1\n",
    "- target_entity: name of the target entity, as identified in step 1\n",
    "- relation: relationship between source_entity and target_entity\n",
    "- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n",
    "\n",
    "Format each relationship as (\"relationship\")\n",
    "\n",
    "3. When finished, output.\n",
    "\n",
    "-Real Data-\n",
    "######################\n",
    "text: {text}\n",
    "######################\n",
    "output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8560eb6-7579-4afc-96ad-aecc261d931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor = GraphRAGExtractor(\n",
    "    llm=llm,\n",
    "    extract_prompt=KG_TRIPLET_EXTRACT_TMPL,\n",
    "    max_paths_per_chunk=2,\n",
    "    parse_fn=parse_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b9f6e-8913-4645-ab52-806430babb7b",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 5: Build the Graph Index</font></b>\n",
    "The <b><font color='blue'>PropertyGraphIndex</font></b> extracts entities and relationships from text using **kg_extractor** and stores them as nodes and edges in the <b><font color='blue'>GraphRAGStore</font></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09da9f0c-a6c0-45fc-8f6f-320be53a9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
    "import networkx as nx\n",
    "from graspologic.partition import hierarchical_leiden\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class GraphRAGStore(SimplePropertyGraphStore):\n",
    "    community_summary = {}\n",
    "    max_cluster_size = 5\n",
    "\n",
    "    def generate_community_summary(self, text):\n",
    "        \"\"\"Generate summary for a given text using an LLM.\"\"\"\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"system\",\n",
    "                content=(\n",
    "                    \"You are provided with a set of relationships from a knowledge graph, each represented as \"\n",
    "                    \"entity1->entity2->relation->relationship_description. Your task is to create a summary of these \"\n",
    "                    \"relationships. The summary should include the names of the entities involved and a concise synthesis \"\n",
    "                    \"of the relationship descriptions. The goal is to capture the most critical and relevant details that \"\n",
    "                    \"highlight the nature and significance of each relationship. Ensure that the summary is coherent and \"\n",
    "                    \"integrates the information in a way that emphasizes the key aspects of the relationships.\"\n",
    "                ),\n",
    "            ),\n",
    "            ChatMessage(role=\"user\", content=text),\n",
    "        ]\n",
    "        response = OpenAI().chat(messages)\n",
    "        clean_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return clean_response\n",
    "\n",
    "    def build_communities(self):\n",
    "        \"\"\"Builds communities from the graph and summarizes them.\"\"\"\n",
    "        nx_graph = self._create_nx_graph()\n",
    "        community_hierarchical_clusters = hierarchical_leiden(\n",
    "            nx_graph, max_cluster_size=self.max_cluster_size\n",
    "        )\n",
    "        community_info = self._collect_community_info(\n",
    "            nx_graph, community_hierarchical_clusters\n",
    "        )\n",
    "        self._summarize_communities(community_info)\n",
    "\n",
    "    def _create_nx_graph(self):\n",
    "        \"\"\"Converts internal graph representation to NetworkX graph.\"\"\"\n",
    "        nx_graph = nx.Graph()\n",
    "        for node in self.graph.nodes.values():\n",
    "            nx_graph.add_node(str(node))\n",
    "        for relation in self.graph.relations.values():\n",
    "            nx_graph.add_edge(\n",
    "                relation.source_id,\n",
    "                relation.target_id,\n",
    "                relationship=relation.label,\n",
    "                description=relation.properties[\"relationship_description\"],\n",
    "            )\n",
    "        return nx_graph\n",
    "\n",
    "    def _collect_community_info(self, nx_graph, clusters):\n",
    "        \"\"\"Collect detailed information for each node based on their community.\"\"\"\n",
    "        community_mapping = {item.node: item.cluster for item in clusters}\n",
    "        community_info = {}\n",
    "        for item in clusters:\n",
    "            cluster_id = item.cluster\n",
    "            node = item.node\n",
    "            if cluster_id not in community_info:\n",
    "                community_info[cluster_id] = []\n",
    "\n",
    "            for neighbor in nx_graph.neighbors(node):\n",
    "                if community_mapping[neighbor] == cluster_id:\n",
    "                    edge_data = nx_graph.get_edge_data(node, neighbor)\n",
    "                    if edge_data:\n",
    "                        detail = f\"{node} -> {neighbor} -> {edge_data['relationship']} -> {edge_data['description']}\"\n",
    "                        community_info[cluster_id].append(detail)\n",
    "        return community_info\n",
    "\n",
    "    def _summarize_communities(self, community_info):\n",
    "        \"\"\"Generate and store summaries for each community.\"\"\"\n",
    "        for community_id, details in community_info.items():\n",
    "            details_text = (\n",
    "                \"\\n\".join(details) + \".\"\n",
    "            )  # Ensure it ends with a period\n",
    "            self.community_summary[\n",
    "                community_id\n",
    "            ] = self.generate_community_summary(details_text)\n",
    "\n",
    "    def get_community_summaries(self):\n",
    "        \"\"\"Returns the community summaries, building them if not already done.\"\"\"\n",
    "        if not self.community_summary:\n",
    "            self.build_communities()\n",
    "        return self.community_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b868540f-a58c-433b-94d4-68d535c49062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain/chains/api/base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.requests import TextRequestsWrapper\n",
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PropertyGraphIndex\n\u001b[0;32m----> 3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mPropertyGraphIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphRAGStore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkg_extractors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkg_extractor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/llama_index/core/indices/property_graph/base.py:117\u001b[0m, in \u001b[0;36mPropertyGraphIndex.__init__\u001b[0;34m(self, nodes, llm, kg_extractors, property_graph_store, vector_store, use_async, embed_model, embed_kg_nodes, callback_manager, transformations, storage_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     storage_context\u001b[38;5;241m.\u001b[39mvector_stores[DEFAULT_VECTOR_STORE] \u001b[38;5;241m=\u001b[39m vector_store\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embed_kg_nodes \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    111\u001b[0m     storage_context\u001b[38;5;241m.\u001b[39mproperty_graph_store\u001b[38;5;241m.\u001b[39msupports_vector_queries\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m embed_kg_nodes\n\u001b[1;32m    113\u001b[0m ):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         resolve_embed_model(embed_model)\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_model\u001b[49m\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/llama_index/core/settings.py:64\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/llama_index/core/embeddings/utils.py:39\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings \u001b[38;5;28;01mas\u001b[39;00m LCEmbeddings\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     LCEmbeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/llama_index/core/bridge/langchain.py:66\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool, StructuredTool, Tool  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_message_histories\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     ChatMessageHistory,\n\u001b[1;32m     65\u001b[0m )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     67\u001b[0m     ChatAnyscale,\n\u001b[1;32m     68\u001b[0m     ChatOpenAI,\n\u001b[1;32m     69\u001b[0m     ChatFireworks,\n\u001b[1;32m     70\u001b[0m )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     HuggingFaceBgeEmbeddings,  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     HuggingFaceEmbeddings,  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     74\u001b[0m )  \u001b[38;5;66;03m# pants: no-infer-dep\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# LLMs\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/langchain_community/chat_models/__init__.py:286\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _module_lookup:\n\u001b[0;32m--> 286\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_module_lookup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/langchain_community/chat_models/anyscale.py:30\u001b[0m\n\u001b[1;32m     26\u001b[0m DEFAULT_API_BASE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.endpoints.anyscale.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m DEFAULT_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mChatAnyscale\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"`Anyscale` Chat large language models.\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;43;03m    See https://www.anyscale.com/ for information about Anyscale.\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;43;03m            chat = ChatAnyscale(model_name=\"meta-llama/Llama-2-7b-chat-hf\")\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@property\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m_llm_type\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[1;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[0;32m--> 224\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypes_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:577\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_pydantic_core_schema__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/main.py:671\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[0;34m(cls, source, handler)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:655\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[1;32m    652\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:924\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[0;32m--> 924\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:739\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    727\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    729\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    738\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 739\u001b[0m         \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m,\n\u001b[1;32m    740\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    743\u001b[0m         ],\n\u001b[1;32m    744\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    745\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    748\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:739\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    727\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    729\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    738\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 739\u001b[0m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    740\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    741\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    742\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    743\u001b[0m         ],\n\u001b[1;32m    744\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    745\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    746\u001b[0m     )\n\u001b[1;32m    747\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    748\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1115\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1110\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1111\u001b[0m     field_info: FieldInfo,\n\u001b[1;32m   1112\u001b[0m     decorators: DecoratorInfos,\n\u001b[1;32m   1113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[1;32m   1117\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1118\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1123\u001b[0m     )\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1308\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1304\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[1;32m   1305\u001b[0m             source_type, annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator\n\u001b[1;32m   1306\u001b[0m         )\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1308\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidators_from_decorators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2107\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[0;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[1;32m   2104\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[1;32m   2105\u001b[0m     )\n\u001b[0;32m-> 2107\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[1;32m   2109\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m CoreMetadataHandler(schema)\u001b[38;5;241m.\u001b[39mmetadata\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:83\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 83\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2091\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2090\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[0;32m-> 2091\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2093\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
      "File \u001b[0;32m~/Github/ml_articles/env/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2447\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[0;34m(tp, schema)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[1;32m   2446\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2447\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m   2448\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2450\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2451\u001b[0m         )\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[1;32m   2454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_annotated(tp):\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "from llama_index.core import PropertyGraphIndex\n",
    "\n",
    "index = PropertyGraphIndex(\n",
    "    nodes=nodes,\n",
    "    property_graph_store=GraphRAGStore(),\n",
    "    kg_extractors=[kg_extractor],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96098aa5-9e0b-4694-8454-c1b1431c50e9",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 6: Detect Communities and Summarize</font></b>\n",
    "Use [**graspologic**](https://github.com/graspologic-org/graspologic)’s [**Hierarchical Leiden algorithm**](https://en.wikipedia.org/wiki/Leiden_algorithm) to detect communities and generate summaries. **Communities are groups of nodes** (<font color='brown'>entities</font>) **that are densely connected internally but sparsely connected to other groups. This algorithm maximizes a metric called modularity, which measures the quality of dividing a graph into communities**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e13adbb9-6b09-444e-8933-3a8e14835368",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mproperty_graph_store\u001b[38;5;241m.\u001b[39mbuild_communities()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "index.property_graph_store.build_communities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987c054-a61e-4905-a2a9-45930b6ba6e6",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Step 7: Query the Graph</font></b>\n",
    "<font size='3ptx'><b>Initialize the <font color='blue'>GraphRAGQueryEngine</font> to query the processed data.</b></font>\n",
    "\n",
    "When a query is submitted, the engine retrieves relevant community summaries from the <b><font color='blue'>GraphRAGStore</font></b>. For each summary, it uses the LLM to generate a specific answer contextualized to the query via the `generate_answer_from_summary` method. These partial answers are then synthesized into a coherent final response using the `aggregate_answers` method, where the LLM combines multiple perspectives into a concise output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17a94a9b-3467-4e77-8407-223da360099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from llama_index.core.llms import LLM\n",
    "class GraphRAGQueryEngine(CustomQueryEngine):\n",
    "    graph_store: GraphRAGStore\n",
    "    llm: LLM\n",
    "\n",
    "    def custom_query(self, query_str: str) -> str:\n",
    "        \"\"\"Process all community summaries to generate answers to a specific query.\"\"\"\n",
    "        community_summaries = self.graph_store.get_community_summaries()\n",
    "        community_answers = [\n",
    "            self.generate_answer_from_summary(community_summary, query_str)\n",
    "            for _, community_summary in community_summaries.items()\n",
    "        ]\n",
    "\n",
    "        final_answer = self.aggregate_answers(community_answers)\n",
    "        return final_answer\n",
    "\n",
    "    def generate_answer_from_summary(self, community_summary, query):\n",
    "        \"\"\"Generate an answer from a community summary based on a given query using LLM.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the community summary: {community_summary}, \"\n",
    "            f\"how would you answer the following query? Query: {query}\"\n",
    "        )\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=\"I need an answer based on the above information.\",\n",
    "            ),\n",
    "        ]\n",
    "        response = self.llm.chat(messages)\n",
    "        cleaned_response = re.sub(r\"^assistant:\\s*\", \"\", str(response)).strip()\n",
    "        return cleaned_response\n",
    "\n",
    "    def aggregate_answers(self, community_answers):\n",
    "        \"\"\"Aggregate individual community answers into a final, coherent response.\"\"\"\n",
    "        # intermediate_text = \" \".join(community_answers)\n",
    "        prompt = \"Combine the following intermediate answers into a final, concise response.\"\n",
    "        messages = [\n",
    "            ChatMessage(role=\"system\", content=prompt),\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=f\"Intermediate answers: {community_answers}\",\n",
    "            ),\n",
    "        ]\n",
    "        final_response = self.llm.chat(messages)\n",
    "        cleaned_final_response = re.sub(\n",
    "            r\"^assistant:\\s*\", \"\", str(final_response)\n",
    "        ).strip()\n",
    "        return cleaned_final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5e5604a-9f0c-4291-add7-2161bf4e3664",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m GraphRAGQueryEngine(\n\u001b[0;32m----> 2\u001b[0m     graph_store\u001b[38;5;241m=\u001b[39m\u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39mproperty_graph_store, llm\u001b[38;5;241m=\u001b[39mllm\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are news related to financial sector?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "query_engine = GraphRAGQueryEngine(\n",
    "    graph_store=index.property_graph_store, llm=llm\n",
    ")\n",
    "response = query_engine.query(\"What are news related to financial sector?\")\n",
    "display(Markdown(f\"{response.response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeaa7c-259f-4c44-a6d9-a3f86d421467",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Wrapping Up</font></b>\n",
    "That’s all! I hope you enjoyed reading this article. <font color='green'><b>There’s no doubt that Graph RAG enables you to answer both specific factual and complex abstract questions by understanding the relationships and structures within your data</b></font>. However, <font color='darkred'><b>it’s still in its early stages and has limitations, particularly in terms of token utilization, which is significantly higher than traditional RAG</b></font>. Nevertheless, it’s an important development, and I personally look forward to seeing what’s next. If you have any questions or suggestions, feel free to share them in the comments section below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
