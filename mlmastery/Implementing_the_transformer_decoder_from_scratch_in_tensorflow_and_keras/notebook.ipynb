{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1909d41d",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras/)) <b><font size='3ptx'>There are many similarities between the Transformer encoder and decoder, such as in their implementation of multi-head attention, layer normalization and a fully connected feed-forward network as their final sub-layer.</font></b>\n",
    "\n",
    "Having implemented the [Transformer encoder](https://github.com/johnklee/ml_articles/blob/master/mlmastery/Implementing_the_transformer_encoder_from_scratch_in_tensorflow_and_keras/notebook.ipynb), we will now proceed to apply our knowledge in implementing the Transformer decoder, as a further step towards implementing the complete Transformer model.\n",
    "\n",
    "In this tutorial, you will discover how to implement the Transformer decoder from scratch in TensorFlow and Keras. After completing this tutorial, you will know:\n",
    "* The layers that form part of the Transformer decoder.\n",
    "* How to implement the Transformer decoder from scratch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ae3ca",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "* [**Recap of the Transformer Architecture**](#sect1)\n",
    "* [**Implementing the Transformer Decoder From Scratch**](#sect2)\n",
    "    * The Decoder Layer\n",
    "    * The Transformer Decoder\n",
    "* Testing Out the Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00adf7",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Prerequisites</font>\n",
    "For this tutorial, we assume that you are already familiar with:\n",
    "* [The Transformer model](https://machinelearningmastery.com/the-transformer-model/)\n",
    "* [The scaled dot-product attention](https://machinelearningmastery.com/?p=13364&preview=true)\n",
    "* [The multi-head attention](https://machinelearningmastery.com/?p=13351&preview=true)\n",
    "* [The Transformer positional encoding](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)\n",
    "* [The Transformer encoder](https://machinelearningmastery.com/?p=13389&preview=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f9a29",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Recap of the Transformer Architecture</font>\n",
    "[Recall](https://machinelearningmastery.com/the-transformer-model/) having seen that the Transformer architecture follows an encoder-decoder structure: the encoder, on the left-hand side, is tasked with mapping an input sequence to a sequence of continuous representations; **the decoder, on the right-hand side, receives the output of the encoder together with the decoder output at the previous time step, to generate an output sequence**.\n",
    "\n",
    "![Transformer decoder in arch](images/1.PNG)\n",
    "\n",
    "In generating an output sequence, the Transformer does not rely on recurrence and convolutions.\n",
    "\n",
    "We had seen that the decoder part of the Transformer shares many similarities in its architecture with the encoder. This tutorial will be exploring these similarities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b30e5",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>The Transformer Decoder</font>\n",
    "Similar to the [Transformer encoder](https://github.com/johnklee/ml_articles/blob/master/mlmastery/Implementing_the_transformer_encoder_from_scratch_in_tensorflow_and_keras/notebook.ipynb), the Transformer decoder also consists of a stack of identical layers. The Transformer decoder, however, implements an additional multi-head attention block, for a total of three main sub-layers:\n",
    "* The first sub-layer comprises a multi-head attention mechanism that receives the queries, keys and values as inputs.\n",
    "* The second sub-layer comprises a second multi-head attention mechanism. \n",
    "* The third sub-layer comprises a fully-connected feed-forward network. \n",
    "\n",
    "![Transformer decoder in arch](images/1.PNG)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Each one of these three sub-layers is also followed by layer normalisation, where the input to the layer normalization step is its corresponding sub-layer input (<font color='brown'>through a residual connection</font>) and output. \n",
    "\n",
    "On the decoder side, the queries, keys and values that are fed into the first multi-head attention block also represent the same input sequence. However, this time round, it is the target sequence that is embedded and augmented with positional information before being supplied to the decoder. The second multi-head attention block, on the other hand, receives the encoder output in the form of keys and values, and the normalized output of the first decoder attention block as the queries. In both cases, the dimensionality of the queries and keys remains equal to $d_{k}$, whereas the dimensionality of the values remains equal to $d_{v}$.\n",
    "\n",
    "Vaswani et al. introduce regularization into the model on the decoder side too, by applying dropout to the output of each sub-layer (before the layer normalization step), as well as to the positional encodings before these are fed into the decoder. \n",
    "\n",
    "Letâ€™s now see how to implement the Transformer decoder from scratch in TensorFlow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5b840e",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Implementing the Transformer Decoder From Scratch</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c66d5",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>The Decoder Layer</font>\n",
    "Since we have already implemented the required sub-layers when we covered the [implementation of the Transformer encoder](https://github.com/johnklee/ml_articles/blob/master/mlmastery/Implementing_the_transformer_encoder_from_scratch_in_tensorflow_and_keras/notebook.ipynb), we will create a class for the decoder layer that makes use of these sub-layers straight away:\n",
    "```python\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from encoder import AddNormalization, FeedForward\n",
    "\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Notice here that since my code for the different sub-layers had been saved into several Python scripts (<font color='brown'>namely,</font> <font color='olive'>multihead_attention.py</font> <font color='brown'>and</font> <font color='olive'>encoder.py</font>), it was necessary to import them to be able to use the required classes. \n",
    "\n",
    "As we had done for the Transformer encoder, we will proceed to create the class method, <font color='blue'>call()</font>, that implements all of the decoder sub-layers:\n",
    "```python\n",
    "...\n",
    "def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "    # Multi-head attention layer\n",
    "    multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Add in a dropout layer\n",
    "    multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    "\n",
    "    # Followed by an Add & Norm layer\n",
    "    addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Followed by another multi-head attention layer\n",
    "    multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    "\n",
    "    # Add in another dropout layer\n",
    "    multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    "\n",
    "    # Followed by another Add & Norm layer\n",
    "    addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    "\n",
    "    # Followed by a fully connected layer\n",
    "    feedforward_output = self.feed_forward(addnorm_output2)\n",
    "    # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "    # Add in another dropout layer\n",
    "    feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    "\n",
    "    # Followed by another Add & Norm layer\n",
    "    return self.add_norm3(addnorm_output2, feedforward_output)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The multi-head attention sub-layers can also receive a padding mask or a look-ahead mask. As a brief reminder of what we had said in a [previous tutorial](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras), <b>the padding mask is necessary to suppress the zero padding in the input sequence from being processed along with the actual input values</b>. The look-ahead mask prevents the decoder from attending to succeeding words, such that the prediction for a particular word can only depend on known outputs for the words that come before it.\n",
    "\n",
    "The same <font color='blue'>call()</font> class method can also receive a training flag to only apply the [**Dropout**](https://keras.io/api/layers/regularization_layers/dropout/) layers during training, when the value of this flag is set to `True`.\n",
    "\n",
    "We will be creating the following <b><font color='blue'>Decoder</font></b> class to implement the Transformer decoder:\n",
    "```python\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)\n",
    "        ...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "As in the Transformer encoder, the input to the first multi-head attention block on the decoder side receives the input sequence after this would have undergone a process of word embedding and positional encoding. For this purpose, an instance of the <b><font color='blue'>PositionEmbeddingFixedWeights</font></b> class (<font color='brown'>covered in this tutorial</font>) is initialized and its output assigned to the `pos_encoding` variable.\n",
    "\n",
    "The final step is to create a class method, <font color='blue'>call()</font>, that applies word embedding and positional encoding to the input sequence and feeds the result, together with the encoder output, to  decoder layers:\n",
    "```python\n",
    "...\n",
    "def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "    # Generate the positional encoding\n",
    "    pos_encoding_output = self.pos_encoding(output_target)\n",
    "    # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    "\n",
    "    # Add in a dropout layer\n",
    "    x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "    # Pass on the positional encoded values to each encoder layer\n",
    "    for i, layer in enumerate(self.decoder_layer):\n",
    "        x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    "\n",
    "    return x\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "The code listing for the full Transformer decoder is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de67490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "from multihead_attention import MultiHeadAttention\n",
    "from positional_encoding import PositionEmbeddingFixedWeights\n",
    "from encoder import AddNormalization, FeedForward\n",
    " \n",
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    " \n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
    " \n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Followed by another multi-head attention layer\n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
    " \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)\n",
    " \n",
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    " \n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(output_target)\n",
    "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    " \n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00df7e8",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Testing Out the Code</font>\n",
    "We will be working with the parameter values specified in the paper, [Attention Is All You Need, by Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762):\n",
    "```python\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "As for the input sequence we will be working with dummy data for the time being until we arrive to the stage of training the complete Transformer model in a separate tutorial, at which point we will be using actual sentences:\n",
    "\n",
    "```python\n",
    "...\n",
    "dec_vocab_size = 20 # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "...\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Next, we will create a new instance of the <b><font color='blue'>Decoder</font></b> class, assigning its to `decoder` variable, and subsequently passing in the input arguments and printing the result. We will be setting the padding and look-ahead masks to None for the time being, but we shall return to these when we implement the complete Transformer model:\n",
    "```python\n",
    "...\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(decoder(input_seq, enc_output, None, True)\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "Tying everything together produces the following code listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cce06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 03:08:53.604258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-21 03:08:53.604337: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-21 03:08:53.604373: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (johnkclee.c.googlers.com): /proc/driver/nvidia/version does not exist\n",
      "2022-10-21 03:08:53.615576: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.8248985   0.2964239   0.7608083  ...  0.17464584  0.23421288\n",
      "    0.26576504]\n",
      "  [-0.77121806  0.2589691   0.8694546  ...  0.22020744  0.25140268\n",
      "    0.30782634]\n",
      "  [-0.7686267   0.14425175  0.8768898  ...  0.21052054  0.27436918\n",
      "    0.34159267]\n",
      "  [-0.83794403  0.05393364  0.79679465 ...  0.19523147  0.28727317\n",
      "    0.3666878 ]\n",
      "  [-0.9168807   0.09301145  0.7091867  ...  0.19346212  0.26375723\n",
      "    0.365894  ]]\n",
      "\n",
      " [[-0.83108205  0.17180996  0.89795715 ...  0.02750048  0.24393825\n",
      "   -0.19888619]\n",
      "  [-0.76181316  0.1321857   1.0282778  ...  0.06248236  0.273029\n",
      "   -0.15914586]\n",
      "  [-0.7523107  -0.00998755  1.054501   ...  0.06388298  0.31676665\n",
      "   -0.11417438]\n",
      "  [-0.82676876 -0.10075451  0.97691804 ...  0.03817829  0.33013073\n",
      "   -0.09794228]\n",
      "  [-0.907696   -0.04062966  0.9192459  ...  0.02174829  0.30881098\n",
      "   -0.11598752]]\n",
      "\n",
      " [[-0.8936668   0.09203106  0.3527665  ... -0.40788966  0.42008328\n",
      "   -0.29435515]\n",
      "  [-0.84474474  0.04377091  0.48532057 ... -0.35225812  0.42709574\n",
      "   -0.24521147]\n",
      "  [-0.856007   -0.07390352  0.5056355  ... -0.34303024  0.44144526\n",
      "   -0.2004536 ]\n",
      "  [-0.9250902  -0.12624598  0.42718646 ... -0.3664929   0.45865732\n",
      "   -0.17294113]\n",
      "  [-0.999649   -0.05093318  0.3627753  ... -0.38281068  0.45410523\n",
      "   -0.17043428]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.7453097   0.14227352  0.88463205 ...  0.01802471  0.37973928\n",
      "    0.18418127]\n",
      "  [-0.6770928   0.08269498  1.0043992  ...  0.06992482  0.41444114\n",
      "    0.2223313 ]\n",
      "  [-0.67247057 -0.0446007   1.0239288  ...  0.07423656  0.4613195\n",
      "    0.26342306]\n",
      "  [-0.7300995  -0.09781528  0.9373224  ...  0.05359143  0.4712013\n",
      "    0.28934425]\n",
      "  [-0.7918888  -0.01129018  0.8454872  ...  0.05537188  0.45249233\n",
      "    0.291517  ]]\n",
      "\n",
      " [[-0.8430782  -0.09971751  0.8800024  ... -0.3028847   0.16915774\n",
      "   -0.28784302]\n",
      "  [-0.77795094 -0.15087539  0.9957356  ... -0.27322522  0.18236165\n",
      "   -0.23893839]\n",
      "  [-0.77423435 -0.25917017  1.011599   ... -0.2651872   0.20359811\n",
      "   -0.17422545]\n",
      "  [-0.8640216  -0.3254342   0.9372177  ... -0.2769565   0.21535689\n",
      "   -0.12457742]\n",
      "  [-0.9617147  -0.2526049   0.87105316 ... -0.274618    0.22206451\n",
      "   -0.11042383]]\n",
      "\n",
      " [[-0.9330709  -0.11479038  0.7027823  ... -0.00674036  0.09731122\n",
      "   -0.24437916]\n",
      "  [-0.8714414  -0.1502938   0.83183426 ...  0.03921399  0.10768803\n",
      "   -0.19190261]\n",
      "  [-0.8581054  -0.26620427  0.8498417  ...  0.06324319  0.12907745\n",
      "   -0.12595321]\n",
      "  [-0.93474954 -0.3423767   0.77618355 ...  0.04632697  0.14610896\n",
      "   -0.07839464]\n",
      "  [-1.0266057  -0.27441046  0.7082685  ...  0.0339808   0.15884203\n",
      "   -0.06737792]]], shape=(64, 5, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "dec_vocab_size = 20  # Vocabulary size for the decoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 512  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the decoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "enc_output = random.random((batch_size, input_seq_length, d_model))\n",
    "\n",
    "decoder = Decoder(dec_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(decoder(input_seq, enc_output, None, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a43f66",
   "metadata": {},
   "source": [
    "Running this code produces an output of shape, `(batch size, sequence length, model dimensionality)`. Note that you will likely see a different output due to the random initialization of the input sequence, and the parameter values of the Dense layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc5d0e",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Further Reading</font>\n",
    "This section provides more resources on the topic if you are looking to go deeper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
