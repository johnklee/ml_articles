{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01adb24e",
   "metadata": {},
   "source": [
    "## <font color='darkblue'><b>Preface</b></font>\n",
    "([article source](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)) <font size='3ptx'><b>Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time, and the task is to predict a category for the sequence</b></font>.\n",
    "\n",
    "This problem is difficult because the sequences can vary in length, comprise a very large vocabulary of input symbols, and may require the model to learn the long-term context or dependencies between symbols in the input sequence.\n",
    "\n",
    "<b>In this post, you will discover how you can develop LSTM recurrent neural network models for sequence classification problems in Python using the [Keras deep learning library](https://keras.io/getting_started/).</b>\n",
    "\n",
    "After reading this post, you will know:\n",
    "* How to develop an LSTM model for a sequence classification problem\n",
    "* How to reduce overfitting in your LSTM models through the use of dropout\n",
    "* How to combine LSTM models with Convolutional Neural Networks that excel at learning spatial relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9afa57",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Problem Description</font>\n",
    "<b><font size='3ptx'>The problem that you will use to demonstrate sequence learning in this tutorial is the [IMDB movie review sentiment classification problem](http://ai.stanford.edu/~amaas/data/sentiment/)</font>. Each movie review is a variable sequence of words, and the sentiment of each movie review must be classified</b>.\n",
    "\n",
    "The Large Movie Review Dataset (<font color='brown'>often referred to as the IMDB dataset</font>) contains 25,000 highly polar movie reviews (<font color='brown'>good or bad</font>) for training and the same amount again for testing. <b>The problem is to determine whether a given movie review has a positive or negative sentiment</b>.\n",
    "\n",
    "The data was collected by [**Stanford researchers and used in a 2011 paper**](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) where a 50/50 split of the data was used for training and testing. <b>An accuracy of 88.89% was achieved</b>.\n",
    "\n",
    "Keras provides built-in access to the IMDB dataset. The <font color='blue'>imdb.load_data()</font> function allows you to load the dataset in a format ready for use in neural networks and deep learning models.\n",
    "\n",
    "<b>The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfe3b6",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Word Embedding</font></b>\n",
    "<b>You will map each movie review into a real vector domain, a popular technique when working with text—called word embedding</b>. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where <b>the similarity between words in terms of meaning translates to closeness in the vector space</b>.\n",
    "\n",
    "Keras provides a convenient way to <b>convert positive integer representations of words into a word embedding by an Embedding layer</b>.\n",
    "\n",
    "You will <b>map each word onto a 32-length real valued vector</b>. You will also <b>limit the total number of words that you are interested in modeling to the 5000 most frequent words and zero out the rest</b>. Finally, the sequence length (<font color='brown'>number of words</font>) in each review varies, so you will <b>constrain each review to be 500 words</b>, truncating long reviews and padding the shorter reviews with zero values.\n",
    "\n",
    "Now that you have defined your problem and how the data will be prepared and modeled, you are ready to develop an LSTM model to classify the sentiment of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07aa8c",
   "metadata": {},
   "source": [
    "<a id='lstm'></a>\n",
    "## <font color='darkblue'>LSTM models</font>\n",
    "* <b><font size='3ptx'><a href='#lstm_1'>Simple LSTM for Sequence Classification</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#lstm_2'>LSTM for Sequence Classification with Dropout</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#lstm_3'>Bidirectional LSTM for Sequence Classification</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#lstm_4'>LSTM and Convolutional Neural Network for Sequence Classification</a></font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe333b",
   "metadata": {},
   "source": [
    "<a id='lstm_1'></a>\n",
    "### <font color='darkgreen'>Simple LSTM for Sequence Classification</font>\n",
    "<b><font size='3ptx'>You can quickly develop a small LSTM for the IMDB problem and achieve good accuracy.</font></b>\n",
    "\n",
    "Let’s start by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure you can easily reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "925b1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c982ac1",
   "metadata": {},
   "source": [
    "You need to load the IMDB dataset. <b>You are constraining the dataset to the top 5,000 words. You will also split the dataset into train (50%) and test (50%) sets</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2354130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e7118",
   "metadata": {},
   "source": [
    "Next, you need to <b>truncate and pad the input sequences, so they are all the same length for modeling</b>. The model will learn that the zero values carry no information. <b>The sequences are not the same length in terms of content, but same-length vectors are required to perform the computation in Keras</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22391b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97d4b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train=(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# 25,000 records of movie reviews; each review has at most 500 words.\n",
    "print(f'Shape of X_train={X_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12ffd7",
   "metadata": {},
   "source": [
    "You can now define, compile and fit your LSTM model.\n",
    "\n",
    "The first layer is the Embedded layer that uses 32-length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (<font color='brown'>smart neurons</font>). Finally, because this is a classification problem, you will use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (<font color='brown'>good and bad</font>) in the problem.\n",
    "\n",
    "Because it is a binary classification problem, log loss is used as the loss function (<font color='brown'>[binary_crossentropy](https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-function) in Keras</font>). The efficient ADAM optimization algorithm is used. The model is fit for only two epochs because it quickly overfits the problem. <b>A large batch size of 64 reviews is used to space out weight updates</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba6aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 15:00:14.090798: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-08-24 15:00:14.090852: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-24 15:00:14.090897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2023-08-24 15:00:14.091183: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3205e22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 346s 881ms/step - loss: 0.5430 - accuracy: 0.7354 - val_loss: 0.4528 - val_accuracy: 0.7893\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 343s 877ms/step - loss: 0.3209 - accuracy: 0.8696 - val_loss: 0.3412 - val_accuracy: 0.8681\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 347s 887ms/step - loss: 0.3147 - accuracy: 0.8729 - val_loss: 0.3138 - val_accuracy: 0.8737\n",
      "CPU times: user 16min 59s, sys: 28min 8s, total: 45min 7s\n",
      "Wall time: 17min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4c9e286d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5d752",
   "metadata": {},
   "source": [
    "Once fit, you can estimate the performance of the model on unseen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db843ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.37%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6f424",
   "metadata": {},
   "source": [
    "You can see that this simple LSTM with little tuning achieves near state-of-the-art results on the IMDB problem. Importantly, this is a template that you can use to apply LSTM networks to your own sequence classification problems.\n",
    "\n",
    "Now, let’s look at some extensions of this simple model that you may also want to bring to your own problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5f001",
   "metadata": {},
   "source": [
    "<a id='lstm_2'></a>\n",
    "### <font color='darkgreen'>LSTM for Sequence Classification with Dropout</font>\n",
    "<b><font size='3ptx'>Recurrent neural networks like LSTM generally have the problem of overfitting.</font></b>\n",
    "\n",
    "[**Dropout**](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) can be applied between layers using the Dropout Keras layer. You can do this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers. For example:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "The full code listing example above with the addition of Dropout layers is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d5a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_with_dropout():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(LSTM(100))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350a48e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_with_dropout = get_lstm_model_with_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20bad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 251s 635ms/step - loss: 0.4827 - accuracy: 0.7683\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 261s 668ms/step - loss: 0.2904 - accuracy: 0.8855\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 258s 659ms/step - loss: 0.2451 - accuracy: 0.9044\n",
      "CPU times: user 13min 49s, sys: 21min 31s, total: 35min 20s\n",
      "Wall time: 12min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4cd69a6a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_with_dropout.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc25b4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.37%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988630a6",
   "metadata": {},
   "source": [
    "You can see dropout having the desired impact on training with a slightly slower trend in convergence and, in this case, a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (<font color='brown'>try it and see</font>).\n",
    "\n",
    "Alternately, dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
    "\n",
    "Keras provides this capability with parameters on the LSTM layer, <b>the <font color='violet'>dropout</font> for configuring the input dropout, and <font color='violet'>recurrent_dropout</font> for configuring the recurrent dropout</b>. For example, you can modify the first example to add dropout to the input and recurrent connections as follows:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "The full code listing with more precise LSTM dropout is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ae560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_with_lstm_dropout(dropout=0.2, recurrent_dropout=0.2):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "  model.add(LSTM(100, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28783a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_with_lstm_dropout = get_lstm_model_with_lstm_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4798d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 320s 811ms/step - loss: 0.4534 - accuracy: 0.7773\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 314s 804ms/step - loss: 0.2879 - accuracy: 0.8843\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 327s 836ms/step - loss: 0.2502 - accuracy: 0.9011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4c9f61fa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_lstm_dropout.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "832871fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.37%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a750e28",
   "metadata": {},
   "source": [
    "<b>You can see that the LSTM-specific dropout has a more pronounced effect on the convergence of the network than the layer-wise dropout</b>. Like above, the number of epochs was kept constant and could be increased to see if the skill of the model could be further lifted.\n",
    "\n",
    "Dropout is a powerful technique for combating overfitting in your LSTM models, and it is a good idea to try both methods. Still, you may get better results with the gate-specific dropout provided in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44430202",
   "metadata": {},
   "source": [
    "<a id='lstm_3'></a>\n",
    "### <font color='darkgreen'><b>Bidirectional LSTM for Sequence Classification</b></font> ([back](#lstm))\n",
    "Sometimes, a sequence is better used in reversed order. In those cases, you can simply reverse a vector `x` using the Python syntax `x[::-1]` before using it to train your LSTM network. Sometimes, neither the forward nor the reversed order works perfectly, but combining them will give better results. In this case, you will need a bidirectional LSTM network.\n",
    "\n",
    "<b>A bidirectional LSTM network is simply two separate LSTM networks; one feeds with a forward sequence and another with reversed sequence. Then the output of the two LSTM networks is concatenated together before being fed to the subsequent layers of the network</b>. In Keras, you have the function [Bidirectional()](https://keras.io/api/layers/recurrent_layers/bidirectional/) to clone an LSTM layer for forward-backward input and concatenate their output. For example:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "Since you created not one, but two LSTMs with 100 units each, <b>this network will take twice the amount of time to train. Depending on the problem, this additional cost may be justified</b>.\n",
    "\n",
    "The full code listing with adding the bidirectional LSTM to the last example is listed below for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "855e0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bidir_lstm_model_with_lstm_dropout(dropout=0.2, recurrent_dropout=0.2):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "  model.add(Bidirectional(LSTM(100, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af0b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 200)              106400    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,601\n",
      "Trainable params: 266,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_with_bidir_lstm_dropout = get_bidir_lstm_model_with_lstm_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ef35820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 505s 1s/step - loss: 0.5189 - accuracy: 0.7382\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 473s 1s/step - loss: 0.3679 - accuracy: 0.8406\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 449s 1s/step - loss: 0.3021 - accuracy: 0.8752\n",
      "CPU times: user 1h 15min 30s, sys: 13min 55s, total: 1h 29min 25s\n",
      "Wall time: 23min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4c99a50a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_with_bidir_lstm_dropout.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "008e1cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.37%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74049bdc",
   "metadata": {},
   "source": [
    "It seems you can only get a slight improvement but with a significantly longer training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773af289",
   "metadata": {},
   "source": [
    "<a id='lstm_4'></a>\n",
    "### <font color='darkgreen'>LSTM and Convolutional Neural Network for Sequence Classification</font> ([back](#lstm))\n",
    "<b><font size='3ptx'>Convolutional neural networks excel at learning the spatial structure in input data.</font></b>\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews, and <b>the CNN may be able to pick out invariant features for the good and bad sentiment. This learned spatial feature may then be learned as sequences by an LSTM layer</b>.\n",
    "\n",
    "<b>You can easily add a one-dimensional CNN and max pooling layers after the Embedding layer, which then feeds the consolidated features to the LSTM</b>. You can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size. For example, you would create the model as follows:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "The full code listing with CNN and LSTM layers is listed below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd48d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_then_lstm_model():\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(LSTM(100))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5067e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm_model = get_cnn_then_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03d639f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "391/391 [==============================] - 258s 657ms/step - loss: 0.2291 - accuracy: 0.9130\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 265s 678ms/step - loss: 0.2546 - accuracy: 0.9007\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 253s 646ms/step - loss: 0.1969 - accuracy: 0.9260\n",
      "CPU times: user 13min 20s, sys: 22min 43s, total: 36min 3s\n",
      "Wall time: 12min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe4c8dfc880>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c9f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.20%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844330d",
   "metadata": {},
   "source": [
    "You can see that you achieve slightly better results than the first example, although with fewer weights and faster training time. You might expect that even better results could be achieved if this example was further extended to use dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718ba8a",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Resources</font>\n",
    "Below are some resources if you are interested in diving deeper into sequence prediction or this specific example.\n",
    "* [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)\n",
    "* [Medium - LSTMs for regression](https://bobrupakroy.medium.com/lstms-for-regression-cc9b6677697f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
