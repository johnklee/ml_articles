{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://machinelearningmastery.com/dynamic-classifier-selection-in-python/)) **<font size='3ptx'>Dynamic classifier selection is a type of ensemble learning algorithm for classification predictive modeling.</font>**\n",
    "\n",
    "The technique involves fitting multiple machine learning models on the training dataset, then selecting the model that is expected to perform best when making a prediction, based on the specific details of the example to be predicted.\n",
    "\n",
    "This can be achieved using a k-nearest neighbor model to locate examples in the training dataset that are closest to the new example to be predicted, **evaluating all models in the pool on this neighborhood and using the model that performs the best on the neighborhood to make a prediction for the new example.**\n",
    "\n",
    "As such, **the dynamic classifier selection can often perform better than any single model in the pool and provides an alternative to averaging the predictions from multiple models, as is the case in other ensemble algorithms.**\n",
    "\n",
    "In this tutorial, you will discover how to develop dynamic classifier selection ensembles in Python. After completing this tutorial, you will know:\n",
    "* Dynamic classifier selection algorithms choose one from among many models to make a prediction for each new example.\n",
    "* How to develop and evaluate dynamic classifier selection models for classification tasks using the scikit-learn API.\n",
    "* How to explore the effect of dynamic classifier selection model hyperparameters on classification accuracy.\n",
    "\n",
    "### <font color='darkgreen'>Tutorial Overview</font>\n",
    "This tutorial is divided into three parts; they are:\n",
    "* <font size='3ptx'>[**Dynamic Classifier Selection**](#sect1)</font>\n",
    "* <font size='3ptx'>[**Dynamic Classifier Selection With Scikit-Learn**](#sect2)</font>\n",
    "    * [**DCS With Overall Local Accuracy (OLA)**](#sect2_1)\n",
    "    * [**DCS With Local Class Accuracy (LCA)**](#sect2_2)\n",
    "* <font size='3ptx'>[**Hyperparameter Tuning for DCS**](#sect3)</font>\n",
    "    * [**Explore k in k-Nearest Neighbor**](#sect3_1)\n",
    "    * [**Explore Algorithms for Classifier Pool**](#sect3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from deslib.dcs.ola import OLA\n",
    "from deslib.dcs.lca import LCA\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Dynamic Classifier Selection</font>\n",
    "**Multiple Classifier Systems** refers to a field of machine learning algorithms that use multiple models to address classification predictive modeling problems. This includes familiar techniques such as [one-vs-rest, one-vs-all](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/), and output error-correcting codes techniques. It also includes more general techniques that select a model to use dynamically for each new example that requires a prediction.\n",
    "> Several approaches are currently used to construct an MCS. One of the most promising MCS approaches is Dynamic Selection (DS), in which the base classifiers are selected on the fly, according to each new sample to be classified. <br/><br/>\n",
    "> [**— Dynamic Classifier Selection: Recent Advances And Perspectives, 2018.**](https://www.sciencedirect.com/science/article/pii/S1566253517304074)\n",
    "\n",
    "For more on these types of multiple classifier systems, see the tutorial:\n",
    "* [How to Use One-vs-Rest and One-vs-One for Multi-Class Classification](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)\n",
    "\n",
    "These methods are generally known by the name: **<font color='darkblue'>Dynamic Classifier Selection</font>**, or DCS for short.\n",
    "> **Dynamic Classifier Selection:** Algorithms that choose one from among many trained models to make a prediction based on the specific details of the input.\n",
    "\n",
    "Given that multiple models are used in DCS, **it is considered a type of ensemble learning technique.**\n",
    "\n",
    "**Dynamic Classifier Selection algorithms generally involve partitioning the input feature space in some way and assigning specific models to be responsible for making predictions for each partition.** There are a variety of different DCS algorithms and research efforts are mainly focused on how to evaluate and assign classifiers to specific regions of the input space.\n",
    "> After training multiple individual learners, DCS dynamically selects one learner for each test instance. DCS makes predictions by using one individual learner. <br/><br/>\n",
    "> [**— Page 93, Ensemble Methods: Foundations and Algorithms, 2012.**](https://amzn.to/32L1yWD)\n",
    "\n",
    "An early and popular approach involves first fitting a small, diverse set of classification models on the training dataset. When a prediction is required, first a [**K-nearest neighbor**](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) (kNN) algorithm is used to find the `k` most similar examples from the training dataset that match the example. **Each previously fit classifier in the model is then evaluated on the neighbor of k training examples and the classifier that performs the best is selected to make a prediction for the new example.**\n",
    "\n",
    "This approach is referred to as “Dynamic Classifier Selection Local Accuracy” or DCS-LA for short and was described by Kevin Woods, et al. in their 1997 paper titled “[Combination Of Multiple Classifiers Using Local Accuracy Estimates.](https://ieeexplore.ieee.org/abstract/document/588027)”\n",
    "> The basic idea is to estimate each classifier’s accuracy in local region of feature space surrounding an unknown test sample, and then use the decision of the most locally accurate classifier.\n",
    "\n",
    "The authors describe two approaches for selecting a single classifier model to make a prediction for a given input example, they are:\n",
    "* **Local Accuracy**, often referred to as LA or Overall Local Accuracy (OLA).\n",
    "* **Class Accuracy**, often referred to as CA or Local Class Accuracy (LCA).\n",
    "\n",
    "**<font color='darkblue'>Local Accuracy</font>** (OLA) involves evaluating the classification accuracy of each model on the neighborhood of `k` training examples. The model that performs the best in this neighborhood is then selected to make a prediction for the new example.\n",
    "> The OLA of each classifier is computed as the percentage of the correct recognition of the samples in the local region. <br/><br/>\n",
    "> [**— Dynamic Selection Of Classifiers—a Comprehensive Review, 2014.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320314001885)\n",
    "\n",
    "**<font color='darkblue'>Class Accuracy</font>** (LCA) involves using each model to make a prediction for the new example and noting the class that was predicted. Then, the accuracy of each model on the neighbor of `k` training examples is evaluated and the model that has the best skill for the class that it predicted on the new example is selected and its prediction returned.\n",
    "> The LCA is estimated for each base classifier as the percentage of correct classifications within the local region, but considering only those examples where the classifier has given the same class as the one it gives for the unknown pattern. <br/><br/>\n",
    "> [**— Dynamic Selection Of Classifiers—a Comprehensive Review, 2014.**](https://www.sciencedirect.com/science/article/abs/pii/S0031320314001885)\n",
    "\n",
    "In both cases, if all fit models make the same prediction for a new input example, then the prediction is returned directly. Now that we are familiar with DCS and the DCS-LA algorithm, let’s look at how we can use it on our own classification predictive modeling projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Dynamic Classifier Selection With Scikit-Learn</font>\n",
    "[**The Dynamic Ensemble Selection Library**](https://github.com/scikit-learn-contrib/DESlib) or DESlib for short is an open source Python library that provides an implementation of many different dynamic classifier selection algorithms. DESlib is an easy-to-use ensemble learning library **focused on the implementation of the state-of-the-art techniques for dynamic classifier and ensemble selection.**\n",
    "\n",
    "First, we can install the DESlib library using the pip package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deslib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can confirm that the library was installed correctly and is ready to be used by loading the library and printing the installed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.5\n"
     ]
    }
   ],
   "source": [
    "# check deslib version\n",
    "import deslib\n",
    "print(deslib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DESlib provides an implementation of the DCS-LA algorithm with each classifier selection technique via the [**OLA**](https://deslib.readthedocs.io/en/latest/modules/dcs/ola.html) and [**LCA**](https://deslib.readthedocs.io/en/latest/modules/dcs/lca.html) classes respectively.\n",
    "\n",
    "Each class can be used as a scikit-learn model directly, allowing the full suite of scikit-learn data preparation, modeling pipelines, and model evaluation techniques to be used directly.\n",
    "\n",
    "Both classes use a k-nearest neighbor algorithm to select the neighbor with a default value of `k=7`.\n",
    "\n",
    "A [**bootstrap aggregation**](https://machinelearningmastery.com/bagging-ensemble-with-python/) (<font color='brown'>bagging</font>) ensemble of decision trees is used as the pool of classifier models considered for each classification that is made by default, although this can be changed by setting “<font color='violet'>pool_classifiers</font>” to a list of models.\n",
    "\n",
    "We can use the [make_classification()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function to create a synthetic binary classification problem with 10,000 examples and 20 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 20) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_1'></a>\n",
    "### <font color='darkgreen'>DCS With Overall Local Accuracy (OLA)</font>\n",
    "**We can evaluate a DCS-LA model using overall local accuracy on the synthetic dataset.**\n",
    "\n",
    "In this case, we will use default model hyperparameters, including bagged decision trees as the pool of classifier models and a `k=7` for the selection of the local neighborhood when making a prediction.\n",
    "\n",
    "We will evaluate the model using repeated stratified k-fold cross-validation with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = OLA()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.883 (0.011)\n"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the DCS-LA with OLA and default hyperparameters achieves a classification accuracy of about 88.3 percent.\n",
    "\n",
    "We can also use the DCS-LA model with OLA as a final model and make predictions for classification. First, the model is fit on all available data, then the predict() function can be called to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLA(DFP=False, DSEL_perc=0.5, IH_rate=0.3, diff_thresh=0.1, k=7,\n",
       "    knn_classifier='knn', knne=False, n_jobs=-1, pool_classifiers=None,\n",
       "    random_state=None, safe_k=None, selection_method='best', with_IH=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = OLA()\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# make a single prediction\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with using DCS-LA with OLA, let’s look at the LCA method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <font color='darkgreen'>DCS With Local Class Accuracy (LCA)</font>\n",
    "**We can evaluate a DCS-LA model using local class accuracy on the synthetic dataset.**\n",
    "\n",
    "In this case, we will use default model hyperparameters, including bagged decision trees as the pool of classifier models and a `k=7` for the selection of the local neighborhood when making a prediction.\n",
    "\n",
    "We will evaluate the model using repeated stratified k-fold cross-validation with three repeats and 10 folds. We will report the mean and standard deviation of the accuracy of the model across all repeats and folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.920 (0.009)\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = LCA()\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see the DCS-LA with LCA and default hyperparameters achieves a classification accuracy of about 92.2 percent.\n",
    "\n",
    "We can also use the DCS-LA model with LCA as a final model and make predictions for classification. First, the model is fit on all available data, then the <font color='blue'>predict()</font> function can be called to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 1\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# define the model\n",
    "model = LCA()\n",
    "\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [0.2929949,-4.21223056,-1.288332,-2.17849815,-0.64527665,2.58097719,0.28422388,-7.1827928,-1.91211104,2.73729512,0.81395695,3.96973717,-2.66939799,3.34692332,4.19791821,0.99990998,-0.30201875,-4.43170633,-2.82646737,0.44916808]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Hyperparameter Tuning for DCS</font>\n",
    "**In this section, we will take a closer look at some of the hyperparameters you should consider tuning for the DCS-LA model and their effect on model performance.**\n",
    "\n",
    "There are many hyperparameters we can look at for DCS-LA, **although in this case, we will look at the value of `k` in the k-nearest neighbor model used in the local evaluation of the models, and how to use a custom pool of classifiers.**\n",
    "\n",
    "We will use the DCS-LA with OLA as the basis for these experiments, although the choice of the specific method is arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_1'></a>\n",
    "### <font color='darkgreen'>Explore k in k-Nearest Neighbor</font>\n",
    "**The configuration of the k-nearest neighbor algorithm is critical to the DCS-LA model** as it defines the scope of the neighborhood in which each classifier is considered for selection.\n",
    "\n",
    "**The `k` value controls the size of the neighborhood and it is important to set it to a value that is appropriate for your dataset, specifically the density of samples in the feature space.** A value too small will mean that relevant examples in the training set might be excluded from the neighborhood, whereas values too large may mean that the signal is being washed out by too many examples.\n",
    "\n",
    "The example below explores the classification accuracy of the DCS-LA with OLA with k values from 2 to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2 0.871 (0.009)\n",
      ">3 0.874 (0.010)\n",
      ">4 0.876 (0.012)\n",
      ">5 0.879 (0.011)\n",
      ">6 0.878 (0.011)\n",
      ">7 0.882 (0.009)\n",
      ">8 0.885 (0.013)\n",
      ">9 0.883 (0.010)\n",
      ">10 0.886 (0.011)\n",
      ">11 0.888 (0.009)\n",
      ">12 0.887 (0.009)\n",
      ">13 0.887 (0.010)\n",
      ">14 0.887 (0.009)\n",
      ">15 0.889 (0.008)\n",
      ">16 0.891 (0.011)\n",
      ">17 0.890 (0.009)\n",
      ">18 0.889 (0.010)\n",
      ">19 0.890 (0.011)\n",
      ">20 0.887 (0.010)\n",
      ">21 0.887 (0.011)\n"
     ]
    }
   ],
   "source": [
    "# get the dataset\n",
    "def get_dataset():\n",
    "    X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "    return X, y\n",
    " \n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for n in range(2,22):\n",
    "        models[str(n)] = OLA(k=n)\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the general trend of increasing model performance and `k value` before reaching a plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_2'></a>\n",
    "### <font color='darkgreen'>Explore Algorithms for Classifier Pool</font>\n",
    "**The choice of algorithms used in the pool for the DCS-LA is another important hyperparameter.**\n",
    "\n",
    "By default, bagged decision trees are used, as it has proven to be an effective approach on a range of classification tasks. Nevertheless, a custom pool of classifiers can be considered.\n",
    "\n",
    "This requires first defining a list of classifier models to use and fitting each on the training dataset. Unfortunately, this means that the automatic k-fold cross-validation model evaluation methods in scikit-learn cannot be used in this case. Instead, we will use a train-test split so that we can fit the classifier pool manually on the training dataset.\n",
    "\n",
    "The list of fit classifiers can then be specified to the OLA (or LCA) class via the “<font color='violet'>pool_classifiers</font>” argument. In this case, we will use a pool that includes logistic regression, a decision tree, and a naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=10000, n_features=20, n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "# split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# define classifiers to use in the pool\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# fit each classifier on the training set\n",
    "for c in classifiers:\n",
    "    c.fit(X_train, y_train)\n",
    "\n",
    "# define the DCS-LA model\n",
    "model = OLA(pool_classifiers=classifiers)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the model achieved an accuracy of about 91.2 percent.\n",
    "\n",
    "**In order to adopt the DCS model, it must perform better than any contributing model**. Otherwise, we would simply use the contributing model that performs better instead.\n",
    "\n",
    "We can check this by evaluating the performance of each contributing classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LogisticRegression: 0.873\n",
      ">DecisionTreeClassifier: 0.886\n",
      ">GaussianNB: 0.875\n"
     ]
    }
   ],
   "source": [
    "# evaluate contributing models\n",
    "for c in classifiers:\n",
    "    yhat = c.predict(X_test)\n",
    "    score = accuracy_score(y_test, yhat)\n",
    "    print('>%s: %.3f' % (c.__class__.__name__, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that again, the DCS-LA achieves an accuracy of about 91.3 percent, which is better than any contributing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Ensembling Comparison</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Keras to train a ensembling model for comparison:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
