{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f515a1-0cad-4aa1-bf81-d8d920b32d2e",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "<font size='3ptx'> ([source](https://machinelearningmastery.com/the-complete-guide-to-using-pydantic-for-validating-llm-outputs/)) <b>Large language models generate text, not structured data. Even when you prompt them to return structured data, they’re still generating text that looks like valid JSON.</b> The output may have incorrect field names, missing required fields, wrong data types, or extra text wrapped around the actual data. Without validation, these inconsistencies cause runtime errors that are difficult to debug.</font>\n",
    "\n",
    "<b>[Pydantic](https://docs.pydantic.dev/latest/) helps you validate data at runtime using Python type hints</b>. It checks that LLM outputs match your expected schema, converts types automatically where possible, and provides clear error messages when validation fails. This gives you a reliable contract between the LLM’s output and your application’s requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148b8e4-c724-415d-8d76-c0f1918a5778",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Agenda</font></b>\n",
    "Topics we will cover include:\n",
    "* Designing robust Pydantic models (including custom validators and nested schemas).\n",
    "* Parsing “messy” LLM outputs safely and surfacing precise validation errors.\n",
    "* Integrating validation with OpenAI, LangChain, and LlamaIndex plus retry strategies.\n",
    "\n",
    "![ui](https://machinelearningmastery.com/wp-content/uploads/2025/12/mlm-complete-guide-pydantic-featured-image.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae5c30-a536-4493-af90-aca2b2d8d09e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Getting Started</font></b>\n",
    "<font size='3ptx'><b>Let’s start with a simple example by building a tool that extracts contact information from text.</b> The LLM reads unstructured text and returns structured data that we validate with Pydantic:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8025427a-94d2-4f31-9cea-293e2689e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, EmailStr, field_validator\n",
    "from typing import Optional\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    phone: Optional[str] = None\n",
    "    company: Optional[str] = None\n",
    "    \n",
    "    @field_validator('phone')\n",
    "    @classmethod\n",
    "    def validate_phone(cls, v):\n",
    "        if v is None:\n",
    "            return v\n",
    "        cleaned = ''.join(filter(str.isdigit, v))\n",
    "        if len(cleaned) < 10:\n",
    "            raise ValueError('Phone number must have at least 10 digits')\n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c5f71-eac2-45ff-b858-4a5592fd31bc",
   "metadata": {},
   "source": [
    "<b>All Pydantic models inherit from [**BaseModel**](https://docs.pydantic.dev/latest/api/base_model/#BaseModel), which provides automatic validation</b>. Type hints like `name: str` help Pydantic validate types at runtime. The `EmailStr` type validates email format without needing a custom regex. Fields marked with `Optional[str] = None` can be missing or null. The `@field_validator` decorator lets you add custom validation logic, like cleaning phone numbers and checking their length.\n",
    "\n",
    "Here’s how to use the model to validate sample LLM output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce5b5cd-2cbc-4072-9848-8df74f854266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarah Johnson\n",
      "sarah.johnson@techcorp.com\n",
      "{'name': 'Sarah Johnson', 'email': 'sarah.johnson@techcorp.com', 'phone': '5551234567', 'company': 'TechCorp Industries'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_response = '''\n",
    "{\n",
    "    \"name\": \"Sarah Johnson\",\n",
    "    \"email\": \"sarah.johnson@techcorp.com\",\n",
    "    \"phone\": \"(555) 123-4567\",\n",
    "    \"company\": \"TechCorp Industries\"\n",
    "}\n",
    "'''\n",
    "\n",
    "data = json.loads(llm_response)\n",
    "contact = ContactInfo(**data)\n",
    "\n",
    "print(contact.name) \n",
    "print(contact.email)  \n",
    "print(contact.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd718277-55c2-4e73-a3cf-bbc59edb6f61",
   "metadata": {},
   "source": [
    "When you create a <b><font color='blue'>ContactInfo</font></b> instance, <b>Pydantic validates everything automatically. If validation fails, you get a clear error message telling you exactly what went wrong</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b84c7-18b6-4edb-92b6-8c598dea9875",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Parsing and Validating LLM Outputs</font></b>\n",
    "<font size='3ptx'><b>LLMs don’t always return perfect JSON</b>. Sometimes they add markdown formatting, explanatory text, or mess up the structure. Here’s how to handle these cases:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2588f1ad-26a3-4a7c-8f27-c2a61deceef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError, field_validator\n",
    "import json\n",
    "import re\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    product_name: str\n",
    "    rating: int\n",
    "    review_text: str\n",
    "    would_recommend: bool\n",
    "    \n",
    "    @field_validator('rating')\n",
    "    @classmethod\n",
    "    def validate_rating(cls, v):\n",
    "        if not 1 <= v <= 5:\n",
    "            raise ValueError('Rating must be an integer between 1 and 5')\n",
    "        return v\n",
    "\n",
    "\n",
    "def extract_json_from_llm_response(response: str) -> dict:\n",
    "    \"\"\"Extract JSON from LLM response that might contain extra text.\"\"\"\n",
    "    json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json.loads(json_match.group())\n",
    "    raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "\n",
    "def parse_review(llm_output: str) -> ProductReview:\n",
    "    \"\"\"Safely parse and validate LLM output.\"\"\"\n",
    "    try:\n",
    "        data = extract_json_from_llm_response(llm_output)\n",
    "        review = ProductReview(**data)\n",
    "        return review\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        raise\n",
    "    except ValidationError as e:\n",
    "        print(f\"Validation error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f80d1-2662-40d1-b173-cc4476f49d21",
   "metadata": {},
   "source": [
    "This approach uses regex to find JSON within response text, handling cases where the LLM adds explanatory text before or after the data. We catch different exception types separately:\n",
    "- <b><font color='blue'>JSONDecodeError</font></b> for malformed JSON,\n",
    "- <b><font color='blue'>ValidationError</font></b> for data that doesn’t match the schema, and\n",
    "- General exceptions for unexpected issues.\n",
    "\n",
    "The `extract_json_from_llm_response` function handles text cleanup while `parse_review` handles validation, keeping concerns separated. In production, you’d want to log these errors or retry the LLM call with an improved prompt.\n",
    "\n",
    "This example shows an LLM response with extra text that our parser handles correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1909d6-0dee-4272-972b-ab1185777ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Wireless Headphones X100\n",
      "Rating: 4/5\n"
     ]
    }
   ],
   "source": [
    "messy_response = '''\n",
    "Here's the review in JSON format:\n",
    "\n",
    "{\n",
    "    \"product_name\": \"Wireless Headphones X100\",\n",
    "    \"rating\": 4,\n",
    "    \"review_text\": \"Great sound quality, comfortable for long use.\",\n",
    "    \"would_recommend\": true\n",
    "}\n",
    "\n",
    "Hope this helps!\n",
    "'''\n",
    "\n",
    "review = parse_review(messy_response)\n",
    "print(f\"Product: {review.product_name}\")\n",
    "print(f\"Rating: {review.rating}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb9ccf3-fc2c-4e06-84c0-02ad1da925cc",
   "metadata": {},
   "source": [
    "The parser extracts the JSON block from the surrounding text and validates it against the <b><font color='blue'>ProductReview</font></b> schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c9c15-de19-4fee-bfc2-66d07ebb52ce",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Advanced: Using Pydantic validator (v1/v2)</font></b>\n",
    "If you want the model itself to accept messy text, you can add a `root_validator` (v1) or `model_validator` (v2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4874ab4c-a47b-45d5-b02c-f949f8e214f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258636/952699920.py:8: PydanticDeprecatedSince20: Pydantic V1 style `@root_validator` validators are deprecated. You should migrate to Pydantic V2 style `@model_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @root_validator(pre=True)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel, root_validator\n",
    "\n",
    "class UserModel(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "    @root_validator(pre=True)\n",
    "    def extract_json(cls, values):\n",
    "        # values could be a dict or raw string\n",
    "        if isinstance(values, str):\n",
    "            import re, json\n",
    "            match = re.search(r'```json(.*?)```', values, re.DOTALL)\n",
    "            if match:\n",
    "                values = json.loads(match.group(1).strip())\n",
    "        print(f'Values: {values}')\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bc4b75-3b41-4a3e-9fad-80d6f5ec2b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: {'name': 'John', 'age': 30}\n",
      "name='John' age=30\n"
     ]
    }
   ],
   "source": [
    "user = UserModel(**{'name': 'John', 'age': 30})\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d6cb5-954f-44ba-8a5b-efc94acdfe39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Working with Nested Models</font></b>\n",
    "<font size='3ptx'>Real-world data is rarely flat. Here’s how to handle nested structures like a product with multiple reviews and specifications:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea285c7-12b6-4e43-94d5-c2bea9b9184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Specification(BaseModel):\n",
    "    key: str\n",
    "    value: str\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    reviewer_name: str\n",
    "    rating: int = Field(..., ge=1, le=5)\n",
    "    comment: str\n",
    "    verified_purchase: bool = False\n",
    "\n",
    "\n",
    "class Product(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    price: float = Field(..., gt=0)\n",
    "    category: str\n",
    "    specifications: List[Specification]\n",
    "    reviews: List[Review]\n",
    "    average_rating: float = Field(..., ge=1, le=5)\n",
    "    \n",
    "    @field_validator('average_rating')\n",
    "    @classmethod\n",
    "    def check_average_matches_reviews(cls, v, info):\n",
    "        reviews = info.data.get('reviews', [])\n",
    "        if reviews:\n",
    "            calculated_avg = sum(r.rating for r in reviews) / len(reviews)\n",
    "            if abs(calculated_avg - v) > 0.1:\n",
    "                raise ValueError(\n",
    "                    f'Average rating {v} does not match calculated average {calculated_avg:.2f}'\n",
    "                )\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9535225-8bc5-43a4-9bbf-4ca74c1b882d",
   "metadata": {},
   "source": [
    "The <b><font color='blue'>Product</font></b> model contains lists of <b><font color='blue'>Specification</font></b> and <b><font color='blue'>Review</font></b> objects, and each nested model is validated independently. Using `Field(..., ge=1, le=5)` adds constraints directly in the type hint, where ge means “greater than or equal” and gt means “greater than”.\n",
    "\n",
    "\n",
    "The `check_average_matches_reviews validator` accesses other fields using `info.data`, allowing you to validate relationships between fields. When you pass nested dictionaries to `Product(**data)`, Pydantic automatically creates the nested <b><font color='blue'>Specification</font></b> and <b><font color='blue'>Review</font></b> objects.\n",
    "\n",
    "This structure ensures data integrity at every level. If a single review is malformed, you’ll know exactly which one and why.\n",
    "\n",
    "This example shows how nested validation works with a complete product structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e989a33-7767-4008-801d-1e468ac468f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart Coffee Maker: $129.99\n",
      "Average Rating: 4.5\n",
      "Number of reviews: 2\n"
     ]
    }
   ],
   "source": [
    "llm_response = {\n",
    "    \"id\": \"PROD-2024-001\",\n",
    "    \"name\": \"Smart Coffee Maker\",\n",
    "    \"price\": 129.99,\n",
    "    \"category\": \"Kitchen Appliances\",\n",
    "    \"specifications\": [\n",
    "        {\"key\": \"Capacity\", \"value\": \"12 cups\"},\n",
    "        {\"key\": \"Power\", \"value\": \"1000W\"},\n",
    "        {\"key\": \"Color\", \"value\": \"Stainless Steel\"}\n",
    "    ],\n",
    "    \"reviews\": [\n",
    "        {\n",
    "            \"reviewer_name\": \"Alex M.\",\n",
    "            \"rating\": 5,\n",
    "            \"comment\": \"Makes excellent coffee every time!\",\n",
    "            \"verified_purchase\": True\n",
    "        },\n",
    "        {\n",
    "            \"reviewer_name\": \"Jordan P.\",\n",
    "            \"rating\": 4,\n",
    "            \"comment\": \"Good but a bit noisy\",\n",
    "            \"verified_purchase\": True\n",
    "        }\n",
    "    ],\n",
    "    \"average_rating\": 4.5\n",
    "}\n",
    "\n",
    "product = Product(**llm_response)\n",
    "print(f\"{product.name}: ${product.price}\")\n",
    "print(f\"Average Rating: {product.average_rating}\")\n",
    "print(f\"Number of reviews: {len(product.reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a07bae-6d01-473f-9a89-65461bae8323",
   "metadata": {},
   "source": [
    "Pydantic validates the entire nested structure in one call, checking that specifications and reviews are properly formed and that the average rating matches the individual review ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5378ef-7f7a-4fac-9b6a-32761bfc1032",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Using Pydantic with LLM APIs and Frameworks</font></b>\n",
    "<font size='3ptx'>So far, we’ve learned that <b>we need a reliable way to convert free-form text into structured, validated data.</b> Now let’s see how to use Pydantic validation with OpenAI’s API, as well as frameworks like [**LangChain**](https://www.langchain.com/) and [**LlamaIndex**](https://www.llamaindex.ai/). Be sure to install the required SDKs.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9f35d-80bf-4ecb-9bfd-2007dc52d6ca",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Using LangChain with Pydantic</font></b>\n",
    "<font size='3ptx'><b>[LangChain](https://www.langchain.com/) provides built-in support for structured output extraction with Pydantic models</b>. There are two main approaches that handle the complexity of prompt engineering and parsing for you.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffbf175d-b785-4963-9956-cdffc4d5ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==1.2.0\n",
      "langchain-classic==1.0.1\n",
      "langchain-community==0.4.1\n",
      "langchain-core==1.2.5\n",
      "langchain-google-genai==4.1.2\n",
      "langchain-text-splitters==1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -P \"(langchain|langchain-core|langchain-community|langchain-google-genai)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f8c635-e12c-4c33-9998-e2868ede95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSerializable, RunnableConfig\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c07f4008-2eae-4895-b67c-757913ea095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "\n",
    "restaurant_text = \"\"\"\n",
    "Mama's Italian Kitchen is a cozy family-owned restaurant serving authentic \n",
    "Italian cuisine. Rated 4.5 stars, it's known for its homemade pasta and \n",
    "wood-fired pizzas. Prices are moderate ($$), and their signature dishes \n",
    "include lasagna bolognese and tiramisu.\n",
    "\"\"\"\n",
    "\n",
    "restaurant_text2 = \"\"\"\n",
    "Sakura Garden Sushi is a serene, contemporary bistro offering traditional Japanese flavors with a modern twist.\n",
    "Rated 4.7 stars, it’s celebrated for its pristine sashimi and creative specialty rolls.\n",
    "Prices are upscale ($$$), and their standout offerings include the bluefin tuna flight and matcha lava cake.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8eb7f5f0-5fbc-47e6-8d78-8015798e8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "JSON_CONTENT_EXTRACT_PTN = r'''```json(?P<json_raw_text>.*)```'''\n",
    "\n",
    "\n",
    "class DebugRunnable(RunnableSerializable):\n",
    "    \"\"\"Print out all data generated by LLM.\"\"\"\n",
    "    \n",
    "    def invoke(self, input: Any, config: Optional[RunnableConfig] = None) -> Any:\n",
    "        print(\"\\n--- [DEBUG: LLM Raw Output] ---\")\n",
    "        # LLM outputs are usually AIMessage objects; \n",
    "        # .content gets you the actual string for cleaner viewing.\n",
    "        if hasattr(input, \"content\"):\n",
    "            print(input.content)\n",
    "            if '```json' in input.content:\n",
    "                print('Unexpected \"```json\" detected! Clean it out...')\n",
    "                mth = re.search(JSON_CONTENT_EXTRACT_PTN, input.content, flags=re.DOTALL)\n",
    "                json_raw_text = mth.group('json_raw_text')\n",
    "                input.content = json_raw_text\n",
    "                print(f'Extracted Json Raw Text:\\n{json_raw_text}\\n')\n",
    "        else:\n",
    "            print(input)\n",
    "        print(\"--- [END DEBUG] ---\\n\")\n",
    "        \n",
    "        # Crucial: Return the input so the next step in the chain receives it\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23a82258-7457-4f9f-97a8-9403fa83c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Restaurant(BaseModel):\n",
    "    \"\"\"Information about a restaurant.\"\"\"\n",
    "    name: str = Field(description=\"The name of the restaurant\")\n",
    "    cuisine: str = Field(description=\"Type of cuisine served\")\n",
    "    price_range: str = Field(description=\"Price range with unit as NT dollar.\")\n",
    "    rating: Optional[float] = Field(default=None, description=\"Rating out of 5.0\")\n",
    "    specialties: List[str] = Field(description=\"Signature dishes or specialties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90288924-3035-483b-b524-97930c8a6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_restaurant_with_parser(text: str) -> Restaurant:\n",
    "    \"\"\"Extract restaurant info using LangChain's PydanticOutputParser.\"\"\"\n",
    "    \n",
    "    parser = PydanticOutputParser(pydantic_object=Restaurant)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Extract restaurant information from the following text.\\n{format_instructions}\\n{text}\\n\",\n",
    "        input_variables=[\"text\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=1.0,  # Gemini 3.0+ defaults to 1.0\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "    debug = DebugRunnable()\n",
    "    chain = prompt | llm | debug | parser\n",
    "\n",
    "    result = chain.invoke({\"text\": text})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf2417-d13c-41dd-b762-ae8b97a45a8c",
   "metadata": {},
   "source": [
    "The <b><font color='blue'>PydanticOutputParser</font></b> automatically generates format instructions from your Pydantic model, including field descriptions and type information. It works with any LLM that can follow instructions and doesn’t require function calling support. The chain syntax makes it easy to compose complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa33ebf8-545b-4425-b365-ce67c1714ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [DEBUG: LLM Raw Output] ---\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Sakura Garden Sushi\",\n",
      "  \"cuisine\": \"Japanese\",\n",
      "  \"price_range\": \"upscale ($$$)\",\n",
      "  \"rating\": 4.7,\n",
      "  \"specialties\": [\n",
      "    \"pristine sashimi\",\n",
      "    \"creative specialty rolls\",\n",
      "    \"bluefin tuna flight\",\n",
      "    \"matcha lava cake\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Unexpected \"```json\" detected! Clean it out...\n",
      "Extracted Json Raw Text:\n",
      "\n",
      "{\n",
      "  \"name\": \"Sakura Garden Sushi\",\n",
      "  \"cuisine\": \"Japanese\",\n",
      "  \"price_range\": \"upscale ($$$)\",\n",
      "  \"rating\": 4.7,\n",
      "  \"specialties\": [\n",
      "    \"pristine sashimi\",\n",
      "    \"creative specialty rolls\",\n",
      "    \"bluefin tuna flight\",\n",
      "    \"matcha lava cake\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "--- [END DEBUG] ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    restaurant_info = extract_restaurant_with_parser(restaurant_text2)    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdba63e1-0eae-4f37-8c0c-badca42f5e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant: Mama's Italian Kitchen\n",
      "Cuisine: Italian\n",
      "Specialties: homemade pasta, wood-fired pizzas, lasagna bolognese, tiramisu\n",
      "Rating: 4.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Restaurant: {restaurant_info.name}\")\n",
    "print(f\"Cuisine: {restaurant_info.cuisine}\")\n",
    "print(f\"Specialties: {', '.join(restaurant_info.specialties)}\")\n",
    "print(f\"Rating: {restaurant_info.rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5bc099-e85c-4a2f-b091-20a41c0ab25e",
   "metadata": {},
   "source": [
    "The second method is to use the native function calling capabilities of modern LLMs through the [with_structured_output() function](https://api.python.langchain.com/en/latest/chains/langchain.chains.structured_output.base.create_structured_output_runnable.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6981eee8-366b-482b-97a6-8872f92b762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_restaurant_structured(text: str) -> Restaurant:\n",
    "    \"\"\"Extract restaurant info using with_structured_output.\"\"\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=1.0,  # Gemini 3.0+ defaults to 1.0\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "    \n",
    "    structured_llm = llm.with_structured_output(Restaurant)\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"Extract restaurant information from the following text:\\n\\n{text}\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | structured_llm\n",
    "    result = chain.invoke({\"text\": text})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ede3837-d27e-44ac-be86-075f7e187f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    restaurant_info = extract_restaurant_structured(restaurant_text)    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ab2f638-8821-4407-a078-9f2004f31f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant: Mama's Italian Kitchen\n",
      "Cuisine: Italian\n",
      "Specialties: homemade pasta, wood-fired pizzas, lasagna bolognese, tiramisu\n",
      "Rating: 4.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Restaurant: {restaurant_info.name}\")\n",
    "print(f\"Cuisine: {restaurant_info.cuisine}\")\n",
    "print(f\"Specialties: {', '.join(restaurant_info.specialties)}\")\n",
    "print(f\"Rating: {restaurant_info.rating}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c349d33-0069-416f-9432-28b7d9f6ebeb",
   "metadata": {},
   "source": [
    "This method produces cleaner, more concise code and makes use of the model’s native function calling capabilities for more reliable extraction. You don’t need to manually create parsers or format instructions, and it’s generally more accurate than prompt-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c714f-a713-4365-8a78-fb024b5fc30c",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Retrying LLM Calls with Better Prompts</font></b>\n",
    "<font size='3ptx'>When the LLM returns invalid data, you can <b>retry with an improved prompt that includes the error message from the failed validation attempt</b></font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "862d8ab1-baaa-4c5a-af02-4146b9565dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "\n",
    "class EventExtraction(BaseModel):\n",
    "    event_name: str\n",
    "    date: str\n",
    "    location: str\n",
    "    attendees: int\n",
    "    event_type: str\n",
    "\n",
    "\n",
    "def extract_with_retry(llm_call_function, max_retries: int = 3) -> Optional[EventExtraction]:\n",
    "    \"\"\"Try to extract valid data, retrying with error feedback if validation fails.\"\"\"\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm_call_function(last_error)\n",
    "            data = json.loads(response)\n",
    "            return EventExtraction(**data)\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            last_error = str(e)\n",
    "            print(f\"Attempt {attempt + 1} failed: {last_error}\")\n",
    "            \n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"Max retries reached, giving up\")\n",
    "                return None\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Attempt {attempt + 1}: Invalid JSON\")\n",
    "            last_error = \"The response was not valid JSON. Please return only valid JSON.\"\n",
    "            \n",
    "            if attempt == max_retries - 1:\n",
    "                return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031d04b-6548-4c01-b1d1-81a21310c697",
   "metadata": {},
   "source": [
    "Each retry includes the previous error message, helping the LLM understand what went wrong. After `max_retries`, the function returns `None` instead of crashing, allowing the calling code to handle the failure gracefully. Printing each attempt’s error makes it easy to debug why extraction is failing.\n",
    "\n",
    "In a real application, your `llm_call_function` would construct a new prompt including the Pydantic error message, like \"`Previous attempt failed with error: {error}. Please fix and try again.`\"\n",
    "\n",
    "This example shows the retry pattern with a mock LLM function that progressively improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "363132e3-b897-47ee-941a-db3d11fcb946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: 2 validation errors for EventExtraction\n",
      "attendees\n",
      "  Field required [type=missing, input_value={'event_name': 'Tech Conf...ation': 'San Francisco'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "event_type\n",
      "  Field required [type=missing, input_value={'event_name': 'Tech Conf...ation': 'San Francisco'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "Attempt 2 failed: 1 validation error for EventExtraction\n",
      "attendees\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='about 500', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n",
      "Attempt 3 failed: 1 validation error for EventExtraction\n",
      "attendees\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='about 500', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/int_parsing\n",
      "Max retries reached, giving up\n",
      "Failed to extract valid data\n"
     ]
    }
   ],
   "source": [
    "def mock_llm_call(previous_error: Optional[str] = None) -> str:\n",
    "    \"\"\"Simulate an LLM that improves based on error feedback.\"\"\"\n",
    "    \n",
    "    if previous_error is None:\n",
    "        return '{\"event_name\": \"Tech Conference 2024\", \"date\": \"2024-06-15\", \"location\": \"San Francisco\"}'\n",
    "    elif \"attendees\" in previous_error.lower():\n",
    "        return '{\"event_name\": \"Tech Conference 2024\", \"date\": \"2024-06-15\", \"location\": \"San Francisco\", \"attendees\": \"about 500\", \"event_type\": \"Conference\"}'\n",
    "    else:\n",
    "        return '{\"event_name\": \"Tech Conference 2024\", \"date\": \"2024-06-15\", \"location\": \"San Francisco\", \"attendees\": 500, \"event_type\": \"Conference\"}'\n",
    "\n",
    "\n",
    "result = extract_with_retry(mock_llm_call)\n",
    "\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nSuccess! Extracted event: {result.event_name}\")\n",
    "    print(f\"Expected attendees: {result.attendees}\")\n",
    "else:\n",
    "    print(\"Failed to extract valid data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25271db0-a144-4fac-ac3e-ce1e3dfcec29",
   "metadata": {},
   "source": [
    "The first attempt misses the required attendees field, the second attempt includes it but with the wrong type, and the third attempt gets everything correct. The retry mechanism handles these progressive improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4634a1f1-e298-4226-a63a-f8e62e22c448",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Conclusion</font></b>\n",
    "<font size='3ptx'><b>[Pydantic](https://docs.pydantic.dev/latest/) helps you go from unreliable LLM outputs into validated, type-safe data structures.</b> By combining clear schemas with robust error handling, you can build AI-powered applications that are both powerful and reliable.\n",
    "\n",
    "Here are the key takeaways:\n",
    "* Define clear schemas that match your needs\n",
    "* Validate everything and handle errors gracefully with retries and fallbacks\n",
    "* Use type hints and validators to enforce data integrity\n",
    "* Include schemas in your prompts to guide the LLM\n",
    "\n",
    "Start with simple models and add validation as you find edge cases in your LLM outputs. Happy exploring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
