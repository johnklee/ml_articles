{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Section7 - Machine Learning Basics</font>\n",
    "This notebook is created from udemy course \"<b>[Deep Learning Prerequisites: The Numpy Stack in Python](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/)</b>\" which introducts Numpy, Scipy, Pandas, and Matplotlib: prep for deep learning, machine learning, and artificial intelligence.\n",
    "\n",
    "## <font color='darkblue'>Section Introduction</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643806#overview))<br/>\n",
    "\n",
    "### <font color='darkgreen'>New Section: Machine Learning</font>\n",
    "* We'll even apply a deep neural network\n",
    "* Think of this like a machine learning mini course\n",
    "* This section not originally part of this course, why?\n",
    "    * Learn to add and multiply before learning calculus\n",
    "    * What order should I take your course in?\n",
    "    \n",
    "![neural network](images/S7_1.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>Problem</font>\n",
    "Those who want to market ML do the opposite:\n",
    "* Make machine learning sound as complex & magical as possible\n",
    "* Give you a basic, but non-mathematical explanation - to reveal the magic and make it as if you've learned a clever trick\n",
    "* The more high level and multive-sounding, the more you feel you've accomplished.\n",
    "* 2 or 3 lines of code to use an API: the matic spell. But you still don't know how the spell works.\n",
    "\n",
    "### <font color='darkgreen'>The realistic approach</font>\n",
    "* Instead of making ML try to sound magical, let's make it sound as dumb as possible.\n",
    "* Demonstrate that ML is no more than a geometry problem - this is real intuition\n",
    "* Instead of matic, use spatial/mathematical reasoning.\n",
    "\n",
    "### <font color='darkgreen'>Numpy's brothers and sisters</font>\n",
    "* Numpy\n",
    "* Scipy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Scikit-learn\n",
    "* (and more...)\n",
    "\n",
    "## <font color='darkblue'>What's Classification</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643808#overview)) <br/>\n",
    "* Classic machine learning benchmark\n",
    "* Given: input image -> predict: what digit:\n",
    "![neural network](images/S7_2.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>2 Common types of data</font>\n",
    "* Images (ex. MINST) -- computer vision\n",
    "* Text (ex. emails) -- natural language processing (NLP)\n",
    "* The entire Internet is (mostly) made up of these 2 things.\n",
    "\n",
    "### <font color='darkgreen'>Text Example: Spam Detection</font>\n",
    "* Input: email, predict: spam or not spam\n",
    "\n",
    "### <font color='darkgreen'>Common Theme</font>\n",
    "![neural network](images/S7_3.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>Learning</font>\n",
    "* How does the machine learning model learn to make correct predictions?\n",
    "* We're given a dataset on which to train\n",
    "* It consists of input data and targets (aka labels)\n",
    "* Learning: given many examples, figure out the \"pattern\"\n",
    "\n",
    "### <font color='darkgreen'>Code</font>\n",
    "Remember, we have to do 2 things: learn and make new predicitons:\n",
    "```python\n",
    "import RandomForestClassifier from sklearn\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, Y) # learning\n",
    "predictions = model.predict(X) # make predictions\n",
    "```\n",
    "\n",
    "### <font color='darkgreen'>Evaluation</font>\n",
    "* How do we know the model have learned something useful?\n",
    "* Measure its accuracy (aka. Classification rate)\n",
    "    * Accuracy = #correct / #total\n",
    "* In code (but also wasy to do manually):\n",
    "```\n",
    "model.score(X, Y)\n",
    "```\n",
    "\n",
    "## <font color='darkblue'>Classification in code</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643814#overview))<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from future.utils import iteritems\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the data\n",
    "data = load_breast_cancer()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what fields supported in data\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 569 instances with 30 features\n",
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What feature(s) we have\n",
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target=0 -> malignant; target=1 -> benign\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X (data), y (target) \n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation: training score\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation: testing score\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making prediction\n",
    "predictions = model.predict(X_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy\n",
    "N = len(y_test)\n",
    "np.sum(predictions == y_test) / N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use DL to solve the same problem here\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pre processing\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\johnlee\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score=0.997; test_score=0.982\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "train_score, test_score = model.score(X_train_std, y_train), model.score(X_test_std, y_test)\n",
    "\n",
    "print(f\"train_score={train_score:.03f}; test_score={test_score:.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>What's Regression</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643816#overview)) <br/>\n",
    "\n",
    "### <font color='darkgreen'>Regression</font>\n",
    "* We just covered classification, which seems very intuitive.\n",
    "* Regression is also very intuitive\n",
    "\n",
    "![Regression ex](images/S7_4.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>Regression vs. Classification</font>\n",
    "* Classification: Predict a category\n",
    "* Regression: Predict a number on the real line\n",
    "* In regression, the numbers do have meaning\n",
    "\n",
    "### <font color='darkgreen'>Ex: Predicting House Prices</font>\n",
    "* Real estate application\n",
    "* Can have >1 input\n",
    "    * neighbor\n",
    "    * floor\n",
    "    * ...\n",
    "    \n",
    "\n",
    "### <font color='darkgreen'>Obligatory Business Example</font>\n",
    "![Regression ex](images/S7_5.png)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>What will the code look like?</font>\n",
    "![Regression ex](images/S7_6.png)\n",
    "<br/>\n",
    "```python\n",
    "model = LinearRegression()\n",
    "model.fit(X, y) # Learning\n",
    "predictions = model.predict(X) # make predictions\n",
    "score = model.score(X, y) # evaluation by MSE\n",
    "```\n",
    "\n",
    "### <font color='darkgreen'>Why not MSE</font>\n",
    "* House prices will range from hundreds of thousands to millions\n",
    "* Grades will range from 0-100\n",
    "* An error of $100^2$ is not bad for house prices, but it is bad for grades!\n",
    "![Regression ex](images/S7_7.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>The $R^2$</font>\n",
    "Is literally the [correlation coefficient squared](https://en.wikipedia.org/wiki/Coefficient_of_determination):\n",
    "> In statistics, the **coefficient of determination**, denoted $R^2$ or $r^2$ and pronounced \"R squared\", is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "![Regression ex](images/S7_8.png)\n",
    "<br/>\n",
    "\n",
    "We'll look at $R^2$ again in a latter course. For now, just remember that important points.\n",
    "* $R^2$=1 is the best ($SS_{res} $ = 0)\n",
    "* $R^2$=0 is the dumbest model (pick the average target, regardless of input)\n",
    "\n",
    "## <font color='darkblue'>Regression in code</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643818#overview)) <br/>\n",
    "In this lecture, we're going to look at how to do regression in code rather than file for this lecture. ([data source:airfoil_self_noise.dat](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>126.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>125.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3048</td>\n",
       "      <td>71.3</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>127.461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1       2     3         4        5\n",
       "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
       "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
       "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
       "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
       "4  2000  0.0  0.3048  71.3  0.002663  127.461"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datas/airfoil_self_noise.dat', sep='\\t', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1503 entries, 0 to 1502\n",
      "Data columns (total 6 columns):\n",
      "0    1503 non-null int64\n",
      "1    1503 non-null float64\n",
      "2    1503 non-null float64\n",
      "3    1503 non-null float64\n",
      "4    1503 non-null float64\n",
      "5    1503 non-null float64\n",
      "dtypes: float64(5), int64(1)\n",
      "memory usage: 70.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Check columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve X, y\n",
    "data = df[list(range(5))].values\n",
    "target = df[5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score=0.515; test score=0.512\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "train_score, test_score = model.score(X_train, y_train), model.score(X_test, y_test)\n",
    "print(f\"train score={train_score:.03f}; test score={test_score:.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124.37422429, 122.27092714, 123.61013951, 131.19680768,\n",
       "       132.16258229, 126.51044553, 120.38656008, 124.92217894,\n",
       "       126.23651668, 118.09571416])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "predictions = model.predict(X_test)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try other model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model2 = RandomForestRegressor()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score=0.989; test score=0.924\n"
     ]
    }
   ],
   "source": [
    "# Evaluation - the result is better than the first model\n",
    "train_score, test_score = model2.score(X_train, y_train), model2.score(X_test, y_test)\n",
    "print(f\"train score={train_score:.03f}; test score={test_score:.03f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>What is a feature vector</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643820#overview)) <br/>\n",
    "\n",
    "![Regression ex](images/S7_9.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>How can I come up with good features?</font>\n",
    "* First option - use your domain knowledge (ex. Your height won't impact your grade score)\n",
    "* Second option - A purely mathematical approach (ex. Convolution, Tylor expansion)\n",
    "* Actually, you can combine two approaches:\n",
    "![feature vector](images/S7_10.png)\n",
    "<br/>\n",
    "\n",
    "## <font color='darkblue'>ML is nothing but Geometry</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643822#overview)) <br/>\n",
    "* Machine learning isn't magic, it's just geometry!\n",
    "* We will see how this \"works\" for both classification and regression\n",
    "![feature vector](images/S7_11.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>Regression</font>\n",
    "\n",
    "### <font color='darkgreen'>Classification</font>\n",
    "\n",
    "\n",
    "## <font color='darkblue'>All data are the same</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643826#overview)) <br/>\n",
    "* The algorithm is the same no matter the data set.\n",
    "* When you plot the data into the coordination, all you need to find the best way to separate those classes (classification) or find a line to fit the trend (regression).\n",
    "\n",
    "## <font color='darkblue'>Comparing Different Machine Learning Models</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643830#overview)) <br/>\n",
    "* Which model should I choose?\n",
    "* Same lines of code no matter which model I choose\n",
    "* Shouldn't always choose the most powerful?\n",
    "* How do I know which one is the most powerful?\n",
    "\n",
    "### <font color='darkgreen'>The Approach</font>\n",
    "* This lecture: some general ideas and concepts\n",
    "* Nothing will repace learning about how these models work\n",
    "* Learn the algorithms, their pros and cons, where they fail and succeed\n",
    "\n",
    "### <font color='darkgreen'>Linear Models</font>\n",
    "* Very easy to interpret\n",
    " \n",
    "![feature vector](images/S7_12.png)\n",
    "<br/>\n",
    "\n",
    "### <font color='darkgreen'>Basic Nonolinear Models</font>\n",
    "* Don't be fooled! They are not necessarily \"better\" than a linear model\n",
    "* Example: Naive Bayes, Decision Tree, K-Lnearest Neighbor\n",
    "\n",
    "### <font color='darkgreen'>Ensemble Models</font>\n",
    "* Random Forest, AdaBoost, ExtraTrees, Gradient Boosted Trees.\n",
    "* Average the predictions from multiple trees\n",
    "* [XGBoost](https://en.wikipedia.org/wiki/XGBoost) has been used to win a significant number of Kaggle contests\n",
    "\n",
    "### <font color='darkgreen'>Supported Vector Machine (SVM)</font>\n",
    "* Was the \"go-to\" method for a long time\n",
    "* Today that is deep learning, but SVM used to beat NN\n",
    "* Powerful and nonlinear, but they do not scale\n",
    "* Most datasets these days are too large.\n",
    "\n",
    "### <font color='darkgreen'>Deep Learning</font>\n",
    "* You've only seen the tip of the iceberg (MLP)\n",
    "* State of the art in CV and NLP\n",
    "* Not \"plug-and-play\" (unlike Random Forest)\n",
    "* You souldn't normally use SKLearn.\n",
    "\n",
    "### <font color='darkgreen'>Summary</font>\n",
    "* Don't take this table as gospel\n",
    "* ML is a field of experimentation, not philosophy\n",
    "![feature vector](images/S7_13.png)\n",
    "<br/>\n",
    "\n",
    "## <font color='darkblue'>Machine Learning and Deep Learning: Future topics</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643834#overview)) <br/>\n",
    "\n",
    "### <font color='darkgreen'>Unsupervised learning</font>\n",
    "\n",
    "### <font color='darkgreen'>Reinforcement learning</font>\n",
    "\n",
    "### <font color='darkgreen'>Practical concepts</font>\n",
    "\n",
    "### <font color='darkgreen'>Hyperparameters</font>\n",
    "\n",
    "## <font color='darkblue'>Summary</font>\n",
    "([course link](https://www.udemy.com/course/deep-learning-prerequisites-the-numpy-stack-in-python/learn/lecture/19643838#overview)) <br/>\n",
    "* Understanding ML as a black box\n",
    "* Input/Output\n",
    "* What functions performs\n",
    "* In the future, implementing ML algorithms just means \"fill in the blanks\"\n",
    "\n",
    "### <font color='darkgreen'>It's not magic, it's geometry</font>\n",
    "\n",
    "### <font color='darkgreen'>All data is the same</font>\n",
    "\n",
    "### <font color='darkgreen'>Integrating ML Code</font>\n",
    "\n",
    "### <font color='darkgreen'>Comparing ML models</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
