{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c88d2cc-c6ec-4713-b869-552390c07e9d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/)) <b><font size='3ptx'>The [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/) (LCEL) is an abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.</font></b>\n",
    "\n",
    "LCEL comes with strong support for:\n",
    "1. Superfast development of chains.\n",
    "2. Advanced features such as streaming, async, parallel execution, and more\n",
    "3. Easy integration with [**LangSmith**](https://www.langchain.com/langsmith) and [**LangServe**](https://python.langchain.com/v0.2/docs/langserve/).\n",
    "\n",
    "In this article, we'll learn what LCEL is, how it works, and the essentials of LCEL chains, pipes, and Runnables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e05719-6be4-4609-9ff7-86e6b96cfeda",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LCEL Syntax</font></b>\n",
    "We'll begin by installing all of the prerequisite libraries that we'll need for this walkthrough. Note, you can follow along via our [Jupyter notebook here](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/11-langchain-expression-language.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3742a4-a448-4bd7-b2b1-10285ed0205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \\\n",
    "#    langchain \\\n",
    "#    anthropic \\\n",
    "#    cohere \\\n",
    "#    docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c10266-9b25-4197-9a66-a632545bae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7805d25d-f60c-4dcb-b315-9e58099d5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4437bcbb-87b4-441b-a767-91dcf50c46b2",
   "metadata": {},
   "source": [
    "To understand LCEL syntax let's first build a simple chain using the traditional LangChain syntax. We will initialize a simple <b><font color='blue'>LLMChain</font></b> using Claude 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e81922-ef55-4387-8d6c-d25256f7c247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2804b03-3fab-4d55-8d6e-814f4bb3cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me small report about {topic}\"\n",
    ")\n",
    "model = ChatAnthropic(\n",
    "    model=\"claude-2.1\",\n",
    "    max_tokens_to_sample=512,\n",
    "    anthropic_api_key=os.environ['ANTHROPIC_API_KEY'],\n",
    ")  # swap Anthropic for OpenAI with `ChatOpenAI` and `openai_api_key`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fec59f-5b09-4400-b703-fbffb5792775",
   "metadata": {},
   "source": [
    "Using this chain we can generate a small report about a particular topic, such as \"Artificial Intelligence\" by calling the <font color='blue'>chain.run</font> method on an [**LLMChain**](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7697646-3e94-4b74-8e80-15d4dcf26be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model_name),\n",
    "    prompt=prompt,\n",
    "    output_key=\"solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34ea9c7-e7ed-4608-be86-52aa5a69f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# and run\n",
    "out = chain.run(topic=\"Artificial Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb436fec-5222-4e5c-a55d-7cdbe746c172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a rapidly advancing field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.\n",
      "\n",
      "AI technologies are being used in a wide range of industries, including healthcare, finance, transportation, and entertainment. Some common applications of AI include virtual assistants like Siri and Alexa, self-driving cars, and recommendation systems used by companies like Amazon and Netflix.\n",
      "\n",
      "There are different types of AI, including narrow AI, which is designed to perform specific tasks, and general AI, which aims to replicate human intelligence and perform a wide range of tasks. While narrow AI is already widely used, the development of general AI is still in its early stages.\n",
      "\n",
      "Despite the many benefits of AI, there are also concerns about its potential impact on jobs, privacy, and ethics. As AI continues to advance, it will be important for policymakers, researchers, and industry leaders to work together to ensure that AI technologies are developed and used responsibly.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454cb11-8632-48bc-8031-98beaf820819",
   "metadata": {},
   "source": [
    "With [**LCEL**](https://python.langchain.com/v0.1/docs/expression_language/) we create our chain differently using pipe operators (`|`) rather than Chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67fcfabe-a1cc-4489-95fd-75e066fcb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, model=model_name)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "lcel_chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09112988-ba44-4dfb-bb34-c9b7d5af8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and run\n",
    "out = lcel_chain.invoke({\n",
    "    \"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20cac15-4d4d-4a78-b1e6-f6495c9da9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a rapidly advancing field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence. These tasks include speech recognition, decision-making, problem-solving, and learning.\n",
      "\n",
      "AI systems are designed to analyze and interpret complex data, recognize patterns, and make decisions based on that information. They can be used in a wide range of applications, from self-driving cars and virtual assistants to medical diagnosis and financial forecasting.\n",
      "\n",
      "One of the key components of AI is machine learning, which involves training algorithms to learn from data and improve their performance over time. This allows AI systems to adapt to new information and make more accurate predictions.\n",
      "\n",
      "While AI has the potential to revolutionize many industries and improve efficiency and productivity, there are also concerns about the ethical implications of AI, including issues related to privacy, bias, and job displacement.\n",
      "\n",
      "Overall, AI is a powerful technology that has the potential to transform the way we live and work, but it is important to approach its development and implementation with caution and consideration for its potential impact on society.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347d47d-a9b1-4007-b4d0-953aca593078",
   "metadata": {},
   "source": [
    "The syntax here is not typical for Python but uses nothing but native Python. <b>Our `|` operator simply takes output from the left and feeds it into the function on the right</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b49f6-5212-42cd-ab16-f7e8a6a7f895",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How the Pipe Operator Works</font></b>\n",
    "<b><font size='3ptx'>To understand what is happening with LCEL and the pipe operator we create our own pipe-compatible functions.</font></b>\n",
    "\n",
    "When the Python interpreter sees the `|` operator between two objects (<font color='brown'>like `a | b`</font>) it attempts to feed object `b` into the [\\_\\_or__](__or__) method of object `a`. That means these patterns are equivalent:\n",
    "```python\n",
    "# object approach\n",
    "chain = a.__or__(b)\n",
    "chain(\"some input\")\n",
    "\n",
    "# pipe approach\n",
    "chain = a | b\n",
    "chain(\"some input\")\n",
    "```\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2820ce63-8b17-432d-8f10-b89f71334fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectWithOr:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __or__(self, obj):\n",
    "        print(f'{self.value } Or {obj}')\n",
    "        if isinstance(obj, ObjectWithOr):\n",
    "            return ObjectWithOr(self.value + obj.value)\n",
    "        else:\n",
    "            return ObjectWithOr(self.value + obj)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cb786c2-ef06-4785-9d26-888385c285e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ObjectWithOr(1)\n",
    "b = ObjectWithOr(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a41b2a87-7fa7-4cba-b1b8-08b9d1c55004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Or 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a | b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ff678de-2814-43e5-b55a-8b7fcd717fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Or 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.__or__(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e86931-6b70-4bee-91ee-d1c831d98d34",
   "metadata": {},
   "source": [
    "With that in mind, we can build a <b><font color='blue'>Runnable</font></b> class that consumes a function and turns it into a function that can be chained with other functions using the pipe operator `|`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e8dc2e-3509-4060-b360-046b57b82b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            # the other func consumes the result of this func\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03daec-a79a-4b35-bed8-1760c1faaaa3",
   "metadata": {},
   "source": [
    "Let's implement this to take the value `3`, add `5` (<font color='brown'>giving `8`</font>), and multiply by `2` — giving us `16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6488d46-8563-4660-a945-653ab53f8c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "# wrap the functions with Runnable\n",
    "add_five = Runnable(add_five)\n",
    "multiply_by_two = Runnable(multiply_by_two)\n",
    "\n",
    "# run them using the object approach\n",
    "chain = add_five.__or__(multiply_by_two)\n",
    "chain(3)  # should return 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707246b2-ea63-4b48-beaf-e3dae001daeb",
   "metadata": {},
   "source": [
    "Using `__or__` directly we get the correct answer, let's try using the pipe operator `|` to chain them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9161146-f27b-4182-95cf-1fefee89300c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain the runnable functions together\n",
    "chain = add_five | multiply_by_two\n",
    "\n",
    "# invoke the chain\n",
    "chain(3)  # we should return 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1a300-21fa-42e5-be10-b4a555c14c9f",
   "metadata": {},
   "source": [
    "With either method, we get the same response, and at its core, this is the pipe logic that [**LCEL**](https://python.langchain.com/v0.1/docs/expression_language/) uses when chaining together components. However, this is not all there is to [**LCEL**](https://python.langchain.com/v0.1/docs/expression_language/), there is more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb95dc-964e-4953-aacd-6a4c74573e2a",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LCEL Deep Dive</font></b>\n",
    "Now that we understand what the LCEL syntax is doing under the hood, let's explore it within the context of LangChain and see a few of the additional methods that are provided to maximize flexibility when working with LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11aca9a-e579-47b5-8771-2c2d7fde28f6",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Runnables</font></b>\n",
    "<b><font size='3ptx'>When working with LCEL we may find that we need to modify the flow of values, or the values themselves as they are passed between components</font></b> — for this, we can use runnables. Let's begin by initializing a couple of simple vector store components:\n",
    "* [**API**:langchain_community.vectorstores.faiss.FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af51d615-e97b-4e4b-b9f3-f94a570a9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27669445-a278-440a-bb2b-070b8c2855dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output =  embeddings_model.embed_query('test text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0fe921-5bb8-43ce-bbf2-d1c69633dcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00817158695845937, -0.0026620087657393135, -0.0003281987911357356]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73cdcbdf-1bdc-4ad4-a151-2774735a0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_faiss_index(embeddings):\n",
    "    vector = np.array(embeddings)\n",
    "    index = faiss.IndexFlatL2(vector.shape[1])\n",
    "    index.add(vector)\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_embedding(text, embeddings_model):\n",
    "    return embeddings_model.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6acd708d-c1a2-435a-bd64-9d500d7f61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array1 = [\"half the info will be here\", \"James' birthday is the 7th December\"]\n",
    "text_array2 = [\"and half here\", \"James was born in 1994\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d0639bd-c45f-4780-9bd7-50ec10175361",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_1 = FAISS.from_texts(text_array1, embeddings_model)\n",
    "faiss_2 = FAISS.from_texts(text_array2, embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70248ba3-cde5-4e4f-b126-2f5ef55171e9",
   "metadata": {},
   "source": [
    "We're creating two local vector stores here and breaking apart two essential pieces of information between the two vector stores. We'll see why soon, but for now we only need one of these. Let's try passing a question through a RAG pipeline using `faiss_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "999c1ed5-d91f-4003-a8f2-b05d384b6bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "\n",
    "retriever_a = faiss_1.as_retriever()\n",
    "retriever_b = faiss_2.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0eafe49-499a-4557-8287-f2515c18fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\"context\": retriever_a, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb55e7c-c271-474f-b4a7-3c45057bb949",
   "metadata": {},
   "source": [
    "We use two new objects here, [**RunnableParallel**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html) and [**RunnablePassthrough**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html). The [**RunnableParallel**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html) object allows us to define multiple values and operations, and run them all in parallel. Here we call `retriever_a` using the input to our chain (below), and then pass the results from `retriever_a` to the next component in the chain via the \"`context`\" parameter.\n",
    "\n",
    "![fig1](images/fig_1.PNG)\n",
    "\n",
    "The [**RunnablePassthrough**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) object is used as a \"`passthrough`\" take takes any input to the current component (<font color='brown'>retrieval</font>) and allows us to provide it in the component output via the \"`question`\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd7a59cd-1085-4695-a1df-19b9b4e41e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chain.invoke(\"When was James born and include the date in the answer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4ba1236-ee2a-4d00-8e2a-7b52dc29f8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James was born on the 7th December.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97664a6d-bf5c-47e2-8b27-1fd17e88dbd6",
   "metadata": {},
   "source": [
    "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information (what year) that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams with the [**RunnableParallel**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb05d6e6-6dfd-42e5-a2fd-9817036ab673",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fcf17-c21a-499f-b6e9-659d9757ea9f",
   "metadata": {},
   "source": [
    "Here we're passing two sets of context to our prompt component via \"`context_a`\" and \"`context_b`\". Using this we get more information into our LLM (<font color='brown'>although oddly the LLM doesn't manage to put two-and-two together</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e128d97-5eb9-4920-a2eb-f478bf9e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chain.invoke(\"When was James born and include the date in the answer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "166883ba-2350-45a8-aec7-5c6434ed9179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James was born on the 7th December 1994.\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad24927f-d0b4-48ea-898a-bd2d24b55da9",
   "metadata": {},
   "source": [
    "Using this approach we're able to have multiple parallel executions and build more complex chains pretty easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b0bd05-c56a-471f-963b-4d33b2fdc809",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Runnable Lambdas</font></b>\n",
    "<b><font size='3ptx'>The [RunnableLambda](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html) is a LangChain abstraction that allows us to turn Python functions into pipe-compatible functions, similar to the [Runnable](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html) class we created near the beginning of this article.</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec944e6-b9d7-4e34-b946-1eabb29ee235",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Example 1: Extended prompt</font></b>\n",
    "Let's try it out with our earlier <font color='blue'>add_five</font> and <font color='blue'>multiply_by_two</font> functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaa1cc3a-3c40-4ab0-91f5-a96cb8117c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "# wrap the functions with RunnableLambda\n",
    "add_five = RunnableLambda(add_five)\n",
    "multiply_by_two = RunnableLambda(multiply_by_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495d840-69da-472f-ae84-f1aec4dcddda",
   "metadata": {},
   "source": [
    "As with our earlier [**Runnable**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html) abstraction, we can use <b><font color='orange'>|</font></b> operators to chain together our [**RunnableLambda**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html) abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2dec740-0d4b-489d-bd7b-5f3756574797",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five | multiply_by_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bca12e-d1d6-48fd-a6b0-0407673472fa",
   "metadata": {},
   "source": [
    "Unlike our [**Runnable**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html) abstraction, we cannot run the [**RunnableLambda**](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableLambda.html) chain by calling it directly, instead we must call <font color='blue'>chain.invoke</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5aede3f-01e9-4215-b2d5-a50ad5cf4510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (3 + 5) * 2 = 16\n",
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e871c-2acb-4efa-a581-65733e897d3c",
   "metadata": {},
   "source": [
    "As before, we can see the same answer. <b>Naturally, we can feed custom functions into our chains using this approach. Let's try a short chain and see where we might want to insert a custom function</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea838e3c-e9df-4514-b066-bbca48e2ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"Tell me an short fact about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16395894-5ba1-4700-a5d1-58667fce6d44",
   "metadata": {},
   "source": [
    "We can run this chain a couple of times to see what type of answers it returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8c99e3f-ba82-403c-9b35-85880dc9a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence is the simulation of human intelligence processes by machines, especially computer systems.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af591bea-f8b4-442d-93a8-be26fa698f2d",
   "metadata": {},
   "source": [
    "Assume you want to extend the prompt to indicate the LLM to use a certain tone or lines or sentences to explain the topics. You could chain another prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b83eae6d-bb92-4b0f-ba80-1e3e164ef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(data_dict):\n",
    "    role = data_dict['role']\n",
    "    num_lines = data_dict['num_lines']\n",
    "    topic = data_dict['topic']\n",
    "    context_str = f'Assume you are a {role}, please explain the topic in {num_lines} line(s).'\n",
    "    return {'context': context_str, 'topic': topic}\n",
    "\n",
    "\n",
    "context = RunnableLambda(add_context)\n",
    "\n",
    "prompt_str = \"{context}\\nTell me an short fact about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c3473d2-ad8c-4bfd-82bc-09a1ef9822db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prompt = context | prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5425565c-c74f-4189-970a-16a0fd173ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Assume you are a student, please explain the topic in 5 line(s).\\nTell me an short fact about Artificial Intelligence')])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_prompt.invoke({\"role\":\"student\", \"num_lines\":5, \"topic\":\"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71309ce-a623-4a6c-983c-25f0a92dc9f8",
   "metadata": {},
   "source": [
    "Now let's see how it works in chain with LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c40053e5-2173-4ee1-a237-7548c55c7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = context | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c0ebc04-bbd1-4f05-b1ec-71edd8bf6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain.invoke({\"role\":\"software engineer\", \"num_lines\":3, \"topic\":\"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8406729-5faf-4175-add3-c4727a0d3f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence is the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "Short fact: The term \"artificial intelligence\" was coined in 1956 by computer scientist John McCarthy.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800c588-5124-407e-bea1-b4d4ecbde6f2",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Example 2: Audio to text</font></b>\n",
    "When working with audio input, integrate an audio-to-text transcription step into your processing pipeline. This converts the audio file into its corresponding text, which can then be used as input for the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6475bda-01ea-4802-9253-154845617def",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Conclusion</font></b>\n",
    "<b>That covers the essentials you need to get started and build with the [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/)</b> (LCEL). With it, we can put together chains very easily — and the current focus of the LangChain team is on further LCEL development and support.\n",
    "\n",
    "The pros and cons of LCEL are varied. Those who love it tend to focus on the minimalist code style, LCEL's support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.\n",
    "\n",
    "<b>Some people are less fond of LCEL. These people typically point to LCEL being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against the [Zen of Python](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fpeps.python.org%2Fpep-0020%2F)</b>, and requires too much effort to learn new (or uncommon) syntax.\n",
    "\n",
    "Both viewpoints are entirely valid, LCEL is a very different approach — there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind, it's well worth learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989823b5-8da9-4d56-89a6-b696a84cab51",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [pinecone-io: learn/generation/langchain/handbook/11-langchain-expression-language.ipynb](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/11-langchain-expression-language.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
