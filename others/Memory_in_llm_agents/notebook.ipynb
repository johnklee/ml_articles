{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c714a45-2a49-44a2-a5e9-0ee36dd4f23c",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://dev.to/datalynx/memory-in-llm-agents-121)) <b><font size='3ptx'>The role of memory in LLM chats</font></b>\n",
    "\n",
    "In the [previous article](https://dev.to/fastovetsilya/using-openai-functions-with-langchain-agents-293a), we discussed how the reasoning and decision-making capabilities of LLM agents can help us solve practical tasks. In the provided example we used OpenAI LLM with function calls to create this agent. Two dummy methods were implemented to demonstrate its capabilities, namely calling them one after the other, and passing the results between one another. The scratchpad that we added to the agent helped it remember the previous steps so that the next steps could be reasoned correctly.\n",
    "\n",
    "<b>However, as the decision-making agent was integrated into the chat, the chat itself lacked memory capability.</b>\n",
    "\n",
    "What this means is that every message that you sent to the chat was treated as a new one. Let’s consider the following case. The agent's reasoning and decision-making are now part of the general dialogue with the same storyline. With each message, the chat decides whether to call a specific tool or to continue a conversation with a simple response. <b>But now the user also needs the chat to remember what the user was talking about before, to be able to adjust future responses</b>.\n",
    "\n",
    "Let’s make an example. <b>Now, you are developing a weather forecast assistant chat that can discuss the weather with you, and advise you on the driving conditions in the specified areas. Apart from being able to retrieve information about the weather, it also needs to remember what you were talking about in the previous messages. This is where the concept of memory comes into play</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df98c200-f4e9-4b03-bc6c-0bbcd2eb6c9c",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LLM memory types</font></b>\n",
    "<b><font size='3ptx'>The memory in relation to the LLM chats is basically a history of messages that is provided to the chat as the input along with the most recent user message, to generate a meaningful response</font></b>. Thus, the simplest form of memory is just the whole history of messages which is called <b><font color='darkblue'>conversation buffer memory</font></b>.\n",
    "\n",
    "<b><font color='darkred'>The problem with this type of memory is that there is a limitation on the input number of tokens that the model can handle.</font></b>\n",
    "\n",
    "Another consideration is the costs associated with each token used in the input. The simple solution is a <b><font color='darkblue'>conversation buffer window memory</font></b> type. What it does is that it only uses the latest `k` messages from the chat history. <b>The obvious problem here is that the chat would not be capable of remembering the long history of the conversation, and the memory would only be short-term</b>.\n",
    "\n",
    "This problem can be solved by another popular type of memory: <b><font color='darkblue'>conversation summary memory</font></b>. At each step, use another LLM call to summarize the most recent user message with the previous message or the previous summary. Then, the generated summary is used in the conversation in place of the actual history of the messages. <b>The generated summary is short, and now the chat is capable of having a long memory without using too many tokens. However, a major drawback of conversation summary memory is that important details can be lost</b>.\n",
    "\n",
    "To be able to handle both of these problems, a combination of the previous two approaches was introduced: a <b><font color='darkblue'>conversation summary buffer memory</font></b> type. It uses the window approach with the specified window size, but instead of deleting the messages older than the window, it triggers the summary generation for them. Thus, the summary is used for long-term memory while the actual messages are used for short-term memory. <b>This way, the chat would remember what you were talking about a long time ago, and the details of the most recent conversation. Another advantage of this approach is that the summary call is not always triggered, as it depends on the chat length. Because generating the summary is another call to the LLM, it often results in time and cost savings</b>.\n",
    "\n",
    "Other types of memory also exist, but they are more specific and less related to the weather assistant example we discussed. For the complete list of the memory types available in Langchain, refer to [this documentation page](https://python.langchain.com/docs/modules/memory/types/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ca2b8-e175-43ef-9798-c2e83df82898",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Example: a driving assistant using weather data</font></b>\n",
    "In our example, we will use OpenWeatherMap API to retrieve real-time weather data. Before the example can be run, it is necessary to register the user on [this website](https://openweathermap.org/). Then, activate the subscription and create an API key. The API key will be activated within 2 hours of the subscription. You will be asked to provide payment info, however, a free tier is available if you don’t exceed the free tier requests limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab783d4-8fca-4030-8c22-eb1c5c6ecf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.format_scratchpad import format_to_openai_function_messages\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a5bdee-fe68-4e34-be9d-514521c3fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_dotenv(find_dotenv(os.path.expanduser('~/.env'))) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6082cf-e135-4259-9c3e-1b8a73e3dcf6",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Define the method used in the agent</font></b>\n",
    "<b><font size='3ptx'>First, we define the function that we need.</font></b>\n",
    "\n",
    "Note that we are using type annotations. This will help Langchain to properly convert the Python function to Langchain Structured Tool and to represent it as an OpenAI function in OpenAI API. For convenience of our testing, we will mock the API for weather of city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e9ecaea-31b9-479a-9c2d-4c345f4c3606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_for_city(city_name: str, units: str=\"imperial\") -> dict:\n",
    "    \"\"\"\n",
    "    Fetches weather data for a specified city.\n",
    "\n",
    "    Parameters:\n",
    "    - city_name (str): The name of the city.\n",
    "    - units (str): Units of measurement. \"metric\" for Celsius, \"imperial\" for Fahrenheit.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Weather data for the city.\n",
    "    \"\"\"\n",
    "    mock_data = {\n",
    "        'London': {'coord': {'lon': -0.1257, 'lat': 51.5085}, 'weather': [{'id': 804, 'main': 'Clouds', 'description': 'overcast clouds', 'icon': '04n'}], 'base': 'stations', 'main': {'temp': 54.34, 'feels_like': 53.83, 'temp_min': 52.68, 'temp_max': 55.27, 'pressure': 1012, 'humidity': 93}, 'visibility': 10000, 'wind': {'speed': 9.22, 'deg': 200}, 'clouds': {'all': 100}, 'dt': 1707954149, 'sys': {'type': 2, 'id': 2075535, 'country': 'GB', 'sunrise': 1707895066, 'sunset': 1707930712}, 'timezone': 0, 'id': 2643743, 'name': 'London', 'cod': 200},\n",
    "        'San Francisco': {'coord': {'lon': -122.4194, 'lat': 37.7749}, 'weather': [{'id': 804, 'main': 'Clouds', 'description': 'overcast clouds', 'icon': '04d'}], 'base': 'stations', 'main': {'temp': 54.72, 'feels_like': 54.03, 'temp_min': 52.32, 'temp_max': 57.88, 'pressure': 1015, 'humidity': 88}, 'visibility': 10000, 'wind': {'speed': 20, 'deg': 142, 'gust': 28.99}, 'clouds': {'all': 100}, 'dt': 1707954565, 'sys': {'type': 2, 'id': 2003880, 'country': 'US', 'sunrise': 1707922868, 'sunset': 1707961609}, 'timezone': -28800, 'id': 5391959, 'name': 'San Francisco', 'cod': 200},\n",
    "        'Las Vegas': {'coord': {'lon': -115.1372, 'lat': 36.175}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 62.74, 'feels_like': 60.22, 'temp_min': 61.11, 'temp_max': 64.83, 'pressure': 1017, 'humidity': 32}, 'visibility': 10000, 'wind': {'speed': 10, 'deg': 90, 'gust': 0}, 'clouds': {'all': 0}, 'dt': 1707954794, 'sys': {'type': 2, 'id': 2083590, 'country': 'US', 'sunrise': 1707920986, 'sunset': 1707959997}, 'timezone': -28800, 'id': 5506956, 'name': 'Las Vegas', 'cod': 200},\n",
    "        'Bakersfield': {'coord': {'lon': -119.0187, 'lat': 35.3733}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 63.07, 'feels_like': 61.81, 'temp_min': 56.88, 'temp_max': 64.15, 'pressure': 1017, 'humidity': 58}, 'visibility': 10000, 'wind': {'speed': 8.05, 'deg': 320}, 'clouds': {'all': 0}, 'dt': 1707954993, 'sys': {'type': 2, 'id': 2019205, 'country': 'US', 'sunrise': 1707921850, 'sunset': 1707960995}, 'timezone': -28800, 'id': 5325738, 'name': 'Bakersfield', 'cod': 200},\n",
    "        'Fresno': {'coord': {'lon': -119.8343, 'lat': 36.6666}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 63.03, 'feels_like': 61.39, 'temp_min': 61.45, 'temp_max': 64, 'pressure': 1019, 'humidity': 50}, 'visibility': 10000, 'wind': {'speed': 9.22, 'deg': 170}, 'clouds': {'all': 0}, 'dt': 1707954426, 'sys': {'type': 1, 'id': 4068, 'country': 'US', 'sunrise': 1707922153, 'sunset': 1707961083}, 'timezone': -28800, 'id': 5350964, 'name': 'Fresno', 'cod': 200},\n",
    "        'New York': {'coord': {'lon': -74.006, 'lat': 40.7143}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01n'}], 'base': 'stations', 'main': {'temp': 32.76, 'feels_like': 21.88, 'temp_min': 28.94, 'temp_max': 35.22, 'pressure': 1020, 'humidity': 41}, 'visibility': 10000, 'wind': {'speed': 17, 'deg': 314, 'gust': 17}, 'clouds': {'all': 0}, 'dt': 1707954762, 'sys': {'type': 2, 'id': 2008776, 'country': 'US', 'sunrise': 1707911526, 'sunset': 1707949714}, 'timezone': -18000, 'id': 5128581, 'name': 'New York', 'cod': 200},\n",
    "        # 'Las Vegas': {'coord': {'lon': -115.1372, 'lat': 36.175}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 62.73, 'feels_like': 60.21, 'temp_min': 61.11, 'temp_max': 64.83, 'pressure': 1017, 'humidity': 32}, 'visibility': 10000, 'wind': {'speed': 10, 'deg': 90, 'gust': 0}, 'clouds': {'all': 0}, 'dt': 1707954946, 'sys': {'type': 2, 'id': 2083590, 'country': 'US', 'sunrise': 1707920986, 'sunset': 1707959997}, 'timezone': -28800, 'id': 5506956, 'name': 'Las Vegas', 'cod': 200},\n",
    "        'Denver': {'coord': {'lon': -104.9847, 'lat': 39.7392}, 'weather': [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04d'}], 'base': 'stations', 'main': {'temp': 47.62, 'feels_like': 45.43, 'temp_min': 41.07, 'temp_max': 50.18, 'pressure': 1013, 'humidity': 37}, 'visibility': 10000, 'wind': {'speed': 5.01, 'deg': 0, 'gust': 11.99}, 'clouds': {'all': 75}, 'dt': 1707954392, 'sys': {'type': 2, 'id': 2004334, 'country': 'US', 'sunrise': 1707918863, 'sunset': 1707957246}, 'timezone': -25200, 'id': 5419384, 'name': 'Denver', 'cod': 200},\n",
    "        'Chicago': {'coord': {'lon': -87.65, 'lat': 41.85}, 'weather': [{'id': 803, 'main': 'Clouds', 'description': 'broken clouds', 'icon': '04n'}], 'base': 'stations', 'main': {'temp': 41.77, 'feels_like': 40.37, 'temp_min': 38.17, 'temp_max': 45.18, 'pressure': 1022, 'humidity': 54}, 'visibility': 10000, 'wind': {'speed': 3, 'deg': 195, 'gust': 3}, 'clouds': {'all': 81}, 'dt': 1707955040, 'sys': {'type': 2, 'id': 2005153, 'country': 'US', 'sunrise': 1707914907, 'sunset': 1707952881}, 'timezone': -21600, 'id': 4887398, 'name': 'Chicago', 'cod': 200},\n",
    "    }\n",
    "    weather_data = mock_data.get(city_name, None)\n",
    "    if not weather_data:\n",
    "        return {\"error\": \"Failed to fetch data\", \"city_name\": city_name, \"units\": units}\n",
    "\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed4c5e-25be-427a-9f4f-02ff6f330a20",
   "metadata": {},
   "source": [
    "Let's call the function to test if it works correctly and examine what the output looks like.\n",
    "\n",
    "This is the output that the agent will use to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2182bf47-9b09-4254-b20c-53215c80c73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base': 'stations',\n",
      " 'clouds': {'all': 100},\n",
      " 'cod': 200,\n",
      " 'coord': {'lat': 51.5085, 'lon': -0.1257},\n",
      " 'dt': 1707954149,\n",
      " 'id': 2643743,\n",
      " 'main': {'feels_like': 53.83,\n",
      "          'humidity': 93,\n",
      "          'pressure': 1012,\n",
      "          'temp': 54.34,\n",
      "          'temp_max': 55.27,\n",
      "          'temp_min': 52.68},\n",
      " 'name': 'London',\n",
      " 'sys': {'country': 'GB',\n",
      "         'id': 2075535,\n",
      "         'sunrise': 1707895066,\n",
      "         'sunset': 1707930712,\n",
      "         'type': 2},\n",
      " 'timezone': 0,\n",
      " 'visibility': 10000,\n",
      " 'weather': [{'description': 'overcast clouds',\n",
      "              'icon': '04n',\n",
      "              'id': 804,\n",
      "              'main': 'Clouds'}],\n",
      " 'wind': {'deg': 200, 'speed': 9.22}}\n"
     ]
    }
   ],
   "source": [
    "city_name = \"London\"\n",
    "weather_data = get_weather_for_city(city_name)\n",
    "pprint(weather_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd4475-73c9-4d04-84da-040d644f66ba",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Define Pydantic arguments schema for these methods</font></b>\n",
    "<b><font size='3ptx'>To better convert Python functions to Langchain Tools, I found it helpful to also describe their inputs using [Pydantic](https://docs.pydantic.dev/latest/) classes.</font></b>\n",
    "\n",
    "Those will be passed together with the function as arguments to the Langchain method that creates Tools from Python functions. For some reason, Pydantic v2 is not yet supported by Langchain, note that Pydantic v1 is used here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fab53ac-3ac8-4315-9cac-c76763275a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetWeatherForCityInput(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic schema for the get_weather function inputs.\n",
    "    \"\"\"\n",
    "    city_name: str = Field(\n",
    "        ...,\n",
    "        description=\"The name of the city for which to fetch weather data.\")\n",
    "    units: str = Field(\n",
    "        default=\"imperial\",\n",
    "        description=\"Units of measurement. Use 'metric' for Celsius or 'imperial' for Fahrenheit. Defaults to 'metric'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59582d93-83d1-45a2-92ca-b4966d0e0170",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Define prompts</font></b>\n",
    "<b><font size='3ptx'>We will use two input prompts: a system prompt and a user input prompt.</font></b>\n",
    "\n",
    "In this case, the system prompt describes what needs to be done, and the user initialization prompt contains the question in it. We describe in detail what needs to be done in the system prompt. Also, we will pass the chat history (memory) in the user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01565b7b-461f-4967-a798-630185849d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_init_prompt = \"\"\"\n",
    "You are a driving assistant capable of accessing weather data in any location. \n",
    "With this weather data, you provide detailed information about how safe it would be to drive in this location.\n",
    "If two locations are provided, you also check two or three locations between them to make sure the entire road is good to drive.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "509e52df-4d56-4dda-80ae-a57941534763",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_init_prompt = \"\"\"\n",
    "Chat history is: {}.\n",
    "The question is: {}. \n",
    "Go!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7ef8f-65e4-4ead-bd34-400407289121",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Define parts of the agent using LCEL</font></b>\n",
    "<b><font size='3ptx'>Here, we define the parts used in the agent and create the agent and the agent executor.</font></b>\n",
    "\n",
    "First, we create the LLM object from ChatOpenAI class for OpeAI API. We pass OpenAI API key here as a parameter.\n",
    "\n",
    "<b>We then initialize the memory object to be used with the agent. We are using the conversation summary buffer memory type</b>. Note that we are using <b>GPT-3.5 instead of GPT-4 here because it is faster, cheaper, and good enough for text summarization tasks</b>. Also, we set <b>max token limit to 1024 tokens. This parameter determines the messages history length before we start summarizing the messages. Increase it to have a longer window of the memory, and decrease it to shorten it</b>.\n",
    "\n",
    "Then, we create a tools list from the Python function. Here, we use a method from [**StructuredTool**](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.StructuredTool.html) to create the **Tools**. The Tools are combined in a list, and then <font color='blue'>bind()</font> method is used to add them to the LLM object that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac02460-ec00-4ab3-bfcd-ee253aea2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.5,\n",
    "    model_name=\"gpt-4\",\n",
    ")\n",
    "\n",
    "# Initialize the memory: conversation summary buffer\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\", # Use a cheaper model to summarize the history\n",
    "    ),\n",
    "    memory_key=\"chat_history\", # What dict key to use to parse in the agent\n",
    "    return_messages=True,\n",
    "    max_token_limit=1024, # The bigger the limit, the more unsummarized messages\n",
    ")\n",
    "\n",
    "# Initialize the tools\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=get_weather_for_city,\n",
    "        args_schema=GetWeatherForCityInput,\n",
    "        description=\"Function to get weather for specified city.\",\n",
    "    ), \n",
    "]\n",
    "llm_with_tools = llm.bind(\n",
    "    functions=[convert_to_openai_function(t) for t in tools]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad0a97-2056-46e4-885e-fec42a10d4ad",
   "metadata": {},
   "source": [
    "In the next step, we initialize the prompt object from the prompt messages that we defined above. It contains the system prompt and a formatted user init prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73b73660-e452-4b61-bb72-a42771c7dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_init_prompt),\n",
    "        (\"user\", user_init_prompt.format(\"{chat_history}\", \"{input}\")),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a4416-9e50-44fb-b8cf-38d3cd59b4cf",
   "metadata": {},
   "source": [
    "The agent is defined using LCEL, which is a recommended way to define chains and agents in Langchain. This [article](https://python.langchain.com/docs/expression_language/why) describes why. <b>The agent combines input formatting, prompt, llm with tools, and a parser</b>. In the case of OpenAI function, it is convenient to use [**OpenAIFunctionsAgentOutputParser**](https://api.python.langchain.com/en/latest/agents/langchain.agents.output_parsers.openai_functions.OpenAIFunctionsAgentOutputParser.html) right out of the box, as we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "568074ab-bff1-4537-b132-877293ab1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_function_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adff1b5-48c9-4ef3-ae34-9e64632c7369",
   "metadata": {},
   "source": [
    "Finally, we initialize the agent executor and set verbose to True to display intermediate steps. This will help us to understand how reasoning works in Langchain Agents. The agent executor is now initialized with the `memory` parameter that automatically loads, parses, and updates memory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05434cec-f624-42c7-ac73-fe9013faf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    memory=memory,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b3f72-49fe-45f6-a5a5-51ddb395f253",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Run the chat with the agent executor</font></b>\n",
    "<b><font size='3ptx'>Let's run the chat to see how it works.</font></b>\n",
    "\n",
    "We start by asking for the driving recommendations for the trip from San Francisco to Las Vegas and get a response from the agent.\n",
    "\n",
    "Then, we tell it that we would like to proceed to New York. If the memory works correctly, the chat assistant should remember the previous conversation and provide driving recommendations from Las Vegas to New York.\n",
    "\n",
    "Finally, we confirm that the memory is working correctly by asking to summarize the whole trip. We type 'exit' to leave the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5d3935c-f11b-41f5-a38b-788faf2ccee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the chatbot. Type \"exit\" to leave the chat.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  I live in New York and my name is John.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_weather_for_city` with `{'city_name': 'New York'}`\n",
      "\n",
      "\n",
      "\u001b[0mTest: city_name: New York, units: imperial\n",
      "\u001b[36;1m\u001b[1;3m{'coord': {'lon': -74.006, 'lat': 40.7143}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01n'}], 'base': 'stations', 'main': {'temp': 32.76, 'feels_like': 21.88, 'temp_min': 28.94, 'temp_max': 35.22, 'pressure': 1020, 'humidity': 41}, 'visibility': 10000, 'wind': {'speed': 17, 'deg': 314, 'gust': 17}, 'clouds': {'all': 0}, 'dt': 1707954762, 'sys': {'type': 2, 'id': 2008776, 'country': 'US', 'sunrise': 1707911526, 'sunset': 1707949714}, 'timezone': -18000, 'id': 5128581, 'name': 'New York', 'cod': 200}\u001b[0m\u001b[32;1m\u001b[1;3mHello John, currently in New York, the weather is clear with a temperature of 32.76°F. The wind speed is 17 mph. The visibility is good at 10000 meters. Please remember to adhere to traffic rules and drive safely.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Chatbot: Hello John, currently in New York, the weather is clear with a temperature of 32.76°F. The wind speed is 17 mph. The visibility is good at 10000 meters. Please remember to adhere to traffic rules and drive safely.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Do you know my name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes, your name is John.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Chatbot: Yes, your name is John.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting chat. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "print('Welcome to the chatbot. Type \"exit\" to leave the chat.')\n",
    "\n",
    "while True:\n",
    "    user_message = input('You: ')\n",
    "    if user_message.lower() in {\"exit\", 'quit', 'q', ''}:\n",
    "        print('Exiting chat. Have a great day!')\n",
    "        break\n",
    "\n",
    "    response = agent_executor.invoke({'input': user_message})\n",
    "    response = response.get('output')\n",
    "\n",
    "    print(f'Chatbot: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54060baf-3aa8-4d17-a48e-2188f10b6426",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Conclusion</font></b>\n",
    "<b><font size='3ptx'>We have created an OpenAI LLM agent capable of invoking the function to retrieve weather data, to provide driving assistance for the user</font></b>. Different types of LLM memory have been considered, and conversation summary buffer memory was integrated into the example. As a result, the chat now has an efficient memory, at a lower cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b8726-375b-4ed1-81b2-caa4eaef5971",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Linkedin - LLM Development: LangChain's Memory Types and their Applications for Chatbots](https://www.linkedin.com/pulse/llm-development-langchains-memory-types-applications-chatbots-h--jsvne/)\n",
    "* [Deeplearning.ai - Langchain Ch3: Memory](https://github.com/johnklee/ml_articles/blob/master/deeplearning_ai/langchain/ch3_memory.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
