{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c86127-9d2b-4b7f-9086-e0424ae0df5f",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://nanonets.com/blog/langchain/)) <font size='3ptx'>At its core, [**LangChain**](https://www.langchain.com/) is an innovative framework tailored for crafting applications that leverage the capabilities of language models</font>. It's a toolkit designed for developers to create applications that are context-aware and capable of sophisticated reasoning. <b>This framework is highly relevant when discussing [Retrieval-Augmented Generation](https://nanonets.com/blog/what-is-retrieval-augmented-generation-rag/) (RAG), a concept that enhances the effectiveness of language models in generating responses based on retrieved data</b>.\n",
    "\n",
    "<b>This means LangChain applications can understand the context, such as prompt instructions or content grounding responses and use [**Large Language Models**](https://nanonets.com/blog/what-are-large-language-models/) for complex reasoning tasks, like deciding how to respond or what actions to take</b>. LangChain represents a unified approach to developing intelligent applications, simplifying the journey from concept to execution with its diverse components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e68a801f-69b4-4807-adf9-53d748b8bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd4f2b-1ca7-4432-b270-33425e8eb7f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Understanding LangChain</font></b>\n",
    "[**LangChain**](https://python.langchain.com/docs/tutorials/) is much more than just a framework; it's a full-fledged ecosystem comprising several integral parts ([more](https://python.langchain.com/v0.2/docs/concepts/)): \n",
    "* **Firstly, there are the <font color='darkblue'>LangChain Libraries</font>, available in both Python and JavaScript**. These libraries are the backbone of LangChain, offering interfaces and integrations for various components. They provide a basic runtime for combining these components into cohesive chains and agents, along with ready-made implementations for immediate use.\n",
    "* **Next, we have <font color='darkblue'>[LangChain Templates](https://templates.langchain.com/)</font>**. These are a collection of deployable reference architectures tailored for a wide array of tasks. Whether you're building a chatbot or a complex analytical tool, these templates offer a solid starting point. If you're looking for insights into how LangChain can integrate with other systems, including leveraging powerful tools like LLamaIndex, this blog will be of great help.\n",
    "* **<font color='darkblue'>LangServe</font> steps in as a versatile library for deploying LangChain chains as REST APIs**. This tool is essential for turning your LangChain projects into accessible and scalable web services.\n",
    "* **Lastly, <font color='darkblue'>LangSmith</font> serves as a developer platform**. It's designed to debug, test, evaluate, and monitor chains built on any LLM framework. The seamless integration with LangChain makes it an indispensable tool for developers aiming to refine and perfect their applications. For more information on how you can streamline and automate your workflows with LLMs, check out our post on Leveraging LLMs to Streamline and Automate Your Workflows.\n",
    "\n",
    "![components](https://python.langchain.com/v0.2/svg/langchain_stack_062024.svg)\n",
    "\n",
    "<b>Together, these components empower you to develop, productionize, and deploy applications with ease</b>. With LangChain, you start by writing your applications using the libraries, referencing templates for guidance. LangSmith then helps you in inspecting, testing, and monitoring your chains, ensuring that your applications are constantly improving and ready for deployment. Finally, with LangServe, you can easily transform any chain into an API, making deployment a breeze.\n",
    "\n",
    "<b>In the next sections, we will delve deeper into how to set up LangChain and begin your journey in creating intelligent, language model-powered applications.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06551ecb-c3f6-4210-9627-127f3fbd8ad3",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Installation and Setup</font></b>\n",
    "<b>Are you ready to dive into the world of LangChain? Setting it up is straightforward, and this guide will walk you through the process step-by-step.</b>\n",
    "\n",
    "The first step in your LangChain journey is to install it. You can do this easily using pip or conda. Run the following command in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9642554c-dd86-4023-8819-3721c5003aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain==0.3.17\n",
      "langchain-anthropic==0.2.1\n",
      "langchain-cli==0.0.35\n",
      "langchain-community==0.3.16\n",
      "langchain-core==0.3.33\n",
      "langchain-experimental==0.0.62\n",
      "langchain-google-genai==2.0.9\n",
      "langchain-google-vertexai==1.0.9\n",
      "langchain-groq==0.1.3\n",
      "langchain-openai==0.3.3\n",
      "langchain-text-splitters==0.3.5\n",
      "langchainhub==0.1.21\n",
      "openinference-instrumentation-langchain==0.1.27\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain\n",
    "!pip freeze | grep 'langchain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca670-a30b-4b15-8c4f-b02a6750458a",
   "metadata": {},
   "source": [
    "LangChain CLI is a handy tool for working with LangChain templates and LangServe projects. To install the LangChain CLI, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661ab748-2c01-4a4e-940f-954c1b2c99ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-cli==0.0.35\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-cli\n",
    "!pip freeze | grep 'langchain-cli'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e13b5-6569-4a1a-816f-e17dd9dd09b9",
   "metadata": {},
   "source": [
    "LangServe is essential for deploying your LangChain chains as a REST API. It gets installed alongside the LangChain CLI.\n",
    "\n",
    "LangChain often requires integrations with model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs. Install the OpenAI Python package using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38235659-efc5-4bfb-b977-0cf27d1a0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-openai==0.3.3\n",
      "llama-index-agent-openai==0.4.0\n",
      "llama-index-embeddings-openai==0.3.1\n",
      "llama-index-llms-openai==0.3.2\n",
      "llama-index-multi-modal-llms-openai==0.3.0\n",
      "llama-index-program-openai==0.3.1\n",
      "llama-index-question-gen-openai==0.3.0\n",
      "openai==1.60.2\n"
     ]
    }
   ],
   "source": [
    "# pip install -U openai\n",
    "!pip freeze | grep 'openai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5e44ad-6c28-4c3f-8c33-a637fdf58753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-google-genai==2.0.9\n"
     ]
    }
   ],
   "source": [
    "# pip install -q --upgrade google-generativeai langchain-google-genai python-dotenv\n",
    "!pip freeze | grep 'langchain-google-genai'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28800c6d-20c8-4125-a19e-0355bfb80f7e",
   "metadata": {},
   "source": [
    "To access the API, set your OpenAI API key as an environment variable:\n",
    "```shell\n",
    "$ export OPENAI_API_KEY=\"your_api_key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bae00794-6c7a-43bd-9de5-35554ee2c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['ALLOW_RESET'] = 'TRUE'\n",
    "_ = load_dotenv(find_dotenv(os.path.expanduser('~/.env')))\n",
    "assert os.environ.get('OPENAI_API_KEY', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672eb6f8-7f3d-4389-be50-cee1fda80333",
   "metadata": {},
   "source": [
    "<b>LangChain allows for the creation of language model applications through modules</b>. These modules can either stand alone or be composed for complex use cases. These modules are:\n",
    "* **Model I/O**: Facilitates interaction with various language models, handling their inputs and outputs efficiently.\n",
    "* **Retrieval**: Enables access to and interaction with application-specific data, crucial for dynamic data utilization.\n",
    "* **Agents**: Empower applications to select appropriate tools based on high-level directives, enhancing decision-making capabilities.\n",
    "* **Chains**: Offers pre-defined, reusable compositions that serve as building blocks for application development.\n",
    "* **Memory**: Maintains application state across multiple chain executions, essential for context-aware interactions.\n",
    "\n",
    "Each module targets specific development needs, making LangChain a comprehensive toolkit for creating advanced language model applications.\n",
    "\n",
    "Along with the above components, we also have <b><font color='darkblue'>LangChain Expression Language</font> (LCEL), which is a declarative way to easily compose modules together, and this enables the chaining of components using a universal Runnable interface</b>.\n",
    "\n",
    "LCEL looks something like this:\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Example chain\n",
    "chain = ChatPromptTemplate() | ChatOpenAI() | CustomOutputParser()\n",
    "```\n",
    "\n",
    "Now that we have covered the basics, we will continue on to:\n",
    "* Dig deeper into each Langchain module in detail.\n",
    "* Learn how to use LangChain Expression Language.\n",
    "* Explore common LangChain examples and implement them.\n",
    "* Explore common use cases\n",
    "* Deploy an end-to-end application with LangServe.\n",
    "* Check out LangSmith for debugging, testing, and monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b2b06e-16b4-4ff3-af3c-87129adce404",
   "metadata": {},
   "source": [
    "<a id='agenda'></a>\n",
    "### <b><font color='darkgreen'>Agenda</font></b>\n",
    "* <font size='3ptx'>[**Module I : Model I/O**](#sect_1)</font>\n",
    "* <font size='3ptx'>[**Module II : Retrieval**](#sect_2)</font>\n",
    "* <font size='3ptx'>[**Module III : Agents**](#sect_3)</font>\n",
    "* <font size='3ptx'>[**Module IV : Chains**](#sect_4)</font>\n",
    "* <font size='3ptx'>[**Module V : Memory**](#sect_5)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486f6d76-09f4-4db7-92f5-f670cfd123b3",
   "metadata": {},
   "source": [
    "<a id='sect_1'></a>\n",
    "## <b><font color='darkblue'>Module I : Model I/O</font></b>\n",
    "<b><font size='3ptx'>In LangChain, the core element of any application revolves around the language model</font>. This module provides the essential building blocks to interface effectively with any language model, ensuring seamless integration and communication.</b>\n",
    "\n",
    "Key Components of Model I/O\n",
    "1. <b>LLMs and Chat Models</b> (used interchangeably)\n",
    "   * LLMs:\n",
    "      - Definition: Pure text completion models.\n",
    "      - Input/Output: Take a text string as input and return a text string as output.\n",
    "   * Chat Models\n",
    "      - Definition: Models that use a language model as a base but differ in input and output formats.\n",
    "      - Input/Output: Accept a list of chat messages as input and return a Chat Message.\n",
    "2. **Prompts**: Templatize, dynamically select, and manage model inputs. Allows for the creation of flexible and context-specific prompts that guide the language model's responses.\n",
    "3. **Output Parsers**: Extract and format information from model outputs. Useful for converting the raw output of language models into structured data or specific formats needed by the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cad98-7bf6-4e29-8f70-01983c9868bf",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>[LLMs](https://nanonets.com/blog/what-are-large-language-models/)</font></b>\n",
    "<b><font size='3ptx'>LangChain's integration with Large Language Models (LLMs) like OpenAI, Cohere, and Hugging Face is a fundamental aspect of its functionality</font>. LangChain itself does not host LLMs but offers a uniform interface to interact with various LLMs.</b>\n",
    "\n",
    "This section provides an overview of using the OpenAI LLM wrapper in LangChain ([details](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)), applicable to other LLM types as well. We have already installed this in the \"Getting Started\" section. Let us initialize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b1a961-86d5-4994-832c-3d2bc0df8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd322d-8e06-4d62-ba1f-b9f702b39f66",
   "metadata": {},
   "source": [
    "LLMs implement the [**Runnable interface**](https://python.langchain.com/docs/expression_language/interface), the basic building block of the LangChain Expression Language (LCEL). This means they support:\n",
    "* [**`Invoked`**](https://python.langchain.com/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n",
    "* [**`Batched`**](https://python.langchain.com/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n",
    "* [**`Streamed`**](https://python.langchain.com/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n",
    "* **`Inspected`**: Schematic information about Runnable's input, output, and configuration can be accessed.\n",
    "* **`Composed`**: Multiple Runnables can be composed to work together using [**the LangChain Expression Language**](https://python.langchain.com/docs/concepts/lcel/) (LCEL) to create complex pipelines.\n",
    "\n",
    "LLMs accept **strings** as inputs, or objects which can be coerced to string prompts, including\n",
    "* **List[BaseMessage]**\n",
    "* **PromptValue**\n",
    "* **More**\n",
    "\n",
    "Let us look at some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12be3682-cfc8-4c97-b407-4dc44f9f6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"List the seven wonders of the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee974b2-a4de-4e63-b0f0-6715386e7593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "1. The Great Wall of China\n",
       "2. The Taj Mahal\n",
       "3. The Colosseum\n",
       "4. The Christ the Redeemer statue\n",
       "5. Machu Picchu\n",
       "6. The Great Pyramid of Giza\n",
       "7. Chichen Itza"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68594a0-15a3-447e-b2c2-b16c766af3e8",
   "metadata": {},
   "source": [
    "You can alternatively call the stream method to stream the text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c3dc93-625c-48cd-9756-5d3586b2cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The 2012 Olympics were held in London, England."
     ]
    }
   ],
   "source": [
    "response_chunks = []\n",
    "for chunk in llm.stream(\"Where were the 2012 Olympics held?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    response_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220e7f27-6d15-4fd9-affa-fcb0e4a7a9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The 2012 Olympics were held in London, England."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(''.join(response_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54619967-539c-4a5a-8456-ef5fbdef3597",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Chat Models</font></b>\n",
    "<b><font size='3ptx'>LangChain's integration with [chat models](https://python.langchain.com/docs/integrations/chat/), a specialized variation of language models, is essential for creating interactive chat applications</font>. While they utilize language models internally, chat models present a distinct interface centered around chat messages as inputs and outputs</b>. \n",
    "\n",
    "This section provides a detailed overview of using Gemini's chat model ([**ChatGoogleGenerativeAI**](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)) in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3fa08e-e813-48f9-bc43-32d4bfb61d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fb5277-a349-4e4b-ba75-ce74caba265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33025bfb-6048-4063-b09d-61daf46cc3f7",
   "metadata": {},
   "source": [
    "Chat models in LangChain work with different message types such as:\n",
    "* [**AIMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html): AIMessage is returned from a chat model as a response to a prompt.\n",
    "* [**HumanMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html): HumanMessages are messages that are passed in from a human to the model\n",
    "* [**SystemMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html): Message for priming AI behavior.\n",
    "* [**FunctionMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.function.FunctionMessage.html): Message for passing the result of executing a tool back to a model.\n",
    "* [**ChatMessage**](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.chat.ChatMessage.html): Message that can be assigned an arbitrary speaker (i.e. role).\n",
    "* [**More**](https://python.langchain.com/api_reference/core/messages.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a66b13d-e610-41dd-a018-55578abf3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Micheal Jordan.\"),\n",
    "    HumanMessage(content=\"Which shoe manufacturer are you associated with?\"),\n",
    "]\n",
    "response = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c292066f-b10f-4205-8a47-fcff2ec5b347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nike.  And I'm incredibly proud of the Jordan Brand and its impact on the game and culture.  It's more than just shoes; it's a legacy.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d9e03-8d51-479d-82c6-d03bc8415f76",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Prompts</font></b>\n",
    "<b><font size='3ptx'>Prompts are essential in guiding language models to generate relevant and coherent outputs</font>. They can range from simple instructions to complex few-shot examples. In LangChain, handling prompts can be a very streamlined process, thanks to several dedicated classes and functions.</b>\n",
    "\n",
    "LangChain's [**PromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) class is a versatile tool for creating string prompts. It uses Python's `str.format` syntax, allowing for dynamic prompt generation. You can define a template with placeholders and fill them with specific values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c005ce1-d64d-499d-8da0-eb48f97aa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simple prompt with placeholders\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fbc1c19-61bb-4a82-9d52-06b830d87e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about robots.\n"
     ]
    }
   ],
   "source": [
    "# Filling placeholders to create a prompt\n",
    "filled_prompt = prompt_template.format(adjective=\"funny\", content=\"robots\")\n",
    "print(filled_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e57afa-3b0c-41c7-b97f-10665f020586",
   "metadata": {},
   "source": [
    "For chat models, prompts are more structured, involving messages with specific roles. LangChain offers [**ChatPromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fbcad2-dee3-4920-b6c4-3d1afbb60e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Defining a chat prompt with various roles\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f265c8ff-eee9-45a0-ad40-3ef3246a4134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful AI bot. Your name is Bob.' additional_kwargs={} response_metadata={}\n",
      "content='Hello, how are you doing?' additional_kwargs={} response_metadata={}\n",
      "content=\"I'm doing well, thanks!\" additional_kwargs={} response_metadata={}\n",
      "content='What is your name?' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "# Formatting the chat prompt\n",
    "formatted_messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "for message in formatted_messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d754c-9e11-411c-84f3-21922173802c",
   "metadata": {},
   "source": [
    "This approach allows for the creation of interactive, engaging chatbots with dynamic responses. Both [**PromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html) and [**ChatPromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) integrate seamlessly with the LangChain Expression Language (LCEL), enabling them to be part of larger, complex workflows. We will discuss more on this later.\n",
    "\n",
    "Custom prompt templates are sometimes essential for tasks requiring unique formatting or specific instructions. Creating a custom prompt template involves defining input variables and a custom formatting method. This flexibility allows LangChain to cater to a wide array of application-specific requirements. \n",
    "\n",
    "LangChain also supports few-shot prompting, enabling the model to learn from examples. This feature is vital for tasks requiring contextual understanding or specific patterns. **Few-shot prompt templates can be built from a set of examples or by utilizing an Example Selector object. Read more [here](https://python.langchain.com/docs/how_to/few_shot_examples/)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b931a117-c2a2-469f-bd61-685f55713975",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Output Parsers</font></b>\n",
    "<b><font size='3ptx'>Output parsers play a crucial role in Langchain, enabling users to structure the responses generated by language models</font>. In this section, we will explore the concept of output parsers and provide code examples using Langchain's [PydanticOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html), [SimpleJsonOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html), [CommaSeparatedListOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html), DatetimeOutputParser, and [XMLOutputParser](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.xml.XMLOutputParser.html).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64948d-a10a-4b95-bd14-53ae98e51428",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>PydanticOutputParser</font></b>\n",
    "Langchain provides the [**PydanticOutputParser**](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.pydantic.PydanticOutputParser.html) for parsing responses into Pydantic data structures. Below is a step-by-step example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5de6d332-399b-495d-b713-7927ead72a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "# Initialize the language model\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28add471-e46d-4381-90c2-1567d2ba0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure using Pydantic\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    @field_validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field_value):  # Renamed 'field' to 'field_value' for clarity\n",
    "        if field_value and field_value[-1] != \"?\": # Added check for empty string\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ce25288-da69-4eda-b8ee-830471a92b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0ff5fda-262c-4033-b324-5f2155561803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"description\": \"question to set up a joke\", \"title\": \"Setup\", \"type\": \"string\"}, \"punchline\": {\"description\": \"answer to resolve the joke\", \"title\": \"Punchline\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44467140-e9b3-4775-8fdf-db425fe554f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with format instructions\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fdef4e7-25d3-4f75-9292-81b0d2bb4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query to prompt the language model\n",
    "query = \"Tell me a joke.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afb17ece-3671-4814-ae36-b4d774a485c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine prompt, model, and parser to get structured output\n",
    "prompt_and_model = prompt | model\n",
    "output = prompt_and_model.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a002777-5226-442b-8402-3c619f4244f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "    \"setup\": \"Why did the tomato turn red?\",\n",
      "    \"punchline\": \"Because it saw the salad dressing!\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9522a06-0640-4e6f-9d40-9a70895155f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the output using the parser\n",
    "parsed_result = parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a731f38d-b210-42ca-94a5-e8c068ad274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the tomato turn red?' punchline='Because it saw the salad dressing!'\n"
     ]
    }
   ],
   "source": [
    "# The result is a structured object\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c5c46a-0025-49c7-9b15-bcc6319256ce",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>SimpleJsonOutputParser</font></b>\n",
    "Langchain's [**SimpleJsonOutputParser**](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.SimpleJsonOutputParser.html) is used when you want to parse JSON-like outputs. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d776fbc1-c8d1-4710-a925-0019210db17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "# Create a JSON prompt\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with `birthdate` and `birthplace` key that answers the following question: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8459b3fe-08bf-4da6-a42c-739fda592db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the JSON parser\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "# Create a chain with the prompt, model, and parser\n",
    "json_chain_no_parser = json_prompt | model\n",
    "json_chain = json_prompt | model | json_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e30f4d3-5908-4d89-997b-7ae75a9dd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_resp = json_chain_no_parser.invoke({\"question\": \"When and where was Elon Musk born?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e42e16c8-7256-42dc-bda0-f80c00ea4d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "\n",
      "{\n",
      "  \"birthdate\": \"June 28, 1971\",\n",
      "  \"birthplace\": \"Pretoria, South Africa\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(text_resp.__class__)\n",
    "print(text_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e62e76bb-36e0-474f-ae45-cae289b3f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream through the results\n",
    "json_resp = json_chain.invoke({\"question\": \"When and where was Elon Musk born?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86d0faa4-0779-492d-8c41-0a0d4b3a561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'birthdate': 'June 28, 1971', 'birthplace': 'Pretoria, South Africa'}\n"
     ]
    }
   ],
   "source": [
    "print(json_resp.__class__)\n",
    "print(json_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bdbcd-967f-4628-a0db-a198f1b2188f",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>CommaSeparatedListOutputParser</font></b>\n",
    "The [**CommaSeparatedListOutputParser**](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.list.CommaSeparatedListOutputParser.html) is handy when you want to extract comma-separated lists from model responses. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "753eb33f-a279-440c-a952-ca9b8fd3387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Create format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a prompt to request a list\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c3f93f8-db1a-4b77-b320-80bb5f13301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query to prompt the model\n",
    "query = \"English Premier League Teams\"\n",
    "\n",
    "# Generate the output\n",
    "output = model.invoke(prompt.format(subject=query))\n",
    "\n",
    "# Parse the output using the parser\n",
    "parsed_result = output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c83ba4c2-1894-49c7-96d9-1758ebff9818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['1. Manchester City', '2. Manchester United', '3. Liverpool', '4. Chelsea', '5. Arsenal']\n"
     ]
    }
   ],
   "source": [
    "# The result is a list of items\n",
    "print(parsed_result.__class__)\n",
    "print(parsed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679ec59-5232-402e-9302-cc5d5a4b8228",
   "metadata": {},
   "source": [
    "These examples showcase how Langchain's output parsers can be used to structure various types of model responses, making them suitable for different applications and formats. Output parsers are a valuable tool for enhancing the usability and interpretability of language model outputs in Langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33982b9b-8da6-4eab-84c9-b08f56d4b9ca",
   "metadata": {},
   "source": [
    "<a id='sect_2'></a>\n",
    "## <b><font color='darkblue'>Module II : Retrieval</font></b>\n",
    "<b><font size='3ptx'>Retrieval in LangChain plays a crucial role in applications that require user-specific data, not included in the model's training set</font>. This process, known as [**Retrieval Augmented Generation**](https://python.langchain.com/docs/tutorials/rag/) (RAG), involves fetching external data and integrating it into the language model's generation process.</b>\n",
    "\n",
    "<b>LangChain provides a comprehensive suite of tools and functionalities to facilitate this process</b>, catering to both simple and complex applications. LangChain achieves retrieval through a series of components which we will discuss one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a57c8f-aa89-437d-8233-06f7d05c81d5",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Document Loaders</font></b>\n",
    "<b><font size='3ptx'>[Document loaders](https://python.langchain.com/docs/how_to/document_loader_custom/#overview) in LangChain enable the extraction of data from various sources</font>. With over 100 loaders available, they support a range of document types, apps and sources (private s3 buckets, public websites, databases)</b>.\n",
    "\n",
    "You can choose a document loader based on your requirements [here](https://python.langchain.com/docs/integrations/document_loaders/).\n",
    "\n",
    "<b>All these loaders ingest data into Document classes. We'll learn how to use data ingested into [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html) classes later.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036a12a-d7be-4d5a-8dcb-2c2fa02ffbd9",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Text File Loader</font></b>\n",
    "[**TextLoader**](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html) loads a simple `.txt` file into a document. e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cda7003-bd43-470b-a147-9971218098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"test_data/sample.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f88e673-3a1e-40f5-b620-6d9773e5364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_data/sample.txt'}, page_content='Hello world! Hi John.\\n')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d601f19-0307-4e09-9f8b-6fcc31bef35e",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>CSV Loader</fnot></b>\n",
    "[**CSVLoader**](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html) loads a CSV file into a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a99687c-0283-4483-864a-23ae7160546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='./test_data/sample.csv')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14a4b22a-f8b4-40f6-8920-91ff93e12d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './test_data/sample.csv', 'row': 0}, page_content='name: John\\nage: 45'),\n",
       " Document(metadata={'source': './test_data/sample.csv', 'row': 1}, page_content='name: Mary\\nage: 32'),\n",
       " Document(metadata={'source': './test_data/sample.csv', 'row': 2}, page_content='name: Peter\\nage: 18')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d308f-72fa-46e6-8370-0d89210952dc",
   "metadata": {},
   "source": [
    "We can choose to customize the parsing by specifying field names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e08c5dc-6885-42c6-bddb-a3ae93c87f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='./test_data/sample2.csv', csv_args={\n",
    "    'delimiter': ',',\n",
    "    'quotechar': '\"',\n",
    "    'fieldnames': ['car brand', 'price']\n",
    "})\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2241f109-be27-4cde-a92a-2189a29f0f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './test_data/sample2.csv', 'row': 0}, page_content='car brand: Toyota Camry\\nprice: 25000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 1}, page_content='car brand: Honda Civic\\nprice: 23000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 2}, page_content='car brand: Ford F-150\\nprice: 40000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 3}, page_content='car brand: Chevrolet Silverado\\nprice: 42000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 4}, page_content='car brand: BMW 3 Series\\nprice: 45000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 5}, page_content='car brand: Mercedes-Benz C-Class\\nprice: 48000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 6}, page_content='car brand: Tesla Model 3\\nprice: 40000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 7}, page_content='car brand: Hyundai Elantra\\nprice: 21000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 8}, page_content='car brand: Kia Sportage\\nprice: 26000'),\n",
       " Document(metadata={'source': './test_data/sample2.csv', 'row': 9}, page_content='car brand: Jeep Wrangler\\nprice: 35000')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c07341-e342-4185-a06b-d27dbab1c83a",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>PDF Loaders</font></b>\n",
    "PDF Loaders in LangChain offer various methods for parsing and extracting content from PDF files. Each loader caters to different requirements and uses different underlying libraries. [**PyPDFLoader**](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html) is used for basic PDF parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b6e1591-dc2e-4fdb-b285-01f0425c8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"test_data/sample.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2cea494-bf19-4719-b448-ced1fd4fc0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_data/sample.pdf', 'page': 0, 'page_label': '1'}, page_content='Hello  World!  Hi  John.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd400f-513f-4f95-bcbc-478876a60419",
   "metadata": {},
   "source": [
    "Let us now proceed and learn how to use these document classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68821e23-7a5b-44bc-9a9e-b8a7c1dd56d2",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Document Transformers</font></b>\n",
    "<b><font size='3ptx'>[Document transformers](https://python.langchain.com/docs/integrations/document_transformers/) in LangChain are essential tools designed to manipulate documents, which we created in our previous subsection.</font>They are used for tasks such as splitting long documents into smaller chunks, combining, and filtering, which are crucial for adapting documents to a model's context window or meeting specific application needs.</b>\n",
    "\n",
    "<b>One such tool is the [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html), a versatile text splitter that uses a character list for splitting</b>. It allows parameters like chunk size, overlap, and starting index. Here's an example of how it's used in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0f296f8-ab9c-4a28-9974-89324bd04a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize the language model\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "long_text = model.invoke(\n",
    "    'Please generate an article to introduce LLM with length just above 2000 characthers.'\n",
    "    'The counting of character should exclude the Punctuation Marks.').strip()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "47db2e27-fd32-4c5e-bc37-61d9c479a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1356\n",
      "The field of law is constantly evolving and becoming more complex, making it essential for legal professionals to stay updated and knowledgeable. This is where a Master of Laws (LLM) degree comes into play. An LLM is a postgraduate law degree that provides specialized legal education and training to individuals who have already completed their Juris Doctor (JD) or equivalent law degree.\n",
      "\n",
      "The LLM degree is designed to enhance the legal skills and knowledge of practicing lawyers, law graduates, and professionals from other fields who wish to gain a deeper understanding of the law. It offers a wide range of specializations, including international law, corporate law, intellectual property law, human rights law, and more. This allows students to tailor their studies to their specific interests and career goals.\n",
      "\n",
      "One of the main benefits of pursuing an LLM is the opportunity to gain a deeper understanding of a particular area of law. This is especially beneficial for lawyers who want to specialize in a specific field or for those who want to transition into a new area of law. The specialized knowledge and skills gained through an LLM can open up new career opportunities and help lawyers stand out in a competitive job market.\n",
      "\n",
      "Moreover, an LLM degree can also provide a global perspective on the law. Many universities offer LLM programs that\n"
     ]
    }
   ],
   "source": [
    "print(len(long_text))\n",
    "print(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "309e032a-4ec5-4c35-ba78-669053c7527a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 18 documents generated!\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([long_text])\n",
    "print(f'Total {len(texts)} documents generated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2a47d6fb-cd94-4d40-9c5f-3e099322c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The field of law is constantly evolving and becoming more complex, making it essential for legal' metadata={'start_index': 0}\n",
      "page_content='essential for legal professionals to stay updated and knowledgeable. This is where a Master of Laws' metadata={'start_index': 77}\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03018c4-b28a-402c-a227-c962cc90a3c0",
   "metadata": {},
   "source": [
    "Another tool is the [**CharacterTextSplitter**](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html), which splits text based on a specified character and includes controls for chunk size and overlap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "601466dc-a207-4a61-9a42-a4fa8b97c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([long_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8392c413-3ad6-49d9-9a59-cd6fec4beb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The field of law is constantly evolving and becoming more complex, making it essential for legal professionals to stay updated and knowledgeable. This is where a Master of Laws (LLM) degree comes into play. An LLM is a postgraduate law degree that provides specialized legal education and training to individuals who have already completed their Juris Doctor (JD) or equivalent law degree.'\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330657d-20d2-483b-9ad2-06ec22047c21",
   "metadata": {},
   "source": [
    "The [**HTMLHeaderTextSplitter**](https://python.langchain.com/api_reference/text_splitters/html/langchain_text_splitters.html.HTMLHeaderTextSplitter.html) is designed to split HTML content based on header tags, retaining the semantic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31448795-3c5b-4455-82fc-7ede73129a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lxml==5.3.0\n"
     ]
    }
   ],
   "source": [
    "# pip install lxml\n",
    "!pip freeze | grep 'lxml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f26a2ca5-a5df-428d-80f3-468a73a0f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
    "\n",
    "html_content = model.invoke(\n",
    "    'Please generate an article to introduce LLM with length just above 3000 characthers.'\n",
    "    'Please use full HTML to wrap up this article and use header tags such as <h1>, <h2> etc to separate the sections.').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "05b8fb1f-a39f-4fbf-9061-b5422e7470df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h1>Introducing the LLM Degree: A Comprehensive Guide</h1>\n",
       "\n",
       "<p>The Master of Laws, or LLM, is a postgraduate degree that is highly sought after by law students and professionals around the world. It is a specialized program that allows students to deepen their knowledge and understanding of a specific area of law, and gain a competitive edge in the legal field. In this article, we will explore what the LLM degree is, its benefits, and how to pursue this prestigious qualification.</p>\n",
       "\n",
       "<h2>What is an LLM Degree?</h2>\n",
       "\n",
       "<p>The LLM degree is a postgraduate law degree that is typically pursued after completing a Bachelor of Laws (LLB) or Juris Doctor (JD) degree. It is an advanced program that focuses on a specific area of law, such as international law, corporate law, or intellectual property law. The duration of an LLM program can range from one to two years, depending on the country and university.</p>\n",
       "\n",
       "<p>LLM programs are offered by top law schools around the world, and they attract students from diverse backgrounds, including recent law graduates, practicing lawyers, and professionals from other fields who wish to gain a deeper understanding of the law.</p>\n",
       "\n",
       "<h2>Benefits"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "890e5882-d8c8-44b2-a593-f9777e268c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\")]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63611bef-98c0-47f8-bc1d-e04120b3be4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Introducing the LLM Degree: A Comprehensive Guide'}, page_content='The Master of Laws, or LLM, is a postgraduate degree that is highly sought after by law students and professionals around the world. It is a specialized program that allows students to deepen their knowledge and understanding of a specific area of law, and gain a competitive edge in the legal field. In this article, we will explore what the LLM degree is, its benefits, and how to pursue this prestigious qualification.'),\n",
       " Document(metadata={'Header 1': 'Introducing the LLM Degree: A Comprehensive Guide', 'Header 2': 'What is an LLM Degree?'}, page_content='The LLM degree is a postgraduate law degree that is typically pursued after completing a Bachelor of Laws (LLB) or Juris Doctor (JD) degree. It is an advanced program that focuses on a specific area of law, such as international law, corporate law, or intellectual property law. The duration of an LLM program can range from one to two years, depending on the country and university.  \\nLLM programs are offered by top law schools around the world, and they attract students from diverse backgrounds, including recent law graduates, practicing lawyers, and professionals from other fields who wish to gain a deeper understanding of the law.')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_header_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629490c-0d50-48f3-ab00-ff1212e53f36",
   "metadata": {},
   "source": [
    "LangChain also offers specific splitters for different programming languages, like the Python Code Splitter and the JavaScript Code Splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "adb4e7aa-0dc2-477e-9b74-5d1cbe30e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='def hello_world():'\n",
      "page_content='function helloWorld() {'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "python_code = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=30, chunk_overlap=5\n",
    ")\n",
    "python_docs = python_splitter.create_documents([python_code])\n",
    "print(python_docs[0])\n",
    "\n",
    "js_code = \"\"\"\n",
    "function helloWorld() {\n",
    "  console.log(\"Hello, World!\");\n",
    "}\n",
    "helloWorld();\n",
    "\"\"\"\n",
    "\n",
    "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.JS, chunk_size=30, chunk_overlap=5\n",
    ")\n",
    "js_docs = js_splitter.create_documents([js_code])\n",
    "print(js_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b615b4-2614-43fc-a5df-73d925eea3aa",
   "metadata": {},
   "source": [
    "For splitting text based on token count, which is useful for language models with token limits, the [**TokenTextSplitter**](TokenTextSplitter) is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4293bbef-e1a1-4f9f-b8a3-6a2c98d6020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The field of law is constantly evolving and becoming more complex, making it essential for legal professionals to stay updated and knowledgeable. This is where a Master of Laws (LLM) degree comes into play. An LLM is a postgraduate law degree that provides specialized legal education and training to individuals who have already completed their Juris Doctor (JD) or equivalent law degree.\n",
      "\n",
      "The LLM degree is designed to enhance the legal skills and knowledge of practicing lawyers, law graduates, and professionals from other\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "texts = text_splitter.split_text(long_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bb842-2351-46b9-b065-0723751c2506",
   "metadata": {},
   "source": [
    "Finally, the [**LongContextReorder**](https://python.langchain.com/api_reference/community/document_transformers/langchain_community.document_transformers.long_context_reorder.LongContextReorder.html) reorders documents to prevent performance degradation in models due to long contexts:\n",
    "```python\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "print(reordered_docs[0])\n",
    "```\n",
    "\n",
    "These tools demonstrate various ways to transform documents in LangChain, from simple text splitting to complex reordering and language-specific splitting. For more in-depth and specific use cases, the LangChain documentation and Integrations section should be consulted.\n",
    "\n",
    "In our examples, <b>the loaders have already created chunked documents for us, and this part is already handled.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a5bc9-88b8-4f40-ac8e-844325693d3c",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Text Embedding Models</font></b>\n",
    "<b><font size='3ptx'>[Text embedding models](https://python.langchain.com/docs/how_to/embed_text/) in LangChain provide a standardized interface for various embedding model providers like OpenAI, Cohere, and Hugging Face</font>. These models transform text into vector representations, enabling operations like semantic search through text similarity in vector space.</b>\n",
    "\n",
    "To get started with text embedding models, you typically need to install specific packages and set up API keys. We have already done this for OpenAI. \n",
    "\n",
    "In LangChain, the `embed_documents` method is used to embed multiple texts, providing a list of vector representations. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cb5bf125-6098-48c4-816e-32290c0760e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the model\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e84e513a-4356-4c01-9ac7-7b64afa9e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a list of texts\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8e1ab314-e827-4628-9e5d-2a3a1b0b5e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents embedded: 5\n",
      "Dimension of each embedding: 1536\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents embedded:\", len(embeddings))\n",
    "print(\"Dimension of each embedding:\", len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc337c-e498-44e5-adc8-c53393a6f97f",
   "metadata": {},
   "source": [
    "Let's try other embedding model provided by GCP Vertex API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a0a23054-8461-4075-b1d0-977228d201a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: ImportError: cannot import name 'replace_defs_in_schema'\n",
    "#from langchain_google_vertexai import VertexAIEmbeddings\n",
    "#embeddings_model = VertexAIEmbeddings(model=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de212b-0512-4fc7-8fb1-312c49285010",
   "metadata": {},
   "source": [
    "For embedding a single text, such as a search query, the `embed_query` method is used. This is useful for comparing a query to a set of document embeddings. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "468bfb17-e4d7-4064-8a72-12787c638008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five dimensions of the embedded query: [0.005329647101461887, -0.0006122003542259336, 0.0389961302280426, -0.002898985054343939, -0.008904732763767242]\n"
     ]
    }
   ],
   "source": [
    "# Embed a single query\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "print(\"First five dimensions of the embedded query:\", embedded_query[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28724e-fc85-48d0-800d-88d4fb76c538",
   "metadata": {},
   "source": [
    "Understanding these embeddings is crucial. Each piece of text is converted into a vector, the dimension of which depends on the model used. For instance, OpenAI models typically produce 1536-dimensional vectors. These embeddings are then used for retrieving relevant information.\n",
    "\n",
    "LangChain's embedding functionality is not limited to OpenAI but is designed to work with various providers. The setup and usage might slightly differ depending on the provider, but the core concept of embedding texts into vector space remains the same. For detailed usage, including advanced configurations and integrations with different embedding model providers, the LangChain documentation in the Integrations section is a valuable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea481f7-b6e8-49f0-8a83-d92853e371f0",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Vector Stores</font></b>\n",
    "<b><font size='3ptx'>[Vector stores](https://python.langchain.com/docs/integrations/vectorstores/) in LangChain support the efficient storage and searching of text embeddings</font>. LangChain integrates with over 50 vector stores, providing a standardized interface for ease of use.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dba244-e712-4e49-b4ad-a024d9d4117b",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Example: Storing and Searching Embeddings</font></b>\n",
    "After embedding texts, we can store them in a vector store like [**Chroma**](https://python.langchain.com/docs/integrations/vectorstores/chroma/) and perform similarity searches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9bfb0424-751e-4df0-907e-a598ad384994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_chroma\n",
    "!pip freeze | grep 'langchain_chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b1af4e2d-5705-434c-b161-035e7f272990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'start_index': 0}, page_content='The sun beat down mercilessly, making the asphalt shimmer with heat.'),\n",
       " Document(metadata={'start_index': 0}, page_content='A sudden downpour drenched the city streets, sending pedestrians scurrying for cover.'),\n",
       " Document(metadata={'start_index': 0}, page_content='The gentle breeze carried the scent of rain and freshly cut grass.'),\n",
       " Document(metadata={'start_index': 0}, page_content='The ripe mangoes dripped with sweet, golden juice.'),\n",
       " Document(metadata={'start_index': 0}, page_content='I love the tangy burst of flavor from a freshly squeezed lemon.'),\n",
       " Document(metadata={'start_index': 0}, page_content='The crisp apple provided a satisfying crunch with every bite.'),\n",
       " Document(metadata={'start_index': 0}, page_content='A majestic eagle soared high above the mountain peaks.'),\n",
       " Document(metadata={'start_index': 0}, page_content='The playful kittens tumbled over each other, chasing a ball of yarn.'),\n",
       " Document(metadata={'start_index': 0}, page_content='A lone wolf howled at the full moon, its mournful cry echoing through the valley.'),\n",
       " Document(metadata={'start_index': 0}, page_content='The tiny hummingbird hovered delicately, sipping nectar from the vibrant flowers.')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the model\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "docs = [\n",
    "    \"The sun beat down mercilessly, making the asphalt shimmer with heat.\",  # Weather\n",
    "    \"A sudden downpour drenched the city streets, sending pedestrians scurrying for cover.\", # Weather\n",
    "    \"The gentle breeze carried the scent of rain and freshly cut grass.\",  # Weather\n",
    "    \"The ripe mangoes dripped with sweet, golden juice.\",  # Fruit\n",
    "    \"I love the tangy burst of flavor from a freshly squeezed lemon.\",  # Fruit\n",
    "    \"The crisp apple provided a satisfying crunch with every bite.\",  # Fruit\n",
    "    \"A majestic eagle soared high above the mountain peaks.\",  # Animal\n",
    "    \"The playful kittens tumbled over each other, chasing a ball of yarn.\",  # Animal\n",
    "    \"A lone wolf howled at the full moon, its mournful cry echoing through the valley.\",  # Animal\n",
    "    \"The tiny hummingbird hovered delicately, sipping nectar from the vibrant flowers.\"  # Animal\n",
    "]\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# split it into chunks\n",
    "texts = text_splitter.create_documents(docs)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1555a8bd-b1d4-46a8-9893-326d7bc6c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = Chroma(embedding_function=embeddings_model)\n",
    "db = Chroma.from_documents(texts, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b440c37f-cd0f-4340-bc01-4ba9cc5c5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_texts = db.similarity_search(\"weather\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "998b938d-b7f3-422a-aa32-6873c9940e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A sudden downpour drenched the city streets, sending pedestrians scurrying for cover.' metadata={'start_index': 0}\n",
      "page_content='The sun beat down mercilessly, making the asphalt shimmer with heat.' metadata={'start_index': 0}\n",
      "page_content='The gentle breeze carried the scent of rain and freshly cut grass.' metadata={'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "for text in similar_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d455a24-48e7-4ec9-981f-32ebacfa452d",
   "metadata": {},
   "source": [
    "Let us alternatively use the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss/) to create indexes for our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "695344ff-5b30-401f-8abc-9ef6cb0e9b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss_vs = FAISS.from_documents(texts, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a9bca172-42bf-4f64-9e7b-c825178e666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_texts = faiss_vs.similarity_search(\"fruit\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4d236ea7-75e0-4361-85ee-eb837c441066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The ripe mangoes dripped with sweet, golden juice.' metadata={'start_index': 0}\n",
      "page_content='The crisp apple provided a satisfying crunch with every bite.' metadata={'start_index': 0}\n",
      "page_content='I love the tangy burst of flavor from a freshly squeezed lemon.' metadata={'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "for text in similar_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f958d26b-bbb4-454e-9074-84f994e12f8a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Retrievers</font></b>\n",
    "<b><font size='3ptx'>[Retrievers](https://python.langchain.com/docs/concepts/retrievers/) in LangChain are interfaces that return documents in response to an unstructured query</font>. They are more general than vector stores, focusing on retrieval rather than storage. Although vector stores can be used as a retriever's backbone, there are other types of retrievers as well.</b>\n",
    "![flow](https://python.langchain.com/assets/images/retriever_concept-1093f15a8f63ddb90bd23decbd249ea5.png)\n",
    "\n",
    "To set up a Chroma retriever, you first install it using\n",
    "```shell\n",
    "$ pip install chromadb\n",
    "```\n",
    ". Then, you load, split, embed, and retrieve documents using a series of Python commands. Here's a code example for setting up a Chroma retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3ef3054f-84f5-4ba2-a120-ab44f5120d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1161, which is longer than the specified 500\n",
      "Created a chunk of size 579, which is longer than the specified 500\n",
      "Created a chunk of size 587, which is longer than the specified 500\n",
      "Created a chunk of size 1142, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "full_text = open(\"test_data/CockatooAI_README.md\", \"r\").read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "55323ebb-4694-4cc8-8553-8d24c4996ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_texts(texts, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e9f673f5-623b-47ca-8425-5c55367dcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What does model A mean?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "379a2b54-8204-4271-843e-46a77a826293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4 docs obtained!\n"
     ]
    }
   ],
   "source": [
    "print(f'Total {len(retrieved_docs)} docs obtained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "619c0113-9a17-4b5b-bfe8-081038cf8856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# How To (Initiative):\n",
      "Use models (A-B-C as shown below) to form the pipeline as the final language tutor:\n",
      "- model A: voice to text\n",
      "- model B(LLM): text  text , might be managed by [langChain](https://python.langchain.com/docs/get_started/introduction), Andrew Ng provided [2-3 free short courses](https://www.deeplearning.ai/short-courses/).\n",
      "- model C: text to voice\n",
      "PS: C might be the same as A, needs more research. But the** Goal towards users** should be the top concern when considering which model to use. Besides, better to use open-source models instead of paying premium APIs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411b6d6-d404-43d7-952b-d36b300866d3",
   "metadata": {},
   "source": [
    "<b>The [**MultiQueryRetriever**](https://python.langchain.com/docs/how_to/MultiQueryRetriever/) automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query</b>. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever can mitigate some of the limitations of the distance-based retrieval and get a richer set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3cf7660d-e123-4516-9ce1-02e31890a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "question = \"List the three models used in Cockatoo AI and give a short description to them.\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "42056842-ddb1-4d41-b820-5da69095c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique documents: 4\n"
     ]
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.invoke(input=question)\n",
    "print(\"Number of unique documents:\", len(unique_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8c8cd0b9-476a-4f5a-8472-f93f20ba3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Project language tutor (focused on listening & speaking)\n",
      "Cockatoo AI is a cutting-edge language tutor project focused on enhancing listening and speaking skills.  Our AI companion engages users in realistic conversations, adapting its speaking style (from direct to philosophical) and difficulty level (A1-C1) to suit individual learning needs. Cockatoo AI can discuss user-provided topics, ask relevant questions, and even express itself with a variety of tones and voices (happy, sad, elderly, adult, male, female, etc.) to provide comprehensive listening practice.  For our team, this project provides valuable experience in LLM development, voice model integration, deep learning, and CI/CD, while also offering opportunities to explore bias mitigation techniques and optimize the balance between fine-tuning and computational resources.  Our architecture utilizes a three-stage pipeline: voice-to-text (Model A), text-to-text processing with LLMs (Model B, potentially managed by LangChain), and text-to-voice (Model C, possibly the same as A). We prioritize open-source models and aim to create a personalized and effective language learning experience.\n"
     ]
    }
   ],
   "source": [
    "print(unique_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7705bdb2-4336-4a35-b6bc-0ff41479ba05",
   "metadata": {},
   "source": [
    "<b>[Contextual Compression](https://python.langchain.com/docs/how_to/contextual_compression/) in LangChain compresses retrieved documents using the context of the query, ensuring only relevant information is returned</b>. This involves content reduction and filtering out less relevant documents. The following code example shows how to use Contextual Compression Retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a869120c-c469-44f2-9537-f6dcf7ee862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "81cb1f39-e232-4043-bb4f-fedb8a29d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"List the three models used in Cockatoo AI and give a short description to them.\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e58602da-d005-4c6e-8254-e6428947e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 4 docs obtained:\n",
      "- Cockatoo AI is a cutting-edge language tutor project focused on enhancing listening and speaking skills.\n",
      "- Our AI companion engages users in realistic conversations, adapting its speaking style (from direct to philosophical) and difficulty level (A1-C1) to suit individual learning needs.\n",
      "- Cockatoo AI can discuss user-provided topics, ask relevant questions, and even express itself with a variety of tones and voices (happy, sad, elderly, adult, male, female, etc.) to provide comprehensive listening practice.\n",
      "- Our architecture utilizes a three-stage pipeline: voice-to-text (Model A), text-to-text processing with LLMs (Model B, potentially managed by LangChain), and text-to-voice (Model C, possibly the same as A).\n",
      "- We prioritize open-source models and aim to create a personalized and effective language learning experience.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total {len(compressed_docs)} docs obtained:')\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b65bdb-ba8f-4630-9f95-47f94868f6c3",
   "metadata": {},
   "source": [
    "The [**EnsembleRetriever**](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.ensemble.EnsembleRetriever.html) combines different retrieval algorithms to achieve better performance. An example of combining BM25 and FAISS Retrievers is shown in the following code ([more](https://python.langchain.com/docs/how_to/ensemble_retriever/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "739f6784-23b3-4914-bf3d-0b5914ec705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lark==1.2.2\n",
      "rank-bm25==0.2.2\n"
     ]
    }
   ],
   "source": [
    "#pip install rank_bm25\n",
    "#pip install lark\n",
    "!pip freeze | grep -P '(bm25|lark)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3a4a2b7c-845b-4226-86f6-c36c364c6a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Project language tutor (focused on listening & speaking)\\nCockatoo AI is a cutting-edge language tutor project focused on enhancing listening and speaking skills.  Our AI companion engages users in realistic conversations, adapting its speaking style (from direct to philosophical) and difficulty level (A1-C1) to suit individual learning needs. Cockatoo AI can discuss user-provided topics, ask relevant questions, and even express itself with a variety of tones and voices (happy, sad, elderly, adult, male, female, etc.) to provide comprehensive listening practice.  For our team, this project provides valuable experience in LLM development, voice model integration, deep learning, and CI/CD, while also offering opportunities to explore bias mitigation techniques and optimize the balance between fine-tuning and computational resources.  Our architecture utilizes a three-stage pipeline: voice-to-text (Model A), text-to-text processing with LLMs (Model B, potentially managed by LangChain), and text-to-voice (Model C, possibly the same as A). We prioritize open-source models and aim to create a personalized and effective language learning experience.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b7d03b06-2e32-45fc-ad6d-0f1bb86fe12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(texts, k=2)\n",
    "faiss_vs = FAISS.from_texts(texts, embedding=embeddings_model)\n",
    "faiss_retriever = faiss_vs.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "91ee8bf7-805c-4eca-876f-5dcc43929012",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ensemble_retriever.get_relevant_documents(\"model A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5058d9d2-3cbe-4747-9295-a8429ebf95e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3 docs obtained!\n",
      "# Good References\n",
      "## For Model A (Speech to text, STT)\n",
      "- The library [`SpeechRecognition`](https://pypi.org/project/SpeechRecognition/)\n",
      "  is used for performing speech recognition, with support for several engines and APIs, online and offline.\n",
      "    - [Notebook - Easy Speech-to-Text with Python](https://github.com/johnklee/ml_articles/blob/master/medium/Easy-speech-to-text-with-python/notebook.ipynb)\n",
      "- [`Whisper`](https://github.com/openai/whisper) is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n",
      "    - [Notebook - Introduction for Whisper in OpenAI](https://github.com/johnklee/ml_articles/blob/master/others/ithome_ithelp_openapi_whisper/notebook.ipynb)\n",
      "## For Model C (Text to speech, TTS)\n",
      "- The company [Evelenlabs](https://elevenlabs.io/) has advanced **non-open sourced** TTS model for multi-languages. It performs well in Chinese (no Taiwanese tone) and English from my perspective, which inlcudes happy/sad/... tones as well as male/female/... sounds.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total {len(docs)} docs obtained!')\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd496bf2-380b-4b96-b0c1-6bc6cd81b680",
   "metadata": {},
   "source": [
    "Generating summaries for better retrieval due to more focused content representation is another method. Here's an example of generating summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "55e3d399-2ee2-4a3f-a5a9-b51b66fff76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "\n",
    "vectorstore = Chroma(collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "chain = (\n",
    "    (lambda x: x.page_content) |\n",
    "    ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\") |\n",
    "    ChatOpenAI(max_retries=0) |\n",
    "    StrOutputParser())\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "87095b7c-2fdf-4163-bab5-b521b29adba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The document is titled \"Misc\" and likely contains miscellaneous information or content. It is not clear what specific information is included in the document based on the title alone.',\n",
       " 'The document provides good references for speech to text (STT) and text to speech (TTS) models. It mentions the `SpeechRecognition` library for STT, with various engines and APIs, and the `Whisper` model for general-purpose speech recognition. For TTS, it recommends the non-open sourced TTS model by Evelenlabs, which excels in multi-languages and various tones and sounds. Links to notebooks for each model are also provided for further exploration.',\n",
       " 'The document outlines a method for creating a language tutor using three models in a pipeline: voice to text (model A), text to text (model B), and text to voice (model C). It suggests using open-source models rather than paid APIs and emphasizes prioritizing the goal towards users when selecting which model to use. Research is needed to determine if models A and C can be the same. Additionally, it mentions resources provided by Andrew Ng for learning more about deep learning techniques.']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d09652a2-7bc5-4fa1-b691-7da0fc78db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(summaries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "15af5708-dfa0-4948-8d30-cd24ee08fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7120d7b-4dc3-4c07-89eb-baa533a39ab0",
   "metadata": {},
   "source": [
    "A self-querying retriever [**SelfQueryRetriever**](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.self_query.base.SelfQueryRetriever.html) is one that, as the name suggests, has the ability to query itself. Specifically, <b>given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store</b>. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters ([details](https://python.langchain.com/docs/how_to/self_query/)).\n",
    "![flow](https://python.langchain.com/assets/images/self_querying-26ac0fc8692e85bc3cd9b8640509404f.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72c78625-f7d8-4721-9fcf-c6661fc76574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4e5c66b-3b94-4ffb-9a46-b814613bace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2496cd4a-f2cb-4268-a42c-e45ec5dabed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "348117a1-328d-477d-adf3-bb5164f4232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example only specifies a filter\n",
    "retrieved_docs = retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d863f1-6aac-43d1-9f05-8e02fa46d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2 docs obtained!\n"
     ]
    }
   ],
   "source": [
    "print(f'Total {len(retrieved_docs)} docs obtained!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8412e0a3-4683-4161-bd3f-f9eb6512093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Three men walk into the Zone, three men walk out of the Zone\n",
      "2. A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f'{i+1}. {doc.page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68066eab-1c6f-4fc1-a419-0dfa7f80cd5c",
   "metadata": {},
   "source": [
    "<a id='sect_3'></a>\n",
    "## <b><font color='darkblue'>Module III : Agents</font></b>\n",
    "<b><font size='3ptx'>LangChain introduces a powerful concept called \"[Agents](https://python.langchain.com/docs/tutorials/agents/)\" that takes the idea of chains to a whole new level</font>. Agents leverage language models to dynamically determine sequences of actions to perform, making them incredibly versatile and adaptive. Unlike traditional chains, where actions are hardcoded in code, agents employ language models as reasoning engines to decide which actions to take and in what order</b>.\n",
    "\n",
    "**The Agent is the core component responsible for decision-making**. It harnesses the power of a language model and a prompt to determine the next steps to achieve a specific objective. The inputs to an agent typically include:\n",
    "* **Tools**: Descriptions of available tools (more on this later).\n",
    "* **User Input**: The high-level objective or query from the user.\n",
    "* **Intermediate Steps**: A history of (action, tool output) pairs executed to reach the current user input.\n",
    "\n",
    "The output of an agent can be the next action to take actions ([**AgentActions**](https://api.python.langchain.com/en/latest/agents/langchain_core.agents.AgentAction.html)) or the final response to send to the user ([**AgentFinish**](https://api.python.langchain.com/en/latest/agents/langchain_core.agents.AgentFinish.html)). An action specifies a tool and the input for that tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77206f1d-822f-4405-9ac0-71691ddbeed6",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Tools</font></b>\n",
    "<b><font size='3ptx'>[Tools](https://python.langchain.com/docs/tutorials/agents/#define-tools) are interfaces that an agent can use to interact with the world</font>. They enable agents to perform various tasks, such as searching the web, running shell commands, or accessing external APIs. In LangChain, tools are essential for extending the capabilities of agents and enabling them to accomplish diverse tasks</b>.\n",
    "\n",
    "To use tools in LangChain, you can load them using the following snippet:\n",
    "```python\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "tool_names = [...]\n",
    "tools = load_tools(tool_names)\n",
    "```\n",
    "\n",
    "Some tools may require a base Language Model (LLM) to initialize. In such cases, you can pass an LLM as well:\n",
    "```python\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "tool_names = [...]\n",
    "llm = ...\n",
    "tools = load_tools(tool_names, llm=llm)\n",
    "```\n",
    "\n",
    "This setup allows you to access a variety of tools and integrate them into your agent's workflows. The complete list of tools with usage documentation is [here](https://python.langchain.com/docs/integrations/tools/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7f02e-3387-4c2a-8564-9d348f3244a8",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>DuckDuckGo</font></b>\n",
    "The [**DuckDuckGo**](https://python.langchain.com/docs/integrations/tools/ddg/) tool enables you to perform web searches using its search engine. Here's how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84538944-17e1-4529-ae09-1858edffe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# search = DuckDuckGoSearchRun()\n",
    "# search.invoke(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09211b22-821f-4364-9ea9-236660c2d2ee",
   "metadata": {},
   "source": [
    "#### <b><font size>Shell (bash)</font></b>\n",
    "The Shell toolkit provides agents with access to the shell environment, allowing them to execute shell commands. This feature is powerful but should be used with caution, especially in sandboxed environments. Here's how you can use the Shell tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e741445f-e246-4a35-9e70-d11f77f12281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " [\"echo 'Hello World!'\", 'time']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "\n",
    "result = shell_tool.run({\"commands\": [\"echo 'Hello World!'\", \"time\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a6c9cc6-a302-456d-afe3-8e64abdd574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "user\t0m0.00s\n",
      "sys\t0m0.00s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8ea9c-5f91-4548-883a-18b38826462f",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Back to Agents</font></b>\n",
    "Let's move on to agents now... TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca91f6d-cee6-4414-99be-5e65bf4afcc2",
   "metadata": {},
   "source": [
    "<a id='sect_4'></a>\n",
    "## <b><font color='darkblue'>Module IV : Chains</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>LangChain is a tool designed for utilizing Large Language Models (LLMs) in complex applications</font></b>. It provides frameworks for creating chains of components, including LLMs and other types of components. Two primary frameworks:\n",
    "* The LangChain Expression Language (LCEL)\n",
    "* Legacy Chain interface\n",
    "\n",
    "<b>[The LangChain Expression Language](https://python.langchain.com/docs/concepts/lcel/) (LCEL) is a syntax that allows for intuitive composition of chains</b>. It supports advanced features like streaming, asynchronous calls, batching, parallelization, retries, fallbacks, and tracing.\n",
    "\n",
    "For example, you can compose a prompt, model, and output parser in LCEL as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccc54a2a-c8dd-411f-9385-4452ad3327e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a very knowledgeable historian who provides accurate and eloquent answers to historical questions.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "runnable = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2687deb2-49fe-4129-bef0-43868d291217",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = runnable.invoke({\"question\": \"What are the seven wonders of the world\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a141edf7-226b-4c16-a304-db0f9521efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Seven Wonders of the Ancient World were a list of remarkable constructions of classical antiquity. They are:\n",
       "\n",
       "1. The Great Pyramid of Giza, Egypt\n",
       "2. The Hanging Gardens of Babylon, Iraq\n",
       "3. The Statue of Zeus at Olympia, Greece\n",
       "4. The Temple of Artemis at Ephesus, Turkey\n",
       "5. The Mausoleum at Halicarnassus, Turkey\n",
       "6. The Colossus of Rhodes, Greece\n",
       "7. The Lighthouse of Alexandria, Egypt\n",
       "\n",
       "It's worth noting that only the Great Pyramid of Giza still exists today, as the other wonders have been destroyed over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c404c-7517-440a-a257-05b9e7bad202",
   "metadata": {},
   "source": [
    "Alternatively, the [**LLMChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html#llmchain) is an option similar to LCEL for composing components. The [**LLMChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html#llmchain) example is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d48e932e-4c77-4deb-9d36-49d716155d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=model, prompt=prompt, output_parser=StrOutputParser())\n",
    "resp = chain.invoke(input=\"What are the seven wonders of the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74babd9b-5fe6-4ff1-bef6-6113342df0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the seven wonders of the world',\n",
       " 'text': 'The Seven Wonders of the Ancient World were a list of remarkable constructions of classical antiquity. They included the Great Pyramid of Giza, the Hanging Gardens of Babylon, the Statue of Zeus at Olympia, the Temple of Artemis at Ephesus, the Mausoleum at Halicarnassus, the Colossus of Rhodes, and the Lighthouse of Alexandria. These wonders were considered marvels of architecture and engineering at the time and have captured the imagination of people for centuries.'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b74720-a196-45ae-ae2d-be72cc305b2b",
   "metadata": {},
   "source": [
    "Chains in [**LLMChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html#llmchain) can also be stateful by incorporating a Memory object ([**ConversationChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversation.base.ConversationChain.html#conversationchain): Chain to have a conversation and load context from memory.). This allows for data persistence across calls, as shown in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66b1c669-3ada-4cba-aecd-1866d47577f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"1\"}}\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "chain = RunnableWithMessageHistory(chat, get_session_history)\n",
    "#conversation = ConversationChain(llm=chat, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b80f8a4-dfd7-4d04-9fce-93fcbba4db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain.invoke(\n",
    "    \"Answer briefly. What are the first 3 colors of a rainbow?\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2db6e73b-0eb7-4019-8dcb-88656418e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "Red, orange, yellow\n"
     ]
    }
   ],
   "source": [
    "print(f'{resp.__class__}')\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b0ec1d3-4f2d-4903-ad9b-ee39e75754b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain.invoke(\"And the next 4?\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e4ade11-1ab0-46df-9419-ca08ce6e5d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "Green, blue, indigo, violet\n"
     ]
    }
   ],
   "source": [
    "print(f'{resp.__class__}')\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ae6e9-771c-4121-81cb-e580dc09816b",
   "metadata": {},
   "source": [
    "<b>LangChain also supports integration with OpenAI's function-calling APIs, which is useful for obtaining structured outputs and executing functions within a chain</b>. For getting structured outputs, you can specify them using Pydantic classes or JsonSchema, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7dbeebdb-320c-4306-afc6-b467219a32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\")\n",
    "    age: int = Field(description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    # Prompt messages here\n",
    "])\n",
    "runnable = create_structured_output_runnable(Person, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f704984e-3f60-412b-88d3-703125285d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = runnable.invoke(\"Sally is 13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "70ad73dd-1f90-4d2a-a8fd-ce5d3448a49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Sally', age=13, fav_food=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4d3b8-a568-448f-9be4-f77ee346b0b0",
   "metadata": {},
   "source": [
    "<b>LangChain leverages OpenAI functions to create various specific chains for different purposes</b>. These include chains for extraction, tagging, OpenAPI, and QA with citations.\n",
    "\n",
    "<b>In the context of extraction, the process is similar to the structured output chain but focuses on information or entity extraction</b> ([more](https://python.langchain.com/docs/how_to/structured_output/)). For tagging, the idea is to label a document with classes such as sentiment, language, style, covered topics, or political tendency.\n",
    "\n",
    "An example of how tagging works in LangChain can be demonstrated with a Python code. The process begins with installing the necessary packages and setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61fe7dd6-3893-4bef-9641-b9f0aa4b4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac78b0-73a6-4c84-9562-d7e12a7b9a62",
   "metadata": {},
   "source": [
    "The schema for tagging is defined, specifying the properties and their expected types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9ad0665-437f-4609-84b5-a2c4adb07928",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"language\": {\"type\": \"string\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "class Schema(BaseModel):\n",
    "    sentiment: str = Field(description=\"The sentiment of sentences\")\n",
    "    aggressiveness: int = Field(description=\"The aggressiveness of sentence\")\n",
    "    language: str = Field(None, description=\"The spoken language\")\n",
    "\n",
    "#llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "#chain = create_tagging_chain(schema, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8873a73-a481-4295-baf7-162e99915b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "structured_llm = llm.with_structured_output(Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f0684f1-54b0-4498-be01-12683008cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = structured_llm.invoke(\"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "929c131b-fa1f-4d3c-b845-c6015c649a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema(sentiment='positivo', aggressiveness=0, language='espaol')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f67de6d-b514-42ba-8277-37b4b61f6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = structured_llm.invoke(\"This movie really sucks and I won't recommend it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "44738624-6b8e-4e96-aecc-eebd3c095651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Schema(sentiment='Negative', aggressiveness=7, language='English')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b6252-c182-4d6b-8e2a-be89463561ce",
   "metadata": {},
   "source": [
    "<b>Additionally, LangChain's metadata tagger document transformer can be used to extract metadata from LangChain Documents</b>, offering similar functionality to the tagging chain but applied to a LangChain Document.\n",
    "\n",
    "Citing retrieval sources is another feature of LangChain, using OpenAI functions to extract citations from text. This is demonstrated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3d36f78-a909-4524-a7f1-5fa3146416da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_citation_fuzzy_match_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "context = \"Alice has blue eyes. Bob has brown eyes. Charlie has green eyes.\"\n",
    "question = \"What color are Bob's eyes?\"\n",
    "\n",
    "chain = create_citation_fuzzy_match_runnable(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89644ab3-c1f4-4247-bb14-abde34f7e0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnswer(question=\"What color are Bob's eyes?\", answer=[FactWithEvidence(fact='Bob has brown eyes.', substring_quote=['Bob has brown eyes'])])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4cafb-6169-4d93-bf07-c0efd8d37078",
   "metadata": {},
   "source": [
    "<b>In LangChain, chaining in Large Language Model (LLM) applications typically involves combining a prompt template with an LLM and optionally an output parser.</b> The recommended way to do this is through the LangChain Expression Language (LCEL), although the legacy [**LLMChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html#llmchain) approach is also supported.\n",
    "\n",
    "Using LCEL, the [**BasePromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.base.BasePromptTemplate.html), [**BaseLanguageModel**](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.base.BaseLanguageModel.html), and [**BaseOutputParser**](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.base.BaseOutputParser.html) all implement the [**Runnable**](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) interface and can be easily piped into one another. Here's an example demonstrating this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f149e473-c0b5-4b68-b290-e0ec9574e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "runnable = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25c8adf7-ee6a-442d-a50e-2183146245c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RainbowSock Co.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"product\": \"colorful socks\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e891cb-ee04-4eae-9e4e-9c2091bd55f6",
   "metadata": {},
   "source": [
    "<b>Routing in LangChain allows for creating non-deterministic chains where the output of a previous step determines the next step</b>. This helps in structuring and maintaining consistency in interactions with LLMs. <b>For instance, if you have two templates optimized for different types of questions, you can choose the template based on user input</b>.\n",
    "\n",
    "Here's how you can achieve this using LCEL with a [**RunnableBranch**](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html#langchain_core.runnables.branch.RunnableBranch), which is initialized with a list of (condition, runnable) pairs and a default runnable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fdd20d9d-5d99-4f27-bebd-95b8b2e14d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1375: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class Topic(BaseModel):\n",
    "    input: str = Field(description=\"The original question.\")\n",
    "    topic: str = Field(description='topic name (e.g., \"math\", \"physics\", or \"general\")')\n",
    "    \n",
    "\n",
    "def output_parser_to_dict(topic_object: Topic) -> dict:\n",
    "    \"\"\"Parses the Topic object and returns a dictionary.\"\"\"\n",
    "    return {'input': topic_object.input, 'topic': topic_object.topic}\n",
    "        \n",
    "\n",
    "question = \"What is the first prime number greater than 40 and it is divisible by 3?\"\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "math_prompt = PromptTemplate.from_template(\n",
    "    \"You are a mathematician . Answer the question as easy as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "physics_prompt = PromptTemplate.from_template(\n",
    "    \"You are a smart physicist. Answer the question as intuitive as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "prompt_branch = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"math\", math_prompt),\n",
    "    (lambda x: x[\"topic\"] == \"physics\", physics_prompt),\n",
    "    general_prompt,\n",
    ")\n",
    "\n",
    "classifier_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the following question into one of the following topics: math, physics, or general. \n",
    "    Respond with only the topic name (e.g., \"math\", \"physics\", or \"general\").\n",
    "\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "structured_llm = ChatOpenAI().with_structured_output(Topic)\n",
    "classifier_chain = classifier_prompt | structured_llm | output_parser_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e78d91be-6a47-4a8d-903f-ee494d98f2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the first prime number greater than 40 and it is divisible by 3?',\n",
       " 'topic': 'math'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43141703-6103-45e7-81fc-2eb01c59af56",
   "metadata": {},
   "source": [
    "The final chain is then constructed using various components, such as a topic classifier, prompt branch, and an output parser, to determine the flow based on the topic of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ebe6ee6-7221-4825-a9f1-1972ce5b80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = (\n",
    "    classifier_chain | RunnablePassthrough.assign(topic=itemgetter(\"topic\"))\n",
    "    | prompt_branch\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "916f7da7-71ea-48d8-afc5-17635785109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = final_chain.invoke(\"What is the first prime number greater than 40 and it is divisible by 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28c1582c-39cd-4edd-803c-16d6ef3eab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first prime number greater than 40 that is divisible by 3 is 43.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d7111-bf9e-4cbf-94f4-5bc7d05aac7f",
   "metadata": {},
   "source": [
    "This approach exemplifies the flexibility and power of LangChain in handling complex queries and routing them appropriately based on the input.\n",
    "\n",
    "<b>In the realm of language models, a common practice is to follow up an initial call with a series of subsequent calls, using the output of one call as input for the next</b>. This sequential approach is especially beneficial when you want to build on the information generated in previous interactions. While <b>[the LangChain Expression Language](https://python.langchain.com/docs/concepts/lcel/) (LCEL) is the recommended method for creating these sequences</b>, the [**SequentialChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sequential.SequentialChain.html) method is still documented for its backward compatibility.\n",
    "\n",
    "To illustrate this, let's consider a scenario where we first generate a play synopsis and then a review based on that synopsis. Using Python's\n",
    "`langchain.prompts` , we create two [**PromptTemplate**](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate) instances: one for the synopsis and another for the review. Here's the code to set up these templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ce48032-c83e-45c7-985b-1f0bdc0d2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "synopsis_prompt = PromptTemplate.from_template(\n",
    "    \"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\\n\\nTitle: {title}\\nPlaywright: This is a synopsis for the above play:\"\n",
    ")\n",
    "\n",
    "review_prompt = PromptTemplate.from_template(\n",
    "    \"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\\n\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ceb8e-72c8-4a9a-a55f-253c6a14f85b",
   "metadata": {},
   "source": [
    "In the LCEL approach, we chain these prompts with `ChatOpenAI` and `StrOutputParser` to create a sequence that first generates a synopsis and then a review. The code snippet is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "546e0230-e2ca-443c-8e3a-887cf9a62d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "chain = (\n",
    "    {\"synopsis\": synopsis_prompt | llm | StrOutputParser()}\n",
    "    | review_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f46fb092-86ae-4f42-ba92-a2d5ac9ef842",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain.invoke({\"title\": \"Tragedy at sunset on the beach\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3f5c5496-8b5d-45cd-935e-0ffd09dc7185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Tragedy at Sunset on the Beach\" is a poignant and gripping production that will leave audiences emotionally moved and on the edge of their seats. Set against the backdrop of a picturesque beach at dusk, the play beautifully captures the complexities of love, loss, and betrayal.\n",
      "\n",
      "The chemistry between the two leads, Sarah and Jack, is palpable and their performances are raw and heartfelt. As the dark secret from the past unravels, the tension between them escalates, keeping the audience engaged and invested in their story.\n",
      "\n",
      "The tragedy that unfolds is heartbreaking and beautifully executed, leaving a lasting impact on the audience. The skillful direction and powerful performances bring the emotional depth of the story to life, leaving no dry eye in the theater.\n",
      "\n",
      "In the end, \"Tragedy at Sunset on the Beach\" is a must-see for theatergoers looking for a thought-provoking and emotionally resonant production. Love may be tested, but the power of the human spirit shines through in this unforgettable play.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d19ca-d23a-49a3-bc9c-5ac6936f2182",
   "metadata": {},
   "source": [
    "If we need both the synopsis and the review, we can use [**RunnablePassthrough**](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html#langchain_core.runnables.passthrough.RunnablePassthrough) to create a separate chain for each and then combine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ccc1e4f4-828c-4c08-8db4-c19829fd53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "synopsis_chain = synopsis_prompt | llm | StrOutputParser()\n",
    "review_chain = review_prompt | llm | StrOutputParser()\n",
    "chain = {\"synopsis\": synopsis_chain} | RunnablePassthrough.assign(review=review_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ede56fda-cf92-436d-9314-cd40b3b07983",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain.invoke({\"title\": \"Tragedy at sunset on the beach\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ecf1e528-4e9a-4b40-a928-cc2c9353a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== synopsis ===:\n",
      "\"Tragedy at Sunset on the Beach\" is a gripping drama that unfolds as a group of friends gather for a relaxing evening on the beach. As the sun sets and tensions rise, long-held secrets and betrayals come to light, leading to a series of heartbreaking events that will change their lives forever.\n",
      "\n",
      "The play delves into themes of love, friendship, and the consequences of choices made in the heat of the moment. As the protagonists struggle to come to terms with the tragedy that has befallen them, they must navigate their own guilt and grief while grappling with the harsh reality of loss.\n",
      "\n",
      "\"Tragedy at Sunset on the Beach\" is a powerful exploration of the fragility of human relationships and the unforeseen events that can shatter them in an instant. It serves as a reminder that even in the most idyllic settings, darkness can descend without warning, forever altering the course of our lives.\n",
      "\n",
      "=== review ===:\n",
      "\"Tragedy at Sunset on the Beach\" is a haunting and emotionally charged play that delves deep into the complexities of human relationships and the devastating impact of secrets and betrayals. From the moment the sun sets on the beach, tension grips the audience and doesn't let go until the heartbreaking conclusion.\n",
      "\n",
      "The skilled cast brings their characters to life with raw intensity, capturing the turmoil and anguish that comes with the unraveling of long-held secrets. The themes of love, friendship, and loss are explored with a depth and sensitivity that resonates long after the final curtain call.\n",
      "\n",
      "Director [director's name] has managed to create a palpable sense of unease and vulnerability on stage, drawing the audience into the characters' inner turmoil and leaving them on the edge of their seats throughout the performance.\n",
      "\n",
      "\"Tragedy at Sunset on the Beach\" is a thought-provoking and gut-wrenching exploration of the human condition, reminding us that even in moments of tranquility, darkness can descend and change our lives forever. This is a must-see production for anyone who appreciates powerful drama and compelling storytelling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in resp.items():\n",
    "    print(f'=== {k} ===:\\n{v}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd8898-5d66-4f74-bcc7-368487ad136c",
   "metadata": {},
   "source": [
    "For scenarios involving more complex sequences, the [**SequentialChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sequential.SequentialChain.html#langchain.chains.sequential.SequentialChain) method comes into play. This allows for multiple inputs and outputs. Consider a case where we need a synopsis based on a play's title and era. Here's how we might set it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f22d51a-a1b9-4fc0-b41c-e5c2735dcf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "synopsis_template = \"You are a playwright. Given the title of play and the era it is set in, it is your job to write a synopsis for that title.\\n\\nTitle: {title}\\nEra: {era}\\nPlaywright: This is a synopsis for the above play:\"\n",
    "synopsis_prompt_template = PromptTemplate(input_variables=[\"title\", \"era\"], template=synopsis_template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=synopsis_prompt_template, output_key=\"synopsis\")\n",
    "\n",
    "review_template = \"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\\n\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=review_template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"review\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[synopsis_chain, review_chain],\n",
    "    input_variables=[\"era\", \"title\"],\n",
    "    output_variables=[\"synopsis\", \"review\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "184b61db-f762-4e23-bcaa-1ab5c7c2a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "resp = overall_chain.invoke({\"title\": \"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "291fe4a3-6f83-4668-9aeb-2f6474127b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Tragedy at sunset on the beach',\n",
       " 'era': 'Victorian England',\n",
       " 'synopsis': ' \\n\\n\"Tragedy at Sunset on the Beach\" is a gripping tale set in the opulent world of Victorian England. The play follows the aristocratic Lawrence family as they escape the hustle and bustle of London society for a peaceful vacation on the shores of the English coast. As the sun sets on the idyllic beach, tensions rise among the family members, revealing long-held secrets and hidden desires. But when a mysterious figure appears on the beach, the family\\'s peaceful getaway turns into a nightmarish tragedy. With betrayal, love, and revenge at its core, this play delves into the dark underbelly of Victorian society and the consequences of keeping up appearances. Will the Lawrence family be able to survive the night, or will the sunset on the beach be their final curtain call? Don\\'t miss this thrilling and heartbreaking tragedy that will leave you on the edge of your seat until the very end.',\n",
       " 'review': '\\n\\n\"Tragedy at Sunset on the Beach\" is a mesmerizing and haunting production that transports the audience to the luxurious world of Victorian England. From the moment the curtains open, the stage is set with a stunning beach backdrop and the sound of crashing waves, immediately immersing the audience in the idyllic setting of the Lawrence family\\'s vacation.\\n\\nAs the story unfolds, the audience is taken on a rollercoaster of emotions as tensions rise and secrets are revealed among the aristocratic family. The talented cast delivers powerful performances, flawlessly capturing the complex relationships and hidden desires of their characters. The chemistry between the actors is palpable, adding an extra layer of authenticity to the already captivating story.\\n\\nBut it is the addition of a mysterious figure on the beach that truly elevates this production to a whole new level. As the night progresses and the Lawrence family\\'s peaceful getaway turns into a tragic nightmare, the audience is kept on the edge of their seats, unsure of what will happen next.\\n\\nThe play masterfully delves into the dark underbelly of Victorian society, exposing the consequences of societal expectations and the dangers of keeping up appearances. The script is brilliantly written, seamlessly weaving together themes of betrayal, love, and revenge to create a gripping and thought-prov'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57a4a5-3c47-4106-b338-d7cb85a30c1e",
   "metadata": {},
   "source": [
    "<b>In scenarios where you want to maintain context throughout a chain or for a later part of the chain, [**SimpleMemory**](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.simple.SimpleMemory.html#langchain.memory.simple.SimpleMemory) can be used. This is particularly useful for managing complex input/output relationships</b>. For instance, in a scenario where we want to generate social media posts based on a play's title, era, synopsis, and review, [**SimpleMemory**](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.simple.SimpleMemory.html#langchain.memory.simple.SimpleMemory) can help manage these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa0d466b-c40b-40fe-9958-f97d8804a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import SimpleMemory\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "template = '''You are a social media manager for a theater company. Given the title of play, the era it is set in, the date, time and location, the synopsis of the play, and the review of the play,\n",
    "\n",
    " it is your job to write a social media post for that play.\\n\\nHere is some context about the time and location of the play:\\nDate and Time: {time}\\nLocation: {location}\\n\\nPlay Synopsis:\\n{synopsis}\\nReview from a New York Times play critic of the above play:\\n{review}\\n\\nSocial Media Post:'''\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\", \"review\", \"time\", \"location\"], template=template)\n",
    "social_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=\"social_post_text\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    memory=SimpleMemory(memories={\"time\": \"December 25th, 8pm PST\", \"location\": \"Theater in the Park\"}),\n",
    "    chains=[synopsis_chain, review_chain, social_chain],\n",
    "    input_variables=[\"era\", \"title\"],\n",
    "    output_variables=[\"social_post_text\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a222af34-647a-4033-a0e5-04aa98131768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "resp = overall_chain.invoke({\"title\": \"Tragedy at sunset on the beach\", \"era\": \"Victorian England\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0d117333-df4c-4867-b0bb-31e0798a8b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Tragedy at sunset on the beach',\n",
       " 'era': 'Victorian England',\n",
       " 'time': 'December 25th, 8pm PST',\n",
       " 'location': 'Theater in the Park',\n",
       " 'social_post_text': '\\n\\n Don\\'t miss the gripping murder mystery \"Tragedy at sunset on the beach\" this December 25th, 8pm PST at the Theater in the Park! Set in Victorian England, this play explores the dark secrets and lies of high society in the picturesque town of Brighton. Join Inspector Hathaway as he unravels the mystery of a young woman\\'s tragic death, and be prepared for a shocking conclusion. With themes of love, betrayal, and societal expectations, this play is not one to be missed. See you there! #TragedyAtSunset #TheaterInthePark #VictorianMystery #MurderMystery #Drama #Romance'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e30d7-6e01-497a-94c4-0dc060bc6510",
   "metadata": {},
   "source": [
    "In addition to sequential chains, there are specialized chains for working with documents. Each of these chains serves a different purpose, from combining documents to refining answers based on iterative document analysis, to mapping and reducing document content for summarization or re-ranking based on scored responses. These chains can be recreated with LCEL for additional flexibility and customization:\n",
    "* [**StuffDocumentsChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain): combines a list of documents into a single prompt passed to an LLM.\n",
    "* [**RefineDocumentsChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.refine.RefineDocumentsChain.html#langchain.chains.combine_documents.refine.RefineDocumentsChain): updates its answer iteratively for each document, suitable for tasks where documents exceed the model's context capacity.\n",
    "* [**MapReduceDocumentsChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain): applies a chain to each document individually and then combines the results.\n",
    "* [**MapRerankDocumentsChain**](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain.html#langchain.chains.combine_documents.map_rerank.MapRerankDocumentsChain): scores each document-based response and selects the highest-scoring one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f1b2a-59da-4a3e-a2e5-085ba4d5713a",
   "metadata": {},
   "source": [
    "<a id='sect_5'></a>\n",
    "## <b><font color='darkblue'>Module V : Memory</font></b> ([back](#agenda))\n",
    "<b><font size='3ptx'>In LangChain, memory is a fundamental aspect of conversational interfaces, allowing systems to reference past interactions</font>. This is achieved through storing and querying information, with two primary actions: reading and writing. The memory system interacts with a chain twice during a run, augmenting user inputs and storing the inputs and outputs for future reference</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0343d7-907d-498e-896c-67a8333db4ea",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Building Memory into a System</font></b>\n",
    "1. **Storing Chat Messages**: The LangChain memory module integrates various methods to store chat messages, ranging from in-memory lists to databases. This ensures that all chat interactions are recorded for future reference.\n",
    "2. **Querying Chat Messages**: Beyond storing chat messages, LangChain employs data structures and algorithms to create a useful view of these messages. Simple memory systems might return recent messages, while more advanced systems could summarize past interactions or focus on entities mentioned in the current interaction.\n",
    "\n",
    "To demonstrate the use of memory in LangChain, consider the [**ConversationBufferMemory**](https://python.langchain.com/api_reference/langchain/memory/langchain.memory.buffer.ConversationBufferMemory.html) class, a simple memory form that stores chat messages in a buffer. Here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1045f-43a0-43f5-927b-ddfb3d695d84",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Google Gemini Pro Usage via Gemini API and LangChain](https://github.com/sugarforever/LangChain-Tutorials/blob/main/LangChain_Google_Gemini_API.ipynb)\n",
    "* [Medium - Different Chain Types using LangChain](https://medium.com/@shravankoninti/different-chain-types-using-langchain-89cacae6ad1f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
