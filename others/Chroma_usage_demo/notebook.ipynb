{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75715b8-479f-41a0-8628-24bc36d8b58f",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "## <b><font color='darkblue'>Meet ChromaDB for LLM Applications</font><b/>\n",
    "<b><font size='3ptx'>[ChromaDB](https://docs.trychroma.com/) is an open-source vector database designed specifically for LLM applications.</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect0_1'>Store documents</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect0_2'>Query Vectorestore</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect0_3'>Update documents</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect0_4'>Delete documents</font></b>\n",
    "\n",
    "<b>ChromaDB offers you both a user-friendly API and impressive performance, making it a great choice for many embedding applications</b>. To get started, activate your virtual environment and run the following command:\n",
    "```shell\n",
    "(venv) $ python -m pip install chromadb\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "If you have any issues installing ChromaDB, take a look at [the troubleshooting guide](https://docs.trychroma.com/troubleshooting#build-error-when-running-pip-install-chromadb) for help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f44508-1a72-4679-ba43-315ca68ca257",
   "metadata": {},
   "source": [
    "<a id='sect0_1'></a>\n",
    "### <b><font color='darkgreen'>Store documents</font></b> ([back](#sect0))\n",
    "<font size='3ptx'><b>Because you have a grasp on vectors and embeddings, and you understand the motivation behind vector databases, the best way to get started is with an example</b></font>.  \n",
    "\n",
    "<b>For this example, you’ll store ten documents to search over.</b> To illustrate the power of embeddings and semantic search, each document covers a different topic, and you’ll see how well ChromaDB associates your queries with similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3091de82-86e4-4a8b-a7e1-2aa64a047fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "CHROMA_DATA_PATH = \"chroma_data/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624403f0-cb55-4ef8-9478-d5b6d2d9ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb3f376-f424-4ef9-a54c-b831715d1535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=demo_docs)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6e794-f643-4c1e-a9cc-d17516e0a826",
   "metadata": {},
   "source": [
    "Next, you instantiate your embedding function and the ChromaDB collection to store your documents in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed826331-7d41-4fba-a84c-e909c1dbe1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_registered is True\n"
     ]
    }
   ],
   "source": [
    "has_registered = False\n",
    "func_select_embedding_model = embedding_functions.SentenceTransformerEmbeddingFunction\n",
    "\n",
    "if not collections:\n",
    "    embedding_func = func_select_embedding_model(\n",
    "        model_name=EMBED_MODEL\n",
    "    )\n",
    "\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=embedding_func,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "else:\n",
    "    has_registered = True\n",
    "    collection = collections[0]\n",
    "\n",
    "print(f'has_registered is {has_registered}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b6aa4-f361-4f74-8308-a813710ba067",
   "metadata": {},
   "source": [
    "You specify an embedding function from the [**SentenceTransformers**](https://sbert.net/) library. ChromaDB will use this to embed all your documents and queries. In this example, you’ll continue using the \"`all-MiniLM-L6-v2`\" model. You then create your first collection.\n",
    "\n",
    "<b>A collection is the object that stores your embedded documents along with any associated metadata</b>. If you’re familiar with relational databases, then you can think of a collection as a table. In this example, your collection is named demo_docs, it uses the \"`all-MiniLM-L6-v2`\" embedding function that you instantiated, and it uses the cosine similarity distance function as specified by `metadata={\"hnsw:space\": \"cosine\"}`.\n",
    "\n",
    "The last step in setting up your collection is to add documents and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f12c176-25bd-433b-8317-4b05c0d125ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "     \"The latest iPhone model comes with impressive features and a powerful camera.\",\n",
    "     \"Exploring the beautiful beaches and vibrant culture of Bali is a dream for many travelers.\",\n",
    "     \"Einstein's theory of relativity revolutionized our understanding of space and time.\",\n",
    "     \"Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens.\",\n",
    "     \"The American Revolution had a profound impact on the birth of the United States as a nation.\",\n",
    "     \"Regular exercise and a balanced diet are essential for maintaining good physical health.\",\n",
    "     \"Leonardo da Vinci's Mona Lisa is considered one of the most iconic paintings in art history.\",\n",
    "     \"Climate change poses a significant threat to the planet's ecosystems and biodiversity.\",\n",
    "     \"Startup companies often face challenges in securing funding and scaling their operations.\",\n",
    "     \"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\",\n",
    "]\n",
    "\n",
    "genres = [\n",
    "     \"technology\",\n",
    "     \"travel\",\n",
    "     \"science\",\n",
    "     \"food\",\n",
    "     \"history\",\n",
    "     \"fitness\",\n",
    "     \"art\",\n",
    "     \"climate change\",\n",
    "     \"business\",\n",
    "     \"music\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d6e435-b6ac-4456-82a4-a0689982bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not has_registered:\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "         ids=[f\"id{i}\" for i in range(len(documents))],\n",
    "         metadatas=[{\"genre\": g} for g in genres]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5fd3b-6fb8-4142-b0ef-18713cd6819e",
   "metadata": {},
   "source": [
    "<b>The `metadatas` argument is optional, but most of the time, it’s useful to store metadata with your embeddings. In this case, you define a single metadata field, \"genre\", that records the genre of each document</b>. When you query a document, metadata provides you with additional information that can be helpful to better understand the document’s contents. You can also filter on metadata fields, just like you would in a relational database query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa906c9-a38b-4da4-ad74-f71eddf99254",
   "metadata": {},
   "source": [
    "<a id='sect0_2'></a>\n",
    "### <b><font color='darkgreen'>Query Vectorestore</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>With documents embedded and stored in a collection, you’re ready to run some semantic queries.</font></b>\n",
    "\n",
    "Below code snippet will send query `Find me some delicious food!` and request only one doc being returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68422b7a-1e66-4d3d-8f03-539fa4202764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n",
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n",
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n",
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Find me some delicious food!\"],\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d8cfa8-fcab-4544-81d5-f260ccd0bc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "581fc6ce-f83d-41f6-a16f-3602ff20c446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51886930-f0af-45a1-acd2-c989e16f1891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id3']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8299a42b-b138-4d6c-b750-a48d7502f43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7638262063498773]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "880d7c4d-94e7-41e0-9204-40b7478ab6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'genre': 'food'}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1793d-faf2-4e8d-b965-30c8f2203b37",
   "metadata": {},
   "source": [
    "As you can see, the embedding for `Traditional Italian pizza is famous for its thin crust, fresh ingredients, and wood-fired ovens` was most similar to the query `Find me some delicious food`. You probably agree that this document is the closest match. You can also see the ID, metadata, and distance associated with the matching document embedding. Here, you’re using **cosine distance**, which is one minus the cosine similarity between two embeddings.\n",
    "\n",
    "With <font color='blue'>collection.query()</font>, you’re not limited to single queries or single results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f76ef86-1066-45a8-9a5f-e8c2161d4587",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Teach me about history\",\n",
    "                 \"What's going on in the world?\"],\n",
    "    include=[\"documents\", \"distances\"],\n",
    "    n_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd53517-0cee-4afa-9c90-44f470e17190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The American Revolution had a profound impact on the birth of the United States as a nation.',\n",
       " \"Leonardo da Vinci's Mona Lisa is considered one of the most iconic paintings in art history.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c969d3d-db21-4363-9fca-7a73b5a9b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6904192480258038, 0.8771601240607931]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1d54b5-a89a-4184-80e2-4d4393be7980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Climate change poses a significant threat to the planet's ecosystems and biodiversity.\",\n",
       " 'The American Revolution had a profound impact on the birth of the United States as a nation.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5858867d-0c78-43be-949e-8c8707a3a20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8002942768712199, 0.9402920899385823]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351aa95-881c-4c6c-af1f-e43a01cb7904",
   "metadata": {},
   "source": [
    "For this query, the two most similar documents weren’t as strong of a match as in the first query. Recall that cosine distance is one minus cosine similarity, so a cosine distance of 0.80 corresponds to a cosine similarity of 0.20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758355c-f97c-4411-b11b-0fe97db326ea",
   "metadata": {},
   "source": [
    "<b><font color='darkred'>Note:</font></b>\n",
    "> <b>Keep in mind that so-called similar documents returned from a semantic search over embeddings may not actually be relevant to the task that you’re trying to solve</b>. The success of a semantic search is somewhat subjective, and you or your stakeholders might not agree on the quality of the results.\n",
    "> <br/><br/>\n",
    "> <b>If there are no relevant documents in your collection for a given query, or your embedding algorithm wasn’t trained on the right or enough data, then your results might be poor</b>. It’s up to you to understand your application, your stakeholders’ expectations, and the limitations of your embedding algorithm and document collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa6868-2426-4596-8225-7a4621592bb1",
   "metadata": {},
   "source": [
    "<b>Another awesome feature of ChromaDB is the ability to filter queries on metadata</b>. To motivate this, suppose you want to find the single document that’s most related to music history. You might run this query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7484404-ed6a-4ab0-adcb-8ff7e46dcb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id9']],\n",
       " 'distances': [[0.8186328079302339]],\n",
       " 'metadatas': [[{'genre': 'music'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d2bf4-8077-40bf-b19f-c036857e8567",
   "metadata": {},
   "source": [
    "our query is `Teach me about music history`, and the most similar document is `Einstein’s theory of relativity revolutionized our understanding of space and time`. While Einstein is a historical figure who was a musician and teacher, this isn’t quite the result that you’re looking for. Because you’re particularly interested in `music` history, you can filter on the \"genre\" metadata field to search over more relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb4769ad-fbf5-425f-bc27-c7664fd25ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id9']],\n",
       " 'distances': [[0.8186328079302339]],\n",
       " 'metadatas': [[{'genre': 'music'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\"]],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    where={\"genre\": {\"$eq\": \"music\"}},\n",
    "    n_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb5b7e-5cde-4bb2-853d-904db21075dc",
   "metadata": {},
   "source": [
    "As you can see, the document about `Beethoven’s Symphony No. 9` is the most similar document. Of course, for this example, there’s only one document with the `music` genre. To make it slightly more difficult, you could filter on both `history` and `music`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9994a28c-e499-4ad0-9615-2e2f9ef7ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.query(\n",
    "    query_texts=[\"Teach me about music history\"],\n",
    "    where={\"genre\": {\"$in\": [\"music\", \"history\"]}},\n",
    "    n_results=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff7a51b2-7c3b-44a5-a3a7-c4b06aef1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"Beethoven's Symphony No. 9 is celebrated for its powerful choral finale, 'Ode to Joy.'\",\n",
       "  'The American Revolution had a profound impact on the birth of the United States as a nation.']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "884a8e36-c68d-48dc-b083-3618b7eb6aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8186328079302339, 0.8200413485985653]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"distances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b926ab-b53f-4c0b-9f8c-24048859f90f",
   "metadata": {},
   "source": [
    "This query filters the collection of documents that have either a music or history genre, as specified by `where={\"genre\": {\"$in\": [\"music\", \"history\"]}}`. As you can see, the `Beethoven document` is still the most similar, while the `American Revolution document` is a close second. These were straightforward filtering examples on a single metadata field, but ChromaDB also supports [**other filtering operations**](https://docs.trychroma.com/usage-guide#:~:text=Filtering%20metadata%20supports%20the%20following%20operators%3A) that you might need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbd6e8-306b-42f0-b137-d6d0cff051e9",
   "metadata": {},
   "source": [
    "<a id='sect0_3'></a>\n",
    "### <b><font color='darkgreen'>Update documents</font></b> ([back](#sect0))\n",
    "<font size='3ptx'><b>If you want to update existing documents, embeddings, or metadata, then you can use <font color='blue'>collection.update()</font>.</b></font>\n",
    "\n",
    "\n",
    "This requires you to know the IDs of the data that you want to update. In this example, you’ll update both the documents and metadata for \"id1\" and \"id2\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "269ba432-ee13-48ff-aedf-3bde5938c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n",
      "Update of nonexisting embedding ID: id1\n",
      "Update of nonexisting embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "collection.update(\n",
    "    ids=[\"id1\", \"id2\"],\n",
    "    documents=[\n",
    "        \"The new iPhone is awesome!\",\n",
    "        \"Bali has beautiful beaches\"],\n",
    "    metadatas=[{\"genre\": \"tech\"}, {\"genre\": \"beaches\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0485aadf-9cd1-45b8-a2fc-6b379ce5e1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = collection.get(ids=[\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a025ae3c-d410-4d7f-9c40-1c90c9cf5676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "031bfd81-c2a0-4a78-b2c6-9349f9b12a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7896a65-f804-40b0-820d-a5aa0689434c",
   "metadata": {},
   "source": [
    "<a id='sect0_4'></a>\n",
    "### <b><font color='darkgreen'>Delete documents</font></b> ([back](#sect0))\n",
    "<font size='3ptx'><b>Lastly, if you want to delete any items in the collection, then you can use <font color='blue'>collection.delete()</font>.</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f719a27e-c1a2-46c8-8329-f1ad688585b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deletion, we have 8 document(s)!\n"
     ]
    }
   ],
   "source": [
    "print(f'Before deletion, we have {collection.count()} document(s)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26810e-13c1-42cb-8e5a-3d692e56f87d",
   "metadata": {},
   "source": [
    "Below code snippet will delete two documents with id `id1` and `id2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "530efb9e-028c-4b62-860a-ca88994894e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n",
      "Delete of nonexisting embedding ID: id1\n",
      "Delete of nonexisting embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "collection.delete(ids=[\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f7da77c-76bf-4651-bf4f-dd879c349c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get([\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83404ddb-6cd9-4a62-a9b0-a8c749665d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After deletion, we have 8 document(s)!\n"
     ]
    }
   ],
   "source": [
    "print(f'After deletion, we have {collection.count()} document(s)!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3e5d59-ce64-47a0-b16d-b543f654db8b",
   "metadata": {},
   "source": [
    "You’ve now seen many of ChromaDB’s main features, and you can learn more with the [**getting started guide**](https://docs.trychroma.com/getting-started) or [**API cheat sheet**](https://docs.trychroma.com/api-reference). You used a collection of ten hand-crafted documents that allowed you to get familiar with ChromaDB’s syntax and querying functionality, but this was by no means a realistic use case. <b>In the next section, you’ll see ChromaDB shine while you embed and query over thousands of real-world documents</b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97ab33f-f2d6-4800-94b8-2c3521328fd2",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <b><font color='darkblue'>Practical Example: Add Context for a Large Language Model (LLM)</font></b>\n",
    "<b><font size='3ptx'>Vector databases are capable of storing all types of embeddings, such as text, audio, and images. However, as you’ve learned, ChromaDB was initially designed with text embeddings in mind, and it’s most often used to build LLM applications.</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1_1'>Prepare and Inspect Your Dataset</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1_2'>Create a Collection and Add Reviews</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1_3'>Connect to an LLM Service</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1_4'>Provide Context to the LLM</a></font></b>\n",
    "\n",
    "<b>In this section, you’ll get hands-on experience using ChromaDB to provide context to OpenAI’s ChatGPT LLM</b>. To set the scene, you’re a software engineer who works on a popular repo **[\"bt_test_common\"](https://github.com/johnklee/bt_test_common)** (Common utilities for BT testing.). You want to help external users to learn more about this repo and how to use the APIs provided by this repo by LLM.\n",
    "\n",
    "You’re responsible for designing and implementing the back-end logic that creates these summaries. You’ll take the following steps:\n",
    "1. <b>Create a ChromaDB collection that stores documents</b> along with associated metadata.\n",
    "2. <b>Create a system that accepts a query, finds semantically similar documents</b>, and uses the similar documents as context to an LLM. The LLM will use the documents to answer the question posed in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec329e41-339d-4d30-89ef-564f0d0e0e8f",
   "metadata": {},
   "source": [
    "<b>This process of retrieving relevant documents and using them as context for a generative model is known as [retrieval-augmented generation](https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en) (RAG)</b>. This allows LLMs to make inferences using information that wasn’t included in their training dataset, and this is the most common way to apply ChromaDB in LLM applications.\n",
    "\n",
    "There are lots of factors and variations to consider when implementing a RAG system, but for this example, you’ll only need to know the fundamentals. Here’s what a RAG system might look like with ChromaDB:\n",
    "\n",
    "![RAG](images/rag_diagram.PNG)\n",
    "\n",
    "We first embed and store the documents in a ChromaDB collection. In this example, those documents are coming from the `bttc` repo. We then run a query  through ChromaDB to find semantically relevant documents, and you pass the query and relevant documents to an LLM to generate a context-informed response.\n",
    "\n",
    "<b>The key here is that the LLM takes both the original query and the relevant documents as input, allowing it to generate a meaningful response that it wouldn’t be able to create without the documents.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c5995-add8-4ed1-a44e-5b12f6d85365",
   "metadata": {},
   "source": [
    "In reality, your deliverable for this project would likely be a [**chatbot**](https://realpython.com/build-a-chatbot-python-chatterbot/) that stakeholders use to ask questions about repo `bttc` through a user interface. While building a full-fledged chatbot is beyond the scope of this tutorial, you can check out libraries like [**LangChain**](https://python.langchain.com/docs/get_started/introduction) that are designed specifically to help you assemble LLM applications.\n",
    "\n",
    "**The focus of this example is for you to see how you can use ChromaDB for RAG**. This practical knowledge will help reduce the learning curve for [**LangChain**](https://python.langchain.com/docs/get_started/introduction) if you choose to go that route in the future. With that, you’re ready to get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa929d2-b04a-4336-b55b-c91b8773743a",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <b><font color='darkgreen'>Prepare and Inspect Your Dataset</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>You’ll use the repo of [`bttc`](https://github.com/johnklee/bt_test_common) to create the collection. </font></b>\n",
    "\n",
    "Once you’ve clone the target repo [`bttc`](https://github.com/johnklee/bt_test_common), export the root path of repo as environment variable `BTTC_REPO_ROOT` for future ingestion:\n",
    "```shell\n",
    "$ git clone git@github.com:johnklee/bt_test_common.git\n",
    "$ cd bt_test_common/\n",
    "$ export BTTC_REPO_ROOT=`pwd`\n",
    "$ env | grep BTTC_REPO_ROOT\n",
    "BTTC_REPO_ROOT=/usr/local/google/home/johnkclee/Github/bt_test_common\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba454b27-f791-4e16-8621-ce4da63b52d5",
   "metadata": {},
   "source": [
    "Here’s a function that you can use to create the dataset of target repo `bttc` for ingestion of ChromaDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649d4ad2-5270-43f7-aa55-933c9f0e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingest_bttc_repo\n",
    "\n",
    "BTTC_REPO_ROOT='/usr/local/google/home/johnkclee/Github/bt_test_common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31407f90-db77-40a5-a8f7-a62e6e9e0ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Sample doc:\n",
      "\tmetadata: {'file_path': '/usr/local/google/home/johnkclee/Github/bt_test_common/setup.py', 'file_type': 'py'}\n",
      "\tfrom __future__ impo\n"
     ]
    }
   ],
   "source": [
    "for dataset in ingest_bttc_repo.ingestion_of_repo_dataset_gen(BTTC_REPO_ROOT):\n",
    "    ids = dataset['ids']\n",
    "    documents = dataset['documents']\n",
    "    metadatas = dataset['metadatas']\n",
    "    print(f'ids: {ids}')\n",
    "    print(f'Sample doc:')\n",
    "    for doc, metadata in zip(documents, metadatas):\n",
    "        print(f'\\tmetadata: {metadata}')\n",
    "        print(f'\\t{doc[:20]}')\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d90f2c-624a-46a3-8ccd-46784a570048",
   "metadata": {},
   "source": [
    "Function <font color='blue'><b>ingest_bttc_repo</b>.ingestion_of_repo_dataset_gen</font> will generate dataset for every 10 documents for ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e5bada-b748-4059-96ed-d912e2baacb3",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <b><font color='darkgreen'>Create a Collection and Add Reviews</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>Next, you’ll create a collection and add the reviews.</font></b>\n",
    "\n",
    "Below codesnippet will ingest the documents for further RAG usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6dcdcb-36c2-4d36-b240-5de3859eba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"bttc_repo_embeddings\"\n",
    "EMBEDDING_FUNC_NAME = \"multi-qa-MiniLM-L6-cos-v1\"\n",
    "COLLECTION_NAME = \"bttc_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00bcc0c-a481-4bad-9cc7-cbc4f5cfc5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.6 s, sys: 10.1 s, total: 56.7 s\n",
      "Wall time: 8.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collection = ingest_bttc_repo.build_chroma_collection(\n",
    "    chroma_path=CHROMA_PATH,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_func_name=EMBEDDING_FUNC_NAME,\n",
    "    default_repo_path=BTTC_REPO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d849a98-8db3-4e89-b59c-7822bf1eb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 72 document(s) being ingested!\n"
     ]
    }
   ],
   "source": [
    "print(f'Total {collection.count()} document(s) being ingested!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74d113-9fd1-4402-a4c0-b85d4b12ad81",
   "metadata": {},
   "source": [
    "Now we could query the created collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9f87d9-c44a-45f9-8580-2e82603de481",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = collection.query(\n",
    "    query_texts=[\"Tell me how to turn on the bluetooth.\"],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"distances\", \"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d4e237d-896e-40d1-9112-95795198677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\"\"\"Utility to support common BT operations/methods.\"\"\"\n",
      "import datetime\n",
      "import deprecation\n",
      "from functools import partial\n",
      "import logging\n",
      "import time\n",
      "\n",
      "from mobly.controllers import android_device\n",
      "from mobly.controllers.android_device_lib import adb\n",
      "from bttc import bt_data\n",
      "from bttc import common_data\n",
      "from bttc import constants\n",
      "from bttc import core\n",
      "from bttc.utils import device_factory\n",
      "from bttc.utils import log_parser\n",
      "\n",
      "imp\n"
     ]
    }
   ],
   "source": [
    "# Get the first querying result for first document\n",
    "print(query_result['documents'][0][0][500:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0341-7cb9-4eb1-a69a-ad6c81021ad6",
   "metadata": {},
   "source": [
    "<a id='sect1_3'></a>\n",
    "### <b><font color='darkgreen'>Connect to an LLM Service</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>As you know, you’re going to use the `bttc` embeddings as context to an LLM.</font></b>\n",
    "\n",
    "This means that you’ll ask the LLM a question like `How to turn off the bluebooth`, and you’ll provide relevant embeddings to help the LLM answer this question. To do this, you’ll first need to install the [**openai**](https://github.com/openai/openai-python) library:\n",
    "```shell\n",
    "(venv) $ python -m pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4440c-f320-4a08-beb2-084d7c11c476",
   "metadata": {},
   "source": [
    "<b>You need an API key to interact with the models in the openai library, and you can check out [this tutorial](https://realpython.com/generate-images-with-dalle-openai-api/#get-your-openai-api-key) to help you get set up</b>. Once you have your API key, you can store it as an environment variable or add it to a configuration file like this [**JSON**](https://realpython.com/python-json/) file that you could name <font color='olive'>config.json</font>:\n",
    "```json\n",
    "{\n",
    "    \"openai-secret-key\": \"<your-api-key>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cec2f9-cdf9-4d46-8dd3-2fe0c1371ac1",
   "metadata": {},
   "source": [
    "To make sure your API works and everything is running properly, you can run the following code, which will ask the LLM a question without considering any of the documents in your ChromaDB collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b139d5-31c9-45bb-88d7-284617a754e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import llm_utils\n",
    "import openai\n",
    "\n",
    "# 0) Initialize an LLM agent.\n",
    "llm_agent = llm_utils.LLMAgent(\n",
    "    context='You are a software engineer.',\n",
    "    model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760b9d5f-ee42-40e3-b0ac-75841cde7a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Ask question and obtain response.\n",
    "question = 'Can you explain what BTTC or repo `bt_test_common` is?'\n",
    "resp = llm_agent.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bec30a75-b09a-417d-9e87-db7b65abe484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTTC stands for \"Back to the Code\" and is a term commonly used in software development to refer to a situation where a developer needs to revisit and work on a particular piece of code or project. It can be used to indicate that a developer is returning to a previous task or codebase after working on something else.\n",
      "\n",
      "As for the repo `bt_test_common`, it seems to be a specific repository or project related to testing common functionalities in a software system. This repository may contain code, scripts, or configurations that are used for testing purposes, particularly for common functionalities that are shared across different parts of the software.\n",
      "\n",
      "Without more context or specific information, it's difficult to provide a more detailed explanation. If you have access to the repository or more details about its purpose, you may be able to find more information about what it contains and how it is used in the software development process.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55ccd5f7-24aa-4e59-a845-3744a27ba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to turn off the bluetooth of device by package `bttc`?\"\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7cbc80e-afa7-4229-9eab-310fd7826dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with a package called `bttc` for turning off Bluetooth on a device. However, if you are looking to programmatically turn off Bluetooth on a device using a software package or library, you may need to use platform-specific APIs or libraries.\n",
      "\n",
      "For example, if you are developing an Android application, you can use the Android BluetoothAdapter API to enable or disable Bluetooth programmatically. Here is an example code snippet in Java:\n",
      "\n",
      "```java\n",
      "BluetoothAdapter bluetoothAdapter = BluetoothAdapter.getDefaultAdapter();\n",
      "if (bluetoothAdapter != null) {\n",
      "    if (bluetoothAdapter.isEnabled()) {\n",
      "        bluetoothAdapter.disable();\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "If you are working on a different platform or device, you will need to refer to the specific documentation and APIs provided by that platform for controlling Bluetooth functionality programmatically.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41bd7cb-379a-4b05-8501-da66317fcc02",
   "metadata": {},
   "source": [
    "In this block, you import `os`, `json`, and `openai` and set the <b><font color='orange'>TOKENIZERS_PARALLELISM</font></b> environment variable to \"false\". Setting this environment variable to \"false\" will suppress a warning related to [**huggingface tokenizers**](https://huggingface.co/docs/transformers/main_classes/tokenizer). You then load the JSON object that stores your OpenAI API key.\n",
    "\n",
    "The context message, `You are a software engineer.`, helps set the behavior of the LLM so that its responses are more likely to have a desired tone. This type of message is also sometimes called a [**role prompt**](https://realpython.com/practical-prompt-engineering/#add-a-role-prompt-to-set-the-tone). <b>The user message, `How to turn off the bluetooth of device by package `bttc`?`, is the actual question or task that you want the LLM to respond to.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c342e01-96b3-4742-a916-0760fd19bd4d",
   "metadata": {},
   "source": [
    "<a id='sect1_4'></a>\n",
    "### <b><font color='darkgreen'>Provide Context to the LLM</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>As you can see, the LLM gives you a fairly generic description of what it takes to promote customer satisfaction. None of this information is particularly useful to you because it isn’t specific to repo `bttc`.</font></b>\n",
    "\n",
    "To make this response more tailored to your business, you need to provide the LLM with some reviews as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca96811d-628b-413e-b8c7-52d16c41750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_format = \"\"\"\n",
    "You are a software engineer with less experience in using bttc.\n",
    "Use the following collected information to answer questions: {}\n",
    "\"\"\"\n",
    "\n",
    "question_format = \"\"\"\n",
    "{}\n",
    "\n",
    "Please give a short description coming after with the bullet point list as summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2bc0a5e-8357-442b-a5c8-7d0f61e3942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_docs = collection.query(\n",
    "    query_texts=[question],\n",
    "    n_results=10,\n",
    "    include=[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf7781d2-a1e2-441a-9f6d-d57fb1e57974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\"\"\"Utility to support common BT operations/methods.\"\"\"\n",
      "import datetime\n",
      "import deprecation\n",
      "from functools import partial\n",
      "import logging\n",
      "import time\n",
      "\n",
      "from mobly.controllers import android_device\n",
      "from mobly.controllers.android_device_lib import adb\n",
      "from bttc import bt_data\n",
      "from bttc import common_data\n",
      "from bttc import constants\n",
      "from bttc import core\n",
      "from bttc.utils import device_factory\n",
      "from bttc.utils import log_parser\n",
      "\n",
      "imp\n"
     ]
    }
   ],
   "source": [
    "reviews_str = \",\".join(related_docs[\"documents\"][0])\n",
    "print(reviews_str[500:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "892f82cd-9542-4464-9587-776c99fe9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_answer_with_summaries = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": context_format.format(reviews_str)},\n",
    "        {\"role\": \"user\", \"content\": question_format.format(question)},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a04e71da-0420-46fb-8777-d8e83b787549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To turn off the Bluetooth of a device using the `bttc` package, you can follow these steps:\n",
      "\n",
      "1. Import the necessary modules:\n",
      "   ```python\n",
      "   from bttc import bt_utils\n",
      "   ```\n",
      "\n",
      "2. Get the device object using `bttc.get`:\n",
      "   ```python\n",
      "   dut = bttc.get('device_serial_number')\n",
      "   ```\n",
      "\n",
      "3. Use the `disable` method from the `bt` utility to turn off Bluetooth:\n",
      "   ```python\n",
      "   dut.bt.disable()\n",
      "   ```\n",
      "\n",
      "Summary:\n",
      "- Import `bt_utils` module.\n",
      "- Get the device object using `bttc.get`.\n",
      "- Use the `disable` method from the `bt` utility to turn off Bluetooth.\n"
     ]
    }
   ],
   "source": [
    "print(good_answer_with_summaries.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae102915-2715-4234-a3f0-115cfda93b67",
   "metadata": {},
   "source": [
    "As before, you import dependencies, define configuration variables, set your OpenAI API key, and obtain the collection `bttc_docs`. You then define context and question variables that you’ll feed into an LLM for inference. <b>The key difference in context is the `{}` at the end, which will be replaced with relevant reviews that give the LLM context to base its answers on</b>.\n",
    "\n",
    "You then pass the question into <font color='blue'>collection.query()</font> and request ten documents that are most related to the question. In this query. Lastly, you pass the comma-separated `review_str` into context and request an answer from \"`gpt-3.5-turbo`\".\n",
    "\n",
    "Notice how much more specific and detailed ChatGPT’s response is now that you’ve given it relevant documents as context. For example, if you look through the documents in `related_docs`, then you’ll see source code that mention how to turn off bluetooth which are incorporated into the LLM’s response.\n",
    "\n",
    "<b><font color='darkred'>Note.</font></b>\n",
    "> <b>It’s a common misconception that setting <font color='blue'>temperature=0</font> guarantees deterministic responses from ChatGPT</b>. While responses are closer to deterministic when temperature=0, [there’s no guarantee](https://arxiv.org/pdf/2308.02828#:~:text=We%20study%20the%20influence%20of,contrary%20to%20many%20people%27s%20beliefs.) that you’ll get the same response for identical requests. Because of this, ChatGPT might output slightly different results than what you see in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38dfdf-2619-4229-9ae9-df26d4674479",
   "metadata": {},
   "source": [
    "<b>You’ve now seen why vector databases like ChromaDB are so useful for adding context to LLMs. In this example, you’ve scratched the surface of what you can create with ChromaDB, so just think about all the potential use cases for applications like this</b>. The LLM and vector database landscape will likely continue to evolve at a rapid pace, but you can now feel confident in your understanding of how the two technologies interplay with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843c4db-e878-4b1d-b4c2-71b9e27775d6",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Medium - Code Generation using Retrieval Augmented Generation + LangChain](https://medium.com/@rubenszimbres/code-generation-using-retrieval-augmented-generation-langchain-861e3c1a1a53)\n",
    "* [Chroma API Cheetsheet](https://docs.trychroma.com/api-reference)\n",
    "* [RealPython - Prompt Engineering: A Practical Example](https://realpython.com/practical-prompt-engineering/#add-a-role-prompt-to-set-the-tone)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
