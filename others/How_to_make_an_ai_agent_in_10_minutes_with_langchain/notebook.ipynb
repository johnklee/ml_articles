{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b055285c-9d2c-43e4-ac8e-f91798f2f1b4",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LangChain Preface</font></b>\n",
    "([article source](https://dev.to/timesurgelabs/how-to-use-langchain-in-10-minutes-56e2)) <b><font size='3ptx'>[LangChain](https://www.langchain.com/) is a powerful library for Python and Javascript/Typescript that allows you to quickly prototype large language model applications.</font></b>\n",
    "\n",
    "It allows you to chain together LLM tasks (<font color='brown'>hence the name</font>) and even allows you to run autonomous agents quickly and easily. Today we will be going over the basics of chains, so you can hit the ground running with your newest LLM projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845fbbd-033e-4692-a673-20eb0587ed49",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Getting Started</font></b>\n",
    "There are a few basic concepts you’ll need to understand in order to get started. <b>Chains</b> can be thought of as a list of actions to take with an LLM or multiple LLM calls in the list. A chain is made up of 3 simple parts.\n",
    "* [**Prompt Template**](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)- so you can quickly change inputs without changing the prompt.\n",
    "* [**LLM**](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) - The AI that actually runs your prompts.\n",
    "* [**Output Parsers**](https://python.langchain.com/docs/modules/model_io/output_parsers/) - Converts the output into something useful, usually just another string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfa68f-667f-40ec-87b1-7daf6e06b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# loads the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fa68a-4e89-4273-bf32-890bfb4cf76f",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Writing the Chain</font></b>\n",
    "<b><font size='3ptx'>For this example, we’re going to write a chain that generates a TikTok script for an educational channel</font></b>. First, we need to generate a description for the TikTok. We will use prompt templating so we can reuse the prompt later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161ff4e-538d-4b31-8b57-df87565f14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_prompt = PromptTemplate.from_template(\n",
    "    \"Write me a description for a TikTok about {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc2a88-b290-494d-9a80-548b6842c64b",
   "metadata": {},
   "source": [
    "This can then be used in a chain. Before we can define a chain, we need to define an LLM for the chain to use. LangChain recommends that most users should use The [**ChatOpenAI**](https://python.langchain.com/v0.2/docs/integrations/chat/openai/) class to get the cost benefits and simplicity of the ChatGPT API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130210f-97e6-407a-812c-52e732fc58e0",
   "metadata": {},
   "source": [
    "Here is what your chain should look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d029f8-ad2e-4d28-a826-f22992e34321",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# description_chain = LLMChain(llm=llm, prompt=description_prompt, verbose=True)\n",
    "llm_chain = RunnableSequence(first=description_prompt, last=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168a6292-5e11-4acb-8016-a7b70f97ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm_chain.invoke({'topic': 'Cats are cool'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abeb515-1ca2-4445-ac51-a4b9efcb2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473076d8-19cd-401b-85c5-ee29c79f703e",
   "metadata": {},
   "source": [
    "Now that we have a description, we need to have it write a script. This is where chaining comes in - we can sequentially call the LLM again using a slightly different prompt. First, let’s define a new prompt for the next chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75d6e1-4558-49ab-8a62-a8e775f1162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_prompt = PromptTemplate.from_template(\n",
    "    \"Write me a script for a TikTok given the following description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39ee94-4d14-4f35-b247-fdbd2794e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_llm_chain = RunnableSequence(first=script_prompt, last=llm)\n",
    "script_llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59dc79-2c72-43b1-80f1-c7e40881a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_output = script_llm_chain.invoke({'description': output.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b909ab-fb13-4d9d-9edf-9232f6d160c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(script_output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1ef3e-c1fa-43e8-83f8-7a4dcbda8606",
   "metadata": {},
   "source": [
    "<b>Using them like this is fine, but what if we want to chain them together?</b> That’s where Sequential Chains comes in. These allow you to tie multiple chains into a single function call, with them executing in order they are defined. There are two types of sequential chains, we’re just going to focus on the simple sequential chain ([**SimpleSequentialChain**](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)). Edit the Chain import line to the following:\n",
    "```python\n",
    "from langchain.chains.sequential import SimpleSequentialChain\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a6530-2048-4164-8c14-4b621bd56a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "description_chain = LLMChain(llm=llm, prompt=description_prompt)\n",
    "script_chain = LLMChain(llm=llm, prompt=script_prompt)\n",
    "\n",
    "tiktok_chain = SimpleSequentialChain(chains=[description_chain, script_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec4fec-f038-4ca9-9658-35f5acfe2801",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = tiktok_chain.invoke(\"cats are cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ec142-7d87-4224-bcd4-494265d31eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad63de-234f-4695-92a3-8e953c1da2e0",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Summary</font></b>\n",
    "<b><font size='3ptx'>[LangChain](https://python.langchain.com/) is a game-changer for anyone looking to quickly prototype large language model applications</font></b>. In just a few minutes, we've walked through the process of creating chains, defining prompts, and even chaining together multiple LLM calls to create a dynamic TikTok script.\n",
    "\n",
    "The power of LangChain lies in its simplicity and flexibility. Whether you're a seasoned developer or just starting out, LangChain's intuitive design allows you to harness the capabilities of large language models like never before. From generating creative content to running autonomous agents, the possibilities are endless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768afde-7f71-4fb3-a68c-90b63a6ff6c8",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How I Made an AI Agent in 10 Minutes with LangChain</font></b>\n",
    "([article source](https://dev.to/timesurgelabs/how-to-make-an-ai-agent-in-10-minutes-with-langchain-3i2n)) <b><font size='3ptx'>[LangChain](https://www.langchain.com/)is a powerful library that allows you to quickly prototype large language model applications. It allows you to chain together LLM tasks (hence the name) and even allows you to run autonomous agents quickly and easily.</b></font>\n",
    "\n",
    "From this section, we'll explore how to create agents and define custom tools that those agents can use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7b88e8c-c333-42d8-92ed-df27c39ef13f",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Quick Concepts</font></b>\n",
    "[**Agents**](https://python.langchain.com/docs/modules/agents) are a way to run an LLM in a loop in order to complete a task. Agents are defined with the following:\n",
    "* [**Agent Type**](https://python.langchain.com/docs/modules/agents/agent_types/) - This defines how the Agent acts and reacts to certain events and inputs. For this tutorial we will focus on the [ReAct Agent Type](https://python.langchain.com/docs/modules/agents/agent_types/react.html).\n",
    "* [**LLM**](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) - The AI that actually runs your prompts.\n",
    "* [**Tools**](https://python.langchain.com/docs/modules/agents/tools/) - These are Python (or JS/TS) functions that your Agent can call to interact with the world outside of itself. These can be as simple or as complex as you want them to be! Many tools make a [Toolkit](https://python.langchain.com/docs/modules/agents/toolkits/). There are [many toolkits already available](https://python.langchain.com/docs/integrations/toolkits/) built-in to LangChain, but for this example we’ll make our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61269dab-60d0-453a-8833-685d7f73c550",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Agents</font></b>\n",
    "<b><font size='3ptx'>Agents use a combination of an LLM (or an LLM Chain) as well as a Toolkit in order to perform a predefined series of steps to accomplish a goal.</font></b> For this example, we’ll create a couple of custom tools as well as LangChain’s provided DuckDuckGo search tool to create a research agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6690e5-06dc-4429-bc02-d0f04dd1d3c3",
   "metadata": {},
   "source": [
    "#### <b>Importing Necessary Libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b8dc6-ac5a-480e-86fc-1fe4cd8e3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain.tools import Tool, DuckDuckGoSearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# loads the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da31c14-2f26-419b-9b73-cf35fa639211",
   "metadata": {},
   "source": [
    "Here's a breakdown of the imports:\n",
    "* [**requests**](https://pypi.org/project/requests/): A popular Python library for making HTTP requests.\n",
    "* [**BeautifulSoup**](https://pypi.org/project/beautifulsoup4/): A library for web scraping purposes to pull the data out of HTML and XML files.\n",
    "* [**load_dotenv**](https://pypi.org/project/python-dotenv/): A method to load environment variables from a <font color='olive'>.env</font> file.\n",
    "* **LangChain specific imports**: These are specific to the LangChain framework and are used to define tools, prompts, chat models, chains, and agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa363d1-ef57-4bb6-baa8-f91e2867f4af",
   "metadata": {},
   "source": [
    "#### <b>Setting Up the DuckDuckGo Search Tool</b>\n",
    "This initializes the DuckDuckGo search tool provided by [**LangChain**](https://www.langchain.com/). It allows you to search the web using DuckDuckGo and retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1d908-f1c5-4ac8-9f56-4bc39bccbbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddg_search = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f6feb-00b3-4a99-8ff8-791750190ca4",
   "metadata": {},
   "source": [
    "#### <b>Defining Headers for Web Requests</b>\n",
    "This sets a user-agent header for our web requests. Some websites might block requests that don't have a user-agent set, thinking they're from bots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23f6a1-2c9d-4378-aac5-b03742d4aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bec6bb-a69c-49a6-b45c-ba0c8d17c0ce",
   "metadata": {},
   "source": [
    "#### <b>Parsing HTML Content</b>\n",
    "This function takes in HTML content, uses BeautifulSoup to parse it, and then extracts all the text from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e54ae47-235b-4824-bc77-09fb0567bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(content) -> str:\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    text_content_with_links = soup.get_text()\n",
    "    return text_content_with_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a979d4-6537-4f0f-a682-ac857bbaa7b1",
   "metadata": {},
   "source": [
    "#### <b>Fetching Web Page Content</b>\n",
    "This function fetches the content of a web page using the [**requests**](https://pypi.org/project/requests/) library and then parses the HTML to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09161bc8-21b3-4707-be72-ba84bb8d4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_web_page(url: str) -> str:\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    return parse_html(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc172e-d1f0-482c-a288-4082f6de2075",
   "metadata": {},
   "source": [
    "#### <b>Creating the Web Fetcher Tool</b>\n",
    "Here, we're creating a new tool using the [**Tool**.from_function](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.Tool.html#langchain_core.tools.Tool.from_function) method. This tool will use our `fetch_web_page function` to fetch and parse web pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ff1ed-ab6b-4ce0-9bcf-9d032b2d5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_fetch_tool = Tool.from_function(\n",
    "    func=fetch_web_page,\n",
    "    name=\"WebFetcher\",\n",
    "    description=\"Fetches the content of a web page\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a265f-4832-4e69-b681-431894071522",
   "metadata": {},
   "source": [
    "#### <b>Setting Up the Summarizer</b>\n",
    "This section sets up a summarizer using the ChatOpenAI model from LangChain. We define a prompt template for summarization, create a chain using the model and the prompt, and then define a tool for summarization. We use ChatGPT 3, 5 16k context as most web pages will exceed the 4k context of ChatGPT 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a0e9c-e5a5-4a8a-b090-aec9d432b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Summarize the following content: {content}\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "summarize_tool = Tool.from_function(\n",
    "    func=llm_chain.run,\n",
    "    name=\"Summarizer\",\n",
    "    description=\"Summarizes a web page\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa2c7c-40de-4a8e-ad5b-aed8890f5852",
   "metadata": {},
   "source": [
    "#### <b>Initializing the Agent</b>\n",
    "Here, we're initializing an agent with the tools we've defined. This agent will be able to search the web, fetch web pages, and summarize them. Notice how we can re-use the LLM from the summarize tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134dee25-9f3d-4218-891c-3a1ad0a61c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [ddg_search, web_fetch_tool, summarize_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985744e-43cd-4491-9cdb-5780de3772aa",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Running the Agent</font></b>\n",
    "Finally, we define a prompt for our agent and run it. The agent will search the web for information about the Python’s Requests Library, fetch the content fetch some of the content, and then summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9c6f1-cbb4-44be-8a8d-432f9cd78b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Research how to use the requests library in Python.\"\n",
    "    \"Use your tools to search and summarize content into a guide on how to use the requests library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed516ae7-5842-41df-a44f-7f175d7a321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7484a-2067-48cb-9311-f5835956f0b8",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Experimentation</font></b>\n",
    "<b><font size='3ptx'>In this section, we'll explore how to modify the code from the blog post draft to use the experimental Plan and Execute agent</font></b>. The Plan and Execute agent accomplishes an objective by first planning what to do, then executing the sub-tasks. The planning is almost always done by an LLM, while the execution is usually done by a separate agent equipped with tools.\n",
    "\n",
    "First, install the [**LangChain experimental package**](https://pypi.org/project/langchain-experimental/).\n",
    "> pip install langchain_experimental\n",
    "\n",
    "Then you can import the necessary modules from the <b><font color='blue'>langchain_experimental.plan_and_execute</font></b> package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3427e-dc08-4e9b-bc30-8cde24d6b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0fbea9-7fa6-48e0-bfe0-e61359e43ebd",
   "metadata": {},
   "source": [
    "Load the planner and executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3070e-9a8e-4cfc-98eb-b8d922b2faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = load_chat_planner(llm)\n",
    "executor = load_agent_executor(llm, tools, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f3630-cc6e-4c98-9d26-b910bbe205aa",
   "metadata": {},
   "source": [
    "Initialize the Plan and Execute agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5625b-060a-4bae-976d-3491bd35e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bcd813-89c5-4af1-b5dd-9038a55f87a3",
   "metadata": {},
   "source": [
    "Run the agent with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431ec6b-c652-4bff-b624-10bc2de3e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Research how to use the print \"Hello word\" in Python. Use your tools to search and summarize content into a guide on how to achieve the goal.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92865bb1-7c69-406e-a1b5-777a36508266",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465148c-8c3f-41f7-8e62-bc1cdf52b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba1ee4-8e55-4aa8-9084-bd0fc2c7ac0d",
   "metadata": {},
   "source": [
    "<font size='3ptx'><b>In this example, the agent will first plan the steps needed to accomplish the objective, then execute the sub-tasks using the tools provided</b></font>. The agent will search the web for information about the requests library in Python, fetch the content of the relevant results, and then summarize them into a guide.\n",
    "\n",
    "Note that the Plan and Execute agent is experimental and may not work as expected in all cases. However, it can be a powerful tool for automating complex tasks that require multiple steps and interactions with external tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cfe2b-b817-4cc0-8c65-5df03d41b51a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Summary</font></b>\n",
    "<b><font size='3ptx'>LangChain is a game-changer for anyone looking to quickly prototype large language model applications. In just a few minutes, we’ve walked through the process of creating agents, defining custom tools, and even experimenting with the experimental Plan and Execute agent to automate complex tasks.</font></b>\n",
    "\n",
    "The power of LangChain lies in its simplicity and flexibility. Whether you’re a seasoned developer or just starting out, LangChain’s intuitive design allows you to harness the capabilities of large language models like never before. From generating creative content to running autonomous agents, the possibilities are endless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161b983-3096-4eaa-9d41-177555260ab3",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How I Use Google's Gemini Pro with LangChain</font></b>\n",
    "<b><font size='3ptx'>Google's Gemini Pro is one of the newest LLMs publicly available, and to the surprise of some its relatively price competitive.</font></b>\n",
    "\n",
    "Its effectively free while you're developing, and after you development is complete its relatively cheap, costing around `$0.00025 per 1K` characters (characters, not tokens like OpenAI), which is slightly more expensive than GPT-3.5-Turbo, and `$0.0025 per image`, which is effectively the same as OpenAI's GPT-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a272e53-3092-4445-a49e-dc58b4220767",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Okay, I get it, how do I use it?</font></b>\n",
    "First of all, we need to install Gemini Pro's libraries and its LangChain adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e2245-c22a-469a-9626-f41e5c240c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pip install google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db36688-bdbf-42c1-941f-01d5fccf2008",
   "metadata": {},
   "source": [
    "Next you need to acquire an API key, which can be done on the [**Google MakerSuite**](https://makersuite.google.com/). In the top left of the page you should see a \"Get API Key\" button.\n",
    "\n",
    "Copy the new API Key and save it to a <font color='olive'>.env</font> file in your project directory.\n",
    "> GOOGLE_API_KEY=new_api_key_here\n",
    "\n",
    "Now we can create a script that calls Gemini Pro via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc129e6a-6410-4b02-9445-ad16d1b6d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f6ed5-9bc3-4779-8adc-f3ea9d1164c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f1be8-03c7-46a6-a847-2b5546f5bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_prompt = PromptTemplate.from_template(\n",
    "    \"You are a content creator. Write me a tweet about {topic}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf5ce5-0f73-4925-a0a7-9a183c6dc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_chain = LLMChain(llm=llm, prompt=tweet_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4777f-8bf1-46c1-b28b-aab945fb64cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"how ai is really cool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5319a-0824-4a60-a863-216426f8bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = tweet_chain.run(topic=topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e40fb2-c71e-42c8-9127-24a316149e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6139dd-d75f-428c-849a-99a6a6747cdc",
   "metadata": {},
   "source": [
    "And that's it! You've now integrated Gemini Pro with LangChain! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
