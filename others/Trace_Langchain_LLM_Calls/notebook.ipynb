{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885c1514-2363-40ab-b560-57f6dd2fe394",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "<b><font size='3ptx'>LangChain provides powerful callback mechanisms that allow you to capture and analyze LLM calls</font>.</b> When using [**`langchain_google_genai.ChatGoogleGenerativeAI`**](https://python.langchain.com/docs/integrations/chat/google_generative_ai/), you can leverage these callbacks to log input prompts, model outputs, token usage, and other relevant information.\n",
    "\n",
    "Here's how you can collect LLM calls for analysis, along with sample code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0448dba-7e78-46e2-9825-7cae1798dec2",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>1. Using LangChain's Built-in Callbacks (e.g., [`LangChainTracer`](https://python.langchain.com/api_reference/core/tracers/langchain_core.tracers.langchain.LangChainTracer.html) for LangSmith)</font></b>\n",
    "<b><font size='3ptx'>LangChain has deep integration with [**LangSmith**](https://www.langchain.com/langsmith), their platform for debugging, testing, evaluating, and monitoring LLM applications</font></b>. If you set up LangSmith, all your LLM calls (including those from ChatGoogleGenerativeAI) will be automatically logged and available for analysis in the LangSmith UI.\n",
    "\n",
    "### <b><font color='darkgreen'>Steps:</font></b>\n",
    "* **Install LangSmith**: `pip install -U langsmith`\n",
    "* **Set environment variables:**\n",
    "```shell\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"your_langsmith_api_key\"\n",
    "export LANGCHAIN_PROJECT=\"your_project_name\" # Optional, for organizing runs\n",
    "```\n",
    "\n",
    "* **Run your LangChain code**: When these environment variables are set, LangChain automatically instruments your LLM calls, sending the data to LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b4048-0799-4272-bb3f-ac8451a8e747",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Sample Code (Conceptual, as LangSmith handles the collection automatically):</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44dc17e4-82ae-4e10-8a24-50cd4d59a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Ensure your Google API Key is set as an environment variable\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "# Ensure LangSmith environment variables are set (as described above)\n",
    "# export LANGCHAIN_TRACING_V2=\"true\"\n",
    "# export LANGCHAIN_API_KEY=\"your_langsmith_api_key\"\n",
    "# export LANGCHAIN_PROJECT=\"My Gemini App\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "]\n",
    "\n",
    "# Invoke the LLM - this call will be automatically traced by LangSmith\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb083fa6-323b-40c8-8047-40ad2239e331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--2ee2560b-57a3-412f-99e0-0206190ad95f-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe38151-33e2-470c-bf6d-65c16e295053",
   "metadata": {},
   "source": [
    "Then you should be able to observe below information from LangSmith page:\n",
    "![ui](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4bc07-ab54-4b44-8fa9-779f68692f74",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>2. Custom Callback Handlers for Local Collection/Logging</font></b>\n",
    "If you don't want to use LangSmith or need more granular control over where and how the data is collected, you can implement a custom [**BaseCallbackHandler**](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html). This allows you to define what happens at different stages of the LLM call (e.g., when a call starts, ends, or streams a chunk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0b55b-0a24-417f-afae-311ba2031f3b",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Steps:</font></b>\n",
    "* **Create a custom callback handler**: Inherit from [**`langchain_core.callbacks.BaseCallbackHandler`**](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.base.BaseCallbackHandler.html) and override the methods you're interested in (e.g., `on_llm_start`, `on_llm_end`, `on_chat_model_start`, `on_chat_model_end`).\n",
    "* **Pass the handler to your LLM**: Use the `callbacks` argument when invoking the LLM or creating a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d60fa3d-df9e-446a-aa87-1148d9e7bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from typing import Any, Dict, List, Union\n",
    "import json\n",
    "\n",
    "# Ensure your Google API Key is set as an environment variable\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
    "\n",
    "# Custom JSON Encoder to handle UUID objects\n",
    "class UUIDEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, UUID):\n",
    "            # Convert UUID objects to their string representation\n",
    "            return str(obj)\n",
    "        # Let the base class default method raise the TypeError for other types\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "class LLMCallCollector(BaseCallbackHandler):\n",
    "    \"\"\"A custom callback handler to collect LLM call details.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm_calls = []\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "        print(f\"LLM Call Started: {serialized.get('name')}\")\n",
    "        self.llm_calls.append({\n",
    "            \"event\": \"llm_start\",\n",
    "            \"model_name\": serialized.get(\"name\"),\n",
    "            \"prompts\": prompts,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(), # Using pandas for timestamp for convenience\n",
    "            \"kwargs\": kwargs\n",
    "        })\n",
    "\n",
    "    def on_llm_end(self, response: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "        print(f\"LLM Call Ended: Response received.\")\n",
    "        # Extract relevant info from response object, which varies by model type\n",
    "        # For ChatGoogleGenerativeAI, response is typically an AIMessage\n",
    "        output_content = None\n",
    "        token_usage = None\n",
    "\n",
    "        if hasattr(response, 'content'):\n",
    "            output_content = response.content\n",
    "        if hasattr(response, 'response_metadata') and 'usage_metadata' in response.response_metadata:\n",
    "            token_usage = response.response_metadata['usage_metadata']\n",
    "\n",
    "        self.llm_calls.append({\n",
    "            \"event\": \"llm_end\",\n",
    "            \"output_content\": output_content,\n",
    "            \"token_usage\": token_usage,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"kwargs\": kwargs\n",
    "        })\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        messages: List[List[BaseMessage]],\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "        print(f\"Chat Model Call Started: {serialized.get('name')}\")\n",
    "        # Convert BaseMessage objects to dictionary for easier logging\n",
    "        formatted_messages = []\n",
    "        for msg_list in messages:\n",
    "            formatted_inner_messages = []\n",
    "            for msg in msg_list:\n",
    "                formatted_inner_messages.append({\"type\": msg.type, \"content\": msg.content})\n",
    "            formatted_messages.append(formatted_inner_messages)\n",
    "\n",
    "        self.llm_calls.append({\n",
    "            \"event\": \"chat_model_start\",\n",
    "            \"model_name\": serialized.get(\"name\"),\n",
    "            \"input_messages\": formatted_messages,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"kwargs\": kwargs\n",
    "        })\n",
    "\n",
    "    def on_chat_model_end(self, response: Any, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when Chat Model ends running.\"\"\"\n",
    "        print(f\"Chat Model Call Ended: Response received.\")\n",
    "        output_content = None\n",
    "        token_usage = None\n",
    "        \n",
    "        if hasattr(response, 'content'):\n",
    "            output_content = response.content\n",
    "        if hasattr(response, 'response_metadata') and 'usage_metadata' in response.response_metadata:\n",
    "            token_usage = response.response_metadata['usage_metadata']\n",
    "            \n",
    "        self.llm_calls.append({\n",
    "            \"event\": \"chat_model_end\",\n",
    "            \"output_content\": output_content,\n",
    "            \"token_usage\": token_usage,\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"kwargs\": kwargs\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1da4054-b4a2-424e-8c6d-48cd012190e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Model Call Started: ChatGoogleGenerativeAI\n",
      "LLM Call Ended: Response received.\n",
      "\n",
      "LLM Response:\n",
      "Here's a fun fact about giraffes:\n",
      "\n",
      "Giraffes only need to drink water once every few days! They get most of their hydration from the plants they eat. This is pretty handy when you live in a dry, African savanna!\n",
      "\n",
      "--- Collected LLM Call Data ---\n",
      "{\n",
      "  \"event\": \"chat_model_start\",\n",
      "  \"model_name\": \"ChatGoogleGenerativeAI\",\n",
      "  \"input_messages\": [\n",
      "    [\n",
      "      {\n",
      "        \"type\": \"system\",\n",
      "        \"content\": \"You are a helpful assistant.\"\n",
      "      },\n",
      "      {\n",
      "        \"type\": \"human\",\n",
      "        \"content\": \"Tell me a fun fact about giraffes.\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"timestamp\": \"2025-07-13T15:34:45.028323\",\n",
      "  \"kwargs\": {\n",
      "    \"run_id\": \"26c0c0d6-b15c-4f2e-9646-eae766e9cccb\",\n",
      "    \"parent_run_id\": null,\n",
      "    \"tags\": [],\n",
      "    \"metadata\": {\n",
      "      \"ls_provider\": \"google_genai\",\n",
      "      \"ls_model_name\": \"gemini-2.0-flash\",\n",
      "      \"ls_model_type\": \"chat\",\n",
      "      \"ls_temperature\": 0.7,\n",
      "      \"revision_id\": \"v0.1-317-g26179b9-dirty\"\n",
      "    },\n",
      "    \"invocation_params\": {\n",
      "      \"model\": \"models/gemini-2.0-flash\",\n",
      "      \"temperature\": 0.7,\n",
      "      \"top_k\": null,\n",
      "      \"n\": 1,\n",
      "      \"safety_settings\": null,\n",
      "      \"response_modalities\": null,\n",
      "      \"thinking_budget\": null,\n",
      "      \"_type\": \"chat-google-generative-ai\",\n",
      "      \"stop\": null\n",
      "    },\n",
      "    \"options\": {\n",
      "      \"stop\": null\n",
      "    },\n",
      "    \"name\": null,\n",
      "    \"batch_size\": 1\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"event\": \"llm_end\",\n",
      "  \"output_content\": null,\n",
      "  \"token_usage\": null,\n",
      "  \"timestamp\": \"2025-07-13T15:34:46.046425\",\n",
      "  \"kwargs\": {\n",
      "    \"run_id\": \"26c0c0d6-b15c-4f2e-9646-eae766e9cccb\",\n",
      "    \"parent_run_id\": null,\n",
      "    \"tags\": []\n",
      "  }\n",
      "}\n",
      "\n",
      "LLM call data saved to llm_calls_log.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM and your custom collector\n",
    "import pandas as pd # Used for timestamp, ensure it's installed: pip install pandas\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "collector = LLMCallCollector()\n",
    "\n",
    "messages_to_send = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Tell me a fun fact about giraffes.\"),\n",
    "]\n",
    "\n",
    "# Invoke the LLM with the custom callback handler\n",
    "response = llm.invoke(messages_to_send, config={\"callbacks\": [collector]})\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n--- Collected LLM Call Data ---\")\n",
    "for call in collector.llm_calls:\n",
    "    print(json.dumps(call, indent=2, default=str))\n",
    "\n",
    "# Example of saving the collected data to a JSON file\n",
    "with open(\"/tmp/llm_calls_log.json\", \"w\") as f:\n",
    "    json.dump(collector.llm_calls, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nLLM call data saved to llm_calls_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd5dc1-1bd6-447f-b77f-4cac5901e3d6",
   "metadata": {},
   "source": [
    "By using either LangSmith or custom callback handlers, you can effectively collect and analyze all your LLM calls made with [**`langchain_google_genai.ChatGoogleGenerativeAI`**](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
