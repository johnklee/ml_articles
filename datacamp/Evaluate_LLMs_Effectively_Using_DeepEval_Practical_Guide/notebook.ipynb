{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0e562e-cfa8-4e2e-a773-e0a822331673",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://www.datacamp.com/tutorial/deepeval?utm_cid=19589720824&utm_aid=157098106775&utm_campaign=230119_1-ps-other~dsa-tofu~all_2-b2c_3-apac_4-prc_5-na_6-na_7-le_8-pdsh-go_9-nb-e_10-na_11-na&utm_loc=9197821-&utm_mtd=-m&utm_kw=&utm_source=google&utm_medium=paid_search&utm_content=ps-other~apac-en~dsa~tofu~tutorial~artificial-intelligence&gad_source=1&gad_campaignid=19589720824&gclid=Cj0KCQiA-YvMBhDtARIsAHZuUzJyeVnID2osRpZlxxBQUcq0XKoaQKp6AHN1AfbUL7TOc4re1IKxP6YaAmRmEALw_wcB)) <font size='3ptx'><b>Learn to use [DeepEval](https://deepeval.com/) to create Pytest-like relevance tests, evaluate LLM outputs with the G-eval metric, and benchmark Qwen 2.5 using MMLU (Massive Multitask Language Understanding).</b></font>\n",
    "\n",
    "Few evaluation frameworks are available in the machine learning ecosystem that can effectively test large language models (LLMs) on various properties. <b>With the exponential growth in the development of LLMs, it has become crucial to assess their performance across various tasks and evaluate their strengths, weaknesses, and limitations</b>. \n",
    "\n",
    "<b><font size='3ptx'>[DeepEval](https://github.com/confident-ai/deepeval) steps in as a comprehensive and reliable solution to address this need</font></b>, offering a robust framework for testing LLMs on multiple dimensions, such as accuracy, reasoning, coherence, and ethical alignment.\n",
    "\n",
    "<b>In this tutorial, you will learn how to set up [DeepEval](https://github.com/confident-ai/deepeval) and create a relevance test similar to the Pytest approach.</b> Then, you will test the LLM outputs using the G-eval metric and run MMLU benchmarking on the Qwen 2.5 model. This is a beginner-friendly and simple tutorial intended for anyone with a technical background who wants to understand the DeepEval ecosystem better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd1c48-a683-4c2f-b752-dd566e366b34",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Understanding DeepEval</font></b>\n",
    "<font size='3ptx'><b>DeepEval is an open-source evaluation framework designed specifically for large language models, enabling developers to efficiently build, improve, test, and monitor LLM-based applications.</b> It simplifies the process of evaluating LLM outputs by allowing users to \"unit test\" their models, much like using Pytest for traditional software testing.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198cdea-1bf3-4956-bc8b-1c964345992a",
   "metadata": {},
   "source": [
    "<b>DeepEval offers a wide range of features to ensure comprehensive evaluation, including 14 research-backed LLM evaluation metrics, synthetic dataset generation, LLMs benchmarks, red team, and real-time evaluations in production</b>. With these features, DeepEval not only streamlines testing but also ensures the safety and reliability of LLM-powered applications.\n",
    "\n",
    "Read the [**LLM Evaluation blog**](https://www.datacamp.com/blog/llm-evaluation) to learn how to evaluate large language models using key metrics, methodologies, and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b7c96-fc8d-404e-95b7-77dc6ccb2e01",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Initiailziation & Importing</font></b>\n",
    "We need to install some packages beforehand to start this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b1ad3-77f0-4c5d-87b8-001d209b4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U deepeval\n",
    "#!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc4c543-7899-49dd-abd2-7dc277296308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes==0.49.2\n",
      "deepeval==3.8.4\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep -P '(deepeval|bitsandbytes)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802c992a-6f4e-438c-9b76-da0301a27af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3e160-ca2b-4ae7-af5f-e7cb6adda40d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Getting Started with DeepEval</font></b>\n",
    "Write the following Python file using the Jupyter Notebook magic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7faa05-1714-45d8-b9fe-b6582c4be3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_relevancy():\n",
    "    relevancy_metric = AnswerRelevancyMetric(threshold=0.5, model=\"gemini-2.5\")\n",
    "    test_case_1 = LLMTestCase(\n",
    "        input=\"Can I return these shoes after 30 days?\",\n",
    "        actual_output=\"Unfortunately, returns are only accepted within 30 days of purchase.\",\n",
    "        retrieval_context=[\n",
    "            \"All customers are eligible for a 30-day full refund at no extra cost.\",\n",
    "            \"Returns are only accepted within 30 days of purchase.\",\n",
    "        ],\n",
    "    )\n",
    "    assert_test(test_case_1, [relevancy_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10565ec3-a864-44d7-97b6-69890ed3c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_relevancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d6723-b7f4-4130-aab5-445ba78a3c59",
   "metadata": {},
   "source": [
    "The following test result shows that the LLM test case has successfully passed the test and achived a perfect relevancy score of 1.0, indicating that the model's response was completely relevant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
