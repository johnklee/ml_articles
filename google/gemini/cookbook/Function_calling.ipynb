{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "960febcd-7eb0-4eb4-aa25-19a301971268",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Gemini API: Function calling with Python</font></b>\n",
    "([source](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Function_calling.ipynb)) <b><font size='3ptx'>Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.</font></b>\n",
    "\n",
    "Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request. <b>This notebook provides code examples to help you get started.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cad93e7-3956-4275-909b-e2363209627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-generativeai==0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep 'google-generativeai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ae7fb5-c391-4a1b-b92a-bbd2bb77a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config_genai\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df080367-5150-4068-82d4-edf239ab1bb2",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Function calling basics</font></b>\n",
    "<b><font size='3ptx'>To use function calling, pass a list of functions to the `tools` parameter when creating a `GenerativeModel`. </font></b>\n",
    "\n",
    "The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
    "\n",
    "<b><font color='orange'>Important:</font></b>\n",
    ">  The SDK converts function parameter type annotations to a format the API understands (`genai.protos.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838d6c87-2897-4cf0-8b49-9772a4ecdfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x7fe04bfd0310>,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a: float, b: float):\n",
    "    \"\"\"returns a + b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def subtract(a: float, b: float):\n",
    "    \"\"\"returns a - b.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float):\n",
    "    \"\"\"returns a * b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def divide(a: float, b: float):\n",
    "    \"\"\"returns a / b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\", tools=[add, subtract, multiply, divide]\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873e9dc-7014-42ad-a58c-edffdb2a863d",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Automatic function calling</font></b>\n",
    "<b><font size='3ptx'>Function calls naturally fit in to [multi-turn chats](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#multi-turn) as they capture a back and forth interaction between the user and model.</font></b>\n",
    "\n",
    "The Python SDK's **[`ChatSession`](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.language_models.ChatSession)** is a great interface for chats because handles the conversation history for you, and using the parameter `enable_automatic_function_calling` simplifies function calling even further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa69492-2313-4ab9-94f4-2234cba66876",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(enable_automatic_function_calling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56fe1d6-66ff-4801-b5d7-01fb306d3aa8",
   "metadata": {},
   "source": [
    "With automatic function calling enabled, `ChatSession.send_message` automatically calls your function if the model asks it to.\n",
    "\n",
    "In the following example, the result appears to simply be a text response containing the correct answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8403c32-2c29-4d40-969e-5b660105e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\n",
    "    \"I have 57 cats, each owns 44 mittens, how many mittens is that in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28081223-e694-4d4e-8bba-f20136299f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a lot of mittens! There are 2508 mittens in total. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ffd3a1-721b-4bb6-acc8-785b27085e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2508"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57 * 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e09502-5a5a-4151-8371-8ba8db2371d5",
   "metadata": {},
   "source": [
    "However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
    "\n",
    "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a `genai.protos.Content` object, which contains the following information:\n",
    "* **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
    "* **Function Call** (`genai.protos.FunctionCall`): A request from the model to execute a specific function with provided arguments.\n",
    "* **Function Response** (`genai.protos.FunctionResponse`): The result returned by the user after executing the requested function.\n",
    "\n",
    "In the previous example with the mittens calculation, the history shows the following sequence:\n",
    "1. **User**: Asks the question about the total number of mittens.\n",
    "2. **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
    "3. **User**: The `ChatSession` automatically executes the function (<font color='brown'>due to `enable_automatic_function_calling` being set</font>) and sends back a `FunctionResponse` with the calculated result.\n",
    "4. **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e63061-5e37-4086-8e1f-1ab3fcd55bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user -> [{'text': 'I have 57 cats, each owns 44 mittens, how many mittens is that in total?'}]\n",
      "--------------------------------------------------------------------------------\n",
      "model -> [{'function_call': {'name': 'multiply', 'args': {'a': 57.0, 'b': 44.0}}}]\n",
      "--------------------------------------------------------------------------------\n",
      "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 2508.0}}}]\n",
      "--------------------------------------------------------------------------------\n",
      "model -> [{'text': \"That's a lot of mittens! There are 2508 mittens in total. \\n\"}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for content in chat.history:\n",
    "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabcf141-abdb-41e0-9e90-d1c6d0d203ef",
   "metadata": {},
   "source": [
    "The model can respond with multiple function calls before returning a text response, and function calls come before the text response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66830c-8ab1-4590-a3b4-1c3910f2e1aa",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Manual function calling</font></b>\n",
    "For more control, you can process [`genai.protos.FunctionCall`](https://ai.google.dev/api/python/google/generativeai/protos/FunctionCall) requests from the model yourself. This would be the case if:\n",
    "* You use a `ChatSession` with the default `enable_automatic_function_calling=False`.\n",
    "* You use `GenerativeModel.generate_content` (and manage the chat history yourself).\n",
    "\n",
    "The following example is a rough equivalent of the [function calling single-turn curl sample](https://ai.google.dev/docs/function_calling#function-calling-single-turn-curl-sample) in Python. It uses functions that return (mock) movie playtime information, possibly from a hypothetical API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9951bd76-a9b0-4ece-8a8c-9452bad454f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_movies(description: str, location: str = \"\"):\n",
    "    \"\"\"find movie titles currently playing in theaters based on any description, genre, title words, etc.\n",
    "\n",
    "    Args:\n",
    "        description: Any kind of description including category or genre, title words, attributes, etc.\n",
    "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
    "    \"\"\"\n",
    "    return [\"Barbie\", \"Oppenheimer\"]\n",
    "\n",
    "\n",
    "def find_theaters(location: str, movie: str = \"\"):\n",
    "    \"\"\"Find theaters based on location and optionally movie title which are is currently playing in theaters.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
    "        movie: Any movie title\n",
    "    \"\"\"\n",
    "    return [\"Googleplex 16\", \"Android Theatre\"]\n",
    "\n",
    "\n",
    "def get_showtimes(location: str, movie: str, theater: str, date: str):\n",
    "    \"\"\"\n",
    "    Find the start times for movies playing in a specific theater.\n",
    "\n",
    "    Args:\n",
    "      location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
    "      movie: Any movie title\n",
    "      thearer: Name of the theater\n",
    "      date: Date for requested showtime\n",
    "    \"\"\"\n",
    "    return [\"10:00\", \"11:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29247b00-f7ea-4662-ad94-f54074a38c67",
   "metadata": {},
   "source": [
    "Use a dictionary to make looking up functions by name easier later on. You can also use it to pass the array of functions to the `tools` parameter of `GenerativeModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bba860c-1ddd-4712-b510-d969fc8fa11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "    \"find_movies\": find_movies,\n",
    "    \"find_theaters\": find_theaters,\n",
    "    \"get_showtimes\": get_showtimes,\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=functions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9cdd00-5c1e-408c-85d5-1d3900911776",
   "metadata": {},
   "source": [
    "After using `generate_content()` to ask a question, the model requests a `function_call`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d71bafa-91f6-4bdb-adef-bde1649380a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\n",
    "    \"Which theaters in Mountain View show the Barbie movie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "861e515b-2f3b-4bc1-b19d-70f35af659e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[function_call {\n",
       "  name: \"find_theaters\"\n",
       "  args {\n",
       "    fields {\n",
       "      key: \"movie\"\n",
       "      value {\n",
       "        string_value: \"Barbie\"\n",
       "      }\n",
       "    }\n",
       "    fields {\n",
       "      key: \"location\"\n",
       "      value {\n",
       "        string_value: \"Mountain View\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c572bdef-95dd-4386-9958-8cac11beaaf3",
   "metadata": {},
   "source": [
    "Since this is not using a `ChatSession` with automatic function calling, you have to call the function yourself. A very simple way to do this would be with `if` statements:\n",
    "```python\n",
    "if function_call.name == 'find_theaters':\n",
    "  find_theaters(**function_call.args)\n",
    "elif ...\n",
    "```\n",
    "\n",
    "However, since you already made the functions dictionary, this can be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dac26cd-1c4a-47d8-935c-c42e16bd95db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Googleplex 16', 'Android Theatre']\n"
     ]
    }
   ],
   "source": [
    "def call_function(function_call, functions):\n",
    "    function_name = function_call.name\n",
    "    function_args = function_call.args\n",
    "    return functions[function_name](**function_args)\n",
    "\n",
    "\n",
    "part = response.candidates[0].content.parts[0]\n",
    "\n",
    "# Check if it's a function call; in real use you'd need to also handle text\n",
    "# responses as you won't know what the model will respond with.\n",
    "if part.function_call:\n",
    "    result = call_function(part.function_call, functions)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004a067-13c0-43fd-85b1-d51d989c7420",
   "metadata": {},
   "source": [
    "Finally, pass the response plus the message history to the next `generate_content()` call to get a final text response from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32ec88f-83ca-43c5-868e-950ffdfd0617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Googleplex 16 and Android Theatre show Barbie. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.protobuf.struct_pb2 import Struct\n",
    "\n",
    "# Put the result in a protobuf Struct\n",
    "s = Struct()\n",
    "s.update({\"result\": result})\n",
    "\n",
    "# Update this after https://github.com/google/generative-ai-python/issues/243\n",
    "function_response = genai.protos.Part(\n",
    "    function_response=genai.protos.FunctionResponse(name=\"find_theaters\", response=s)\n",
    ")\n",
    "\n",
    "# Build the message history\n",
    "messages = [\n",
    "    # fmt: off\n",
    "    {\"role\": \"user\",\n",
    "     \"parts\": [\"Which theaters in Mountain View show the Barbie movie?.\"]},\n",
    "    {\"role\": \"model\",\n",
    "     \"parts\": response.candidates[0].content.parts},\n",
    "    {\"role\": \"user\",\n",
    "     \"parts\": [function_response]},\n",
    "    # fmt: on\n",
    "]\n",
    "\n",
    "# Generate the next response\n",
    "response = model.generate_content(messages)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248994c-bfc5-486f-a026-703860c249fd",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Parallel function calls</font></b>\n",
    "<b><font size='3ptx'>The Gemini API can call multiple functions in a single turn. This caters for scenarios where there are multiple function calls that can take place independently to complete a task.</font></b>\n",
    "\n",
    "First set the tools up. Unlike the movie example above, <b>these functions do not require input from each other to be called so they should be good candidates for parallel calling</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213d5231-1f5a-40c7-ae69-edde06629b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_disco_ball(power: bool) -> bool:\n",
    "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
    "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      energetic: Whether the music is energetic or not.\n",
    "      loud: Whether the music is loud or not.\n",
    "      bpm: The beats per minute of the music.\n",
    "\n",
    "    Returns: The name of the song being played.\n",
    "    \"\"\"\n",
    "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
    "    return \"Never gonna give you up.\"\n",
    "\n",
    "\n",
    "def dim_lights(brightness: float) -> bool:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "    \"\"\"\n",
    "    print(f\"Lights are now set to {brightness:.0%}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea61d5c-aef3-4a1c-9532-cb8862dd7b8a",
   "metadata": {},
   "source": [
    "Now call the model with an instruction that could use all of the specified tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e238395-4a34-442f-8843-e90c4e1de4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_disco_ball(power=True)\n",
      "start_music(loud=True, energetic=True, bpm=120.0)\n",
      "dim_lights(brightness=0.5)\n"
     ]
    }
   ],
   "source": [
    "# Set the model up with tools.\n",
    "house_fns = [power_disco_ball, start_music, dim_lights]\n",
    "# Try this out with Pro and Flash...\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
    "\n",
    "# Call the API.\n",
    "chat = model.start_chat()\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call.\n",
    "for part in response.parts:\n",
    "    if fn := part.function_call:\n",
    "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "        print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72c5ea-1505-4ef3-af95-d63731d53ca1",
   "metadata": {},
   "source": [
    "Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82781076-33f3-4c95-8051-4c9c96d98445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the responses from the specified tools.\n",
    "responses = {\n",
    "    \"power_disco_ball\": True,\n",
    "    \"start_music\": \"Never gonna give you up.\",\n",
    "    \"dim_lights\": True,\n",
    "}\n",
    "\n",
    "# Build the response parts.\n",
    "response_parts = [\n",
    "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
    "    for fn, val in responses.items()\n",
    "]\n",
    "\n",
    "response = chat.send_message(response_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbadc3a4-3552-4079-b8aa-9b29a2ddee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've powered up the disco ball, started playing \"Never gonna give you up\" at a pretty high volume and dimmed the lights. Get ready to party! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffa772-9000-4d39-a2f3-5cb2c2ac5a5f",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* The [**genai.GenerativeModel**](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) class\n",
    "  - Its [GenerativeModel.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) method builds a [**genai.protos.GenerateContentRequest**](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentRequest) behind the scenes.\n",
    "    - The request's `.tools` field contains a list of 1 [genai.protos.Tool](https://ai.google.dev/api/python/google/generativeai/protos/Tool) object.\n",
    "    - The tool's `function_declarations` attribute contains a list of [**FunctionDeclarations**](https://ai.google.dev/api/python/google/generativeai/protos/FunctionDeclaration) objects.\n",
    "* The [response](https://ai.google.dev/api/python/google/generativeai/protos/GenerateContentResponse) may contain a [**genai.protos.FunctionCall**](https://ai.google.dev/api/python/google/generativeai/protos/FunctionCall), in `response.candidates[0].contents.parts[0]`.\n",
    "* if `enable_automatic_function_calling` is set the [**genai.ChatSession**](https://ai.google.dev/api/python/google/generativeai/ChatSession) executes the call, and sends back the [**genai.protos.FunctionResponse**](https://ai.google.dev/api/python/google/generativeai/protos/FunctionResponse).\n",
    "* In response to a [**FunctionCall**](https://ai.google.dev/api/python/google/generativeai/protos/FunctionCall) the model always expects a [**FunctionResponse**](https://ai.google.dev/api/python/google/generativeai/protos/FunctionResponse).\n",
    "* If you reply manually using [chat.send_message](https://ai.google.dev/api/python/google/generativeai/ChatSession#send_message) or [model.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) remember thart the API is stateless you have to send the whole conversation history (<font color='brown'>a list of [content](https://ai.google.dev/api/python/google/generativeai/protos/Content) objects</font>), not just the last one containing the `FunctionResponse`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
