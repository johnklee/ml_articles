{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3446858-e182-40da-8a5d-9712eca74461",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-rag-quickstart)) Before trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using client libraries](https://cloud.google.com/vertex-ai/docs/start/client-libraries). For more information, see the [Vertex AI Python API reference documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest).\n",
    "\n",
    "To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](https://cloud.google.com/docs/authentication/set-up-adc-local-dev-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0eb98c4-a712-45da-8bb3-dc7ca351c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582f2fa-8904-4a33-a3da-0299414c4cbf",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Use the quickstart to get familiar with RAG</font></b>\n",
    "<b><font size='3ptx'>Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language models (LLMs) by grounding them in external knowledge sources</font>.</b>\n",
    "\n",
    "<b>Instead of solely relying on their pre-trained knowledge, RAG models first retrieve relevant information from a knowledge base</b> (like a document database or the internet) <b>based on the user's query. This retrieved information is then fed into the LLM along with the original prompt, allowing the LLM to generate more accurate, informed, and up-to-date responses</b>. This mitigates issues like hallucination and knowledge gaps, making LLMs more reliable and adaptable to specific domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4ed1a-82bb-4b16-9670-d0a6a303c28a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Vertex AI RAG Engine overview</font></b>\n",
    "([source](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview)) <b>Vertex AI RAG Engine, a component of the Vertex AI Platform, facilitates Retrieval-Augmented Generation (RAG)</b>.\n",
    "\n",
    "Vertex AI RAG Engine is also a data framework for developing context-augmented large language model (LLM) applications. Context augmentation occurs when you apply an LLM to your data. This implements retrieval-augmented generation (RAG).\n",
    "\n",
    "A common problem with LLMs is that they don't understand private knowledge, that is, your organization's data. <b>With Vertex AI RAG Engine, you can enrich the LLM context with additional private information, because the model can reduce hallucination and answer questions more accurately</b>.\n",
    "\n",
    "By combining additional knowledge sources with the existing knowledge that LLMs have, a better context is provided. <b>The improved context along with the query enhances the quality of the LLM's response</b>.\n",
    "\n",
    "The following image illustrates the key concepts to understanding Vertex AI RAG Engine.\n",
    "\n",
    "![flow](https://cloud.google.com/static/vertex-ai/images/Vertex-RAG-Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd9f5b-0206-4c22-963c-5808f343e6e9",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Get started</font></b>\n",
    "Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f94b9dc-96ba-4f97-94a4-7ff512cb0548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-cloud-aiplatform==1.80.0\n"
     ]
    }
   ],
   "source": [
    "# %pip install --quiet google-cloud-aiplatform\n",
    "!pip freeze | grep 'google-cloud-aiplatform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352e9c19-c5d9-46fa-b8c8-e1b99aa61783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChunkingConfig',\n",
       " 'EmbeddingModelConfig',\n",
       " 'Filter',\n",
       " 'HybridSearch',\n",
       " 'JiraQuery',\n",
       " 'JiraSource',\n",
       " 'LayoutParserConfig',\n",
       " 'LlmParserConfig',\n",
       " 'LlmRanker',\n",
       " 'Pinecone',\n",
       " 'RagCorpus',\n",
       " 'RagEmbeddingModelConfig',\n",
       " 'RagFile',\n",
       " 'RagManagedDb',\n",
       " 'RagResource',\n",
       " 'RagRetrievalConfig',\n",
       " 'RagVectorDbConfig',\n",
       " 'RankService',\n",
       " 'Ranking',\n",
       " 'Retrieval',\n",
       " 'SharePointSource',\n",
       " 'SharePointSources',\n",
       " 'SlackChannel',\n",
       " 'SlackChannelsSource',\n",
       " 'TransformationConfig',\n",
       " 'VertexAiSearchConfig',\n",
       " 'VertexFeatureStore',\n",
       " 'VertexPredictionEndpoint',\n",
       " 'VertexRagStore',\n",
       " 'VertexVectorSearch',\n",
       " 'Weaviate',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'create_corpus',\n",
       " 'delete_corpus',\n",
       " 'delete_file',\n",
       " 'get_corpus',\n",
       " 'get_file',\n",
       " 'import_files',\n",
       " 'import_files_async',\n",
       " 'list_corpora',\n",
       " 'list_files',\n",
       " 'rag_data',\n",
       " 'rag_retrieval',\n",
       " 'rag_store',\n",
       " 'retrieval_query',\n",
       " 'update_corpus',\n",
       " 'upload_file',\n",
       " 'utils']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242a1851-483e-42c6-8f9a-1a467beb9a66",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Set Google Cloud project information and initialize Vertex AI SDK</font></b>\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [**enable the Vertex AI API**](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e354a372-7a53-42f5-bef5-22443d5d287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import vertexai\n",
    "from vertexai.preview import rag\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "os.environ['ALLOW_RESET'] = 'TRUE'\n",
    "_ = load_dotenv(find_dotenv(os.path.expanduser('~/.env')))\n",
    "# https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-rag-quickstart\n",
    "# https://cloud.google.com/vertex-ai/docs/start/install-sdk\n",
    "# https://stackoverflow.com/questions/76802606/google-vertex-ai-prediction-api-authentication\n",
    "key_path = os.path.expanduser('~/llm-demo-407300-fe74f0703d20.json')\n",
    "\n",
    "# Create a credentials object using your service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\n",
    "        'https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "# Deployed Endpoint model configs\n",
    "project_id = \"llm-demo-407300\"\n",
    "endpoint_id = \"endpoint_id\"\n",
    "location = 'us-central1'\n",
    "\n",
    "# RAG settings\n",
    "CORPUS_ISPLAY_NAME = \"test_corpus\"\n",
    "\n",
    "vertexai.init(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255fa0e-9c60-4a2d-a1b0-d35379d6ecf6",
   "metadata": {},
   "source": [
    "#### <b>Restart runtime</b>\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f12383e-11af-463a-876e-9855d7c0c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9316c82-aa7c-4c9a-a426-c06a524f1a5a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Create Corpus</font></b>\n",
    "Create a RAG Corpus, Import Files for further query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76c7024-f8f6-4cfd-9e94-e9da8a3fa667",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpa_page_iter = rag.list_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c52a4f-4962-47df-9365-2e5a6befb151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is testing corpus created? True with name as \"projects/llm-demo-407300/locations/us-central1/ragCorpora/2305843009213693952\"\n"
     ]
    }
   ],
   "source": [
    "is_test_corpus_created = False\n",
    "created_corpus_name = None\n",
    "\n",
    "for corpa in corpa_page_iter:\n",
    "    if corpa.display_name == CORPUS_ISPLAY_NAME:\n",
    "        is_test_corpus_created = True\n",
    "        created_corpus_name = corpa.name\n",
    "        break\n",
    "        \n",
    "print(f'Is testing corpus created? {is_test_corpus_created} with name as \"{created_corpus_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a006f3e-5e12-4053-9a1c-a0b31b94d49b",
   "metadata": {},
   "source": [
    "We could import files from Google Drive into corpus. Eligible paths can be formatted as:\n",
    "* https://drive.google.com/drive/folders/{folder_id}\n",
    "* https://drive.google.com/file/d/{file_id}.\n",
    "\n",
    "Remember to grant \"Viewer\" access to the \"Vertex RAG Data Service Agent\" (with the format of `service-{project_number}@gcp-sa-vertex-rag.iam.gserviceaccount.com`) for your Drive folder/files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0deb6fd6-548f-4af4-bdb1-405878907171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 35 ms, total: 186 ms\n",
      "Wall time: 1.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rag_corpus = None\n",
    "if not is_test_corpus_created:\n",
    "    # Create RagCorpus\n",
    "    # Configure embedding model, for example \"text-embedding-004\".\n",
    "    embedding_model_config = rag.EmbeddingModelConfig(\n",
    "        publisher_model=\"publishers/google/models/text-embedding-004\")\n",
    "\n",
    "    rag_corpus = rag.create_corpus(\n",
    "        display_name=CORPUS_ISPLAY_NAME,\n",
    "        embedding_model_config=embedding_model_config,\n",
    "    )\n",
    "else:\n",
    "    rag_corpus = rag.get_corpus(created_corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65773b0d-6ea5-4c2a-a232-2ee93691bafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Files to the RagCorpus\n",
    "# Supports Google Cloud Storage and Google Drive Links\n",
    "if not is_test_corpus_created:\n",
    "    paths = [\n",
    "        'https://drive.google.com/file/d/17CGIWzv1f40QBzxuoc_09wFSBKgECjpp/view?usp=drive_link',\n",
    "    ]\n",
    "\n",
    "    rag.import_files(\n",
    "        rag_corpus.name,\n",
    "        paths,\n",
    "        chunk_size=512,  # Optional\n",
    "        chunk_overlap=100,  # Optional\n",
    "        max_embedding_requests_per_min=900,  # Optional\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222aec63-f20a-4fc8-b2a2-5a87cfd3af9c",
   "metadata": {},
   "source": [
    "Upload a local file to the corpus ([more](https://cloud.google.com/vertex-ai/generative-ai/docs/samples/generativeaionvertexai-rag-upload-file)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae4603e-cc6d-42a6-931b-13fda1f57237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/llm-demo-407300/locations/us-central1/ragCorpora/2305843009213693952'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_corpus.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1afabff-165c-4cd6-9b69-7db34467c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/john/Gitrepos/ml_articles/google/vertex_ai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fc30891-ea55-47d9-9ba5-5d00ef2a73d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working yet!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import google.auth.exceptions\n",
    "from google.cloud import storage\n",
    "#if not is_test_corpus_created:\n",
    "\n",
    "try:\n",
    "    rag_file = rag.upload_file(\n",
    "        corpus_name=rag_corpus.name,\n",
    "        path=\"test_local_file.txt\",\n",
    "        display_name=\"test_local_file.txt\",\n",
    "        description=\"About Cockatoo.AI\",\n",
    "    )\n",
    "    print(f\"File uploaded successfully. RagFile name: {rag_file.name}\")  # Print the RagFile name\n",
    "except google.auth.exceptions.GoogleAuthError as e:\n",
    "    print(f\"Authentication error: {e}\")\n",
    "    print(\"Please ensure you are properly authenticated with Google Cloud.\")\n",
    "    print(\"Use 'gcloud auth login' and 'gcloud config set project YOUR_PROJECT_ID'.\")\n",
    "    print(\"Or, if using a service account, set the GOOGLE_APPLICATION_CREDENTIALS environment variable.\")\n",
    "#except storage.exceptions.Forbidden as e:\n",
    "#    print(f\"Permission error: {e}\")\n",
    "#    print(\"Ensure your account/service account has the 'Storage Object Admin' or 'Storage Object Creator' role.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "'''\n",
    "print('Not working yet!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1036f3-d992-4d34-803e-42ab2c5dd226",
   "metadata": {},
   "source": [
    "Import files from Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac4389c-bbd6-4f23-893a-77230761e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "INPUT_GCS_BUCKET = 'gs://llm-agent-storage'\n",
    "\n",
    "storage_client = storage.Client.from_service_account_json(key_path)\n",
    "bucket = storage_client.get_bucket('llm-agent-storage')\n",
    "blob_name = path_to_file = 'test_local_file.txt'\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.upload_from_filename(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be188517-b8cc-4f29-9f8a-c72f4ddfc0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag.import_files(\n",
    "    corpus_name=rag_corpus.name,\n",
    "    paths=[INPUT_GCS_BUCKET],\n",
    "    chunk_size=1024,  # Optional\n",
    "    chunk_overlap=100,  # Optional\n",
    "    max_embedding_requests_per_min=900,  # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a71be01a-8f19-48c7-8587-7ff7d37cac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_file_iter = rag.list_files(rag_corpus.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "367258af-fed3-4f57-8f41-8540dac2a848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John_K_Lee_20250124.pdf\n",
      "test.txt\n",
      "test_local_file.txt\n"
     ]
    }
   ],
   "source": [
    "for rag_file in rag_file_iter:\n",
    "    print(rag_file.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d458a-8530-460f-a6a2-c8daae173264",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Direct context retrieval</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbcec9d0-41a1-44e6-8bba-fe6ea094ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct context retrieval\n",
    "response = rag.retrieval_query(\n",
    "    rag_resources=[\n",
    "        rag.RagResource(\n",
    "            rag_corpus=rag_corpus.name,\n",
    "            # Optional: supply IDs from `rag.list_files()`.\n",
    "            # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
    "        )\n",
    "    ],\n",
    "    text=\"What is Cockatoo.AI?\",\n",
    "    similarity_top_k=10,  # Optional\n",
    "    vector_distance_threshold=0.5,  # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd3f7e17-1d8c-4cc4-bf7c-f973a9d9bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contexts {\n",
       "  contexts {\n",
       "    source_uri: \"gs://llm-agent-storage/test.txt\"\n",
       "    text: \"Project `Cockatoo.AI` as a language tutor (focused on listening & speaking)\\r\\nIn the Open issues, please put your name at the end of the task that you are interested in.\\r\\n\\r\\nGoal towards users:\\r\\nSpeak as native as possible to users so users can practice listening as well as speaking.\\r\\nCan adjust the language tutor’s teaching attitude (e.g., Direct vs Plato)\\r\\nCan adjust the language tutor’s speaking/work usage difficulties (e.g., A1, A2, B1, B2, C1 levels) to make learning easier for users.\\r\\nCan discuss the topic the user uploaded/raised and can correspondingly ask appropriate questions to the user.\\r\\nCan practice listening to various tones from AI (e.g., expressing the sentence with happiness, sadness, …, from elderly/adult/woman/man …)\\r\\nGoal towards team members:\\r\\nImprove LLM, voice models, DL, and other framework knowledge/skills.\\r\\nCan use the language tutor to benefit him/herself.\\r\\nLearn CI/CD and do research on improving potential product biasedness and relevant skills.\\r\\nLearn the balance between fine-tuning and our computing powers.\\r\\nHow To (Initiative):\\r\\nUse models (A-B-C as shown below) to form the pipeline as the final language tutor:\\r\\n\\r\\nmodel A: voice to text\\r\\nmodel B(LLM): text à text , might be managed by langChain, Andrew Ng provided 2-3 free short courses.\\r\\nmodel C: text to voice PS: C might be the same as A, needs more research. But the** Goal towards users** should be the top concern when considering which model to use. Besides, better to use open-source models instead of paying premium APIs.\\r\\nFor model C, one may use LangChain to manage LLM. langChain is a tool that helps build automation pipelines, store vector databases (our own data), and automate the monitoring of LLM with various technical strategies as well as prompt engineering. For example, we may manage the prompt strategy by telling LLM to self-supervise the output text quality, ie, tell LLM to redo it again if the previous output was not good.\\r\\n\\r\\nMarket Competitors (for language tutoring)\\r\\nLangotalk (multi-lan)\\r\\nTutor Lily: AI Language Tutor (multi-lan)\\r\\nGood References\\r\\nFor Model A (Speech to text, STT)\\r\\nThe library SpeechRecognition is used for performing speech recognition, with support for several engines and APIs, online and offline.\\r\\nNotebook - Easy Speech-to-Text with Python\\r\\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\\r\\nNotebook - Introduction for Whisper in OpenAI\\r\\nFor Model C (Text to speech, TTS)\\r\\nThe company Evelenlabs has advanced non-open sourced TTS model for multi-languages. It performs well in Chinese (no Taiwanese tone) and English from my perspective, which inlcudes happy/sad/... tones as well as male/female/... sounds.\\r\\nMisc\"\n",
       "    distance: 0.33863286400720694\n",
       "    source_display_name: \"test.txt\"\n",
       "    score: 0.33863286400720694\n",
       "  }\n",
       "  contexts {\n",
       "    source_uri: \"gs://llm-agent-storage/test_local_file.txt\"\n",
       "    text: \"Hello world\"\n",
       "    distance: 0.49883396988384054\n",
       "    source_display_name: \"test_local_file.txt\"\n",
       "    score: 0.49883396988384054\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b057315a-d795-416e-bb65-0df9b5cdecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project `Cockatoo.AI` as a language tutor (focused on listening & speaking)\n",
      "In the Open issues, please put your name at the end of the task that you are interested in.\n",
      "\n",
      "Goal towards users:\n",
      "Speak as native as possible to users so users can practice listening as well as speaking.\n",
      "Can adjust the language tutor’s teaching attitude (e.g., Direct vs Plato)\n",
      "Can adjust the language tutor’s speaking/work usage difficulties (e.g., A1, A2, B1, B2, C1 levels) to make learning easier for users.\n",
      "Can discuss the topic the user uploaded/raised and can correspondingly ask appropriate questions to the user.\n",
      "Can practice listening to various tones from AI (e.g., expressing the sentence with happiness, sadness, …, from elderly/adult/woman/man …)\n",
      "Goal towards team members:\n",
      "Improve LLM, voice models, DL, and other framework knowledge/skills.\n",
      "Can use the language tutor to benefit him/herself.\n",
      "Learn CI/CD and do research on improving potential product biasedness and relevant skills.\n",
      "Learn the balance between fine-tuning and our computing powers.\n",
      "How To (Initiative):\n",
      "Use models (A-B-C as shown below) to form the pipeline as the final language tutor:\n",
      "\n",
      "model A: voice to text\n",
      "model B(LLM): text à text , might be managed by langChain, Andrew Ng provided 2-3 free short courses.\n",
      "model C: text to voice PS: C might be the same as A, needs more research. But the** Goal towards users** should be the top concern when considering which model to use. Besides, better to use open-source models instead of paying premium APIs.\n",
      "For model C, one may use LangChain to manage LLM. langChain is a tool that helps build automation pipelines, store vector databases (our own data), and automate the monitoring of LLM with various technical strategies as well as prompt engineering. For example, we may manage the prompt strategy by telling LLM to self-supervise the output text quality, ie, tell LLM to redo it again if the previous output was not good.\n",
      "\n",
      "Market Competitors (for language tutoring)\n",
      "Langotalk (multi-lan)\n",
      "Tutor Lily: AI Language Tutor (multi-lan)\n",
      "Good References\n",
      "For Model A (Speech to text, STT)\n",
      "The library SpeechRecognition is used for performing speech recognition, with support for several engines and APIs, online and offline.\n",
      "Notebook - Easy Speech-to-Text with Python\n",
      "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.\n",
      "Notebook - Introduction for Whisper in OpenAI\n",
      "For Model C (Text to speech, TTS)\n",
      "The company Evelenlabs has advanced non-open sourced TTS model for multi-languages. It performs well in Chinese (no Taiwanese tone) and English from my perspective, which inlcudes happy/sad/... tones as well as male/female/... sounds.\n",
      "Misc\n"
     ]
    }
   ],
   "source": [
    "print(response.contexts.contexts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3a0bd-7bb9-4b5b-affc-697868b0a491",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Enhance generation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16c61434-854d-43d3-aedd-580d4ab3bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RAG retrieval tool\n",
    "rag_retrieval_tool = Tool.from_retrieval(\n",
    "    retrieval=rag.Retrieval(\n",
    "        source=rag.VertexRagStore(\n",
    "            rag_resources=[\n",
    "                rag.RagResource(\n",
    "                    rag_corpus=rag_corpus.name,  # Currently only 1 corpus is allowed.\n",
    "                    # Optional: supply IDs from `rag.list_files()`.\n",
    "                    # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\n",
    "                )\n",
    "            ],\n",
    "            similarity_top_k=3,  # Optional\n",
    "            vector_distance_threshold=0.5,  # Optional\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02a387ac-9948-4e45-a576-b2dd7e748e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gemini-pro model instance\n",
    "rag_model = GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash-001\", tools=[rag_retrieval_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2edcb885-80cf-432b-a2b6-2d0c4800adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "response = rag_model.generate_content(\"Show me the academic background of John.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff2b9da7-f23b-4f48-bc04-cbb02bc4a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "John has a Master of Science in Computer Science & Information Engineering from National Taiwan University and a Bachelor of Science in Electronic Engineering from National Taiwan University of Science and Technology. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e332da3-0ca2-4cb9-877d-6a2c79db7714",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Delete corpus</font></b>\n",
    "We could use `rag.delete_corpus` to delete the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b0d2a91-4475-40ee-850f-bd07e0e0cefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted the RagCorpus.\n"
     ]
    }
   ],
   "source": [
    "rag.delete_corpus(created_corpus_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dad94-3adb-415b-90bf-6d285cdfa64f",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Intro to Building a Scalable and Modular RAG System with RAG Engine in Vertex AI](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/rag-engine/intro_rag_engine.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
