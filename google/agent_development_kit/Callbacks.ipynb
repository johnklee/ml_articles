{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e204864-3f38-4f56-83ea-8788ce0ef577",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Callbacks: Observe, Customize, and Control Agent Behavior</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39655915-22fe-42a9-930c-a9cf4b5c5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def show_source_code(src_path: str):\n",
    "    source_code = !cat $src_path\n",
    "    display(Markdown(f\"\"\"\n",
    "```python\n",
    "{'\\n'.join(source_code)}\n",
    "```\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fdf77-201b-4ecf-af38-1d9bda1e34fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Introduction: What are Callbacks and Why Use Them?</font></b>\n",
    "<font size='3ptx'><b>Callbacks are a cornerstone feature of ADK, providing a powerful mechanism to hook into an agent's execution process.</b> They allow you to observe, customize, and even control the agent's behavior at specific, predefined points without modifying the core ADK framework code.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5120dc-9e36-4b0c-b5d2-7096da10a24d",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>What are they?</font></b>\n",
    "\n",
    "In essence, callbacks are standard functions that you define. You then associate these functions with an agent when you create it. The ADK framework automatically calls your functions at key stages, letting you observe or intervene. Think of it like checkpoints during the agent's process:\n",
    "* <b><font size='3ptx'>Before the agent starts its main work on a request</font></b>, and after it finishes: When you ask an agent to do something (<font color='brown'>e.g., answer a question</font>), it runs its internal logic to figure out the response.\n",
    "* The <b><font size='3ptx'>Before Agent</font></b> callback executes right before this main work begins for that specific request.\n",
    "* The <b><font size='3ptx'>After Agent</font></b> callback executes right after the agent has finished all its steps for that request and has prepared the final result, but just before the result is returned.\n",
    "* <b><font size='3ptx'>This \"main work\" encompasses the agent's entire process for handling that single request</font></b>. This might involve deciding to call an LLM, actually calling the LLM, deciding to use a tool, using the tool, processing the results, and finally putting together the answer. These callbacks essentially wrap the whole sequence from receiving the input to producing the final output for that one interaction.\n",
    "* <b><font size='3ptx'>Before sending a request to, or after receiving a response from, the Large Language Model (LLM)</font></b>: These callbacks (<b><font size='3ptx'>Before Model</font></b>, <b><font size='3ptx'>After Model</font></b>) allow you to inspect or modify the data going to and coming from the LLM specifically.\n",
    "* <b><font size='3ptx'>Before executing a tool</font></b> (<font color='brown'>like a Python function or another agent</font>) <b><font size='3ptx'>or after it finishes</font></b>: Similarly, <b><font size='3ptx'>Before Tool</font></b> and <b><font size='3ptx'>After Tool</font></b> callbacks give you control points specifically around the execution of tools invoked by the agent.\n",
    "\n",
    "![flow](https://google.github.io/adk-docs/assets/callback_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa709c86-96e0-4659-9d21-2e161b216020",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>Why use them?</font></b> Callbacks unlock significant flexibility and enable advanced agent capabilities:\n",
    "* <b><font size='3ptx'>Observe & Debug</font></b>: Log detailed information at critical steps for monitoring and troubleshooting.\n",
    "* <b><font size='3ptx'>Customize & Control</font></b>: Modify data flowing through the agent (<font color='brown'>like LLM requests or tool results</font>) or even bypass certain steps entirely based on your logic.\n",
    "* <b><font size='3ptx'>Implement Guardrails</font></b>: Enforce safety rules, validate inputs/outputs, or prevent disallowed operations.\n",
    "* <b><font size='3ptx'>Manage State</font></b>: Read or dynamically update the agent's session state during execution.\n",
    "* <b><font size='3ptx'>Integrate & Enhance</font></b>: Trigger external actions (<font color='brown'>API calls, notifications</font>) or add features like caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea63622-5277-4eda-aeca-e37e16e4805d",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>How are they added</font></b>:\n",
    "\n",
    "```python\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmResponse, LlmRequest\n",
    "from typing import Optional\n",
    "\n",
    "# --- Define your callback function ---\n",
    "def my_before_model_logic(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    print(f\"Callback running before model call for agent: {callback_context.agent_name}\")\n",
    "    # ... your custom logic here ...\n",
    "    return None # Allow the model call to proceed\n",
    "\n",
    "# --- Register it during Agent creation ---\n",
    "my_agent = LlmAgent(\n",
    "    name=\"MyCallbackAgent\",\n",
    "    model=\"gemini-2.0-flash\", # Or your desired model\n",
    "    instruction=\"Be helpful.\",\n",
    "    # Other agent parameters...\n",
    "    before_model_callback=my_before_model_logic # Pass the function here\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9fb31-ebb8-4f3b-baad-d1534ac11278",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>The Callback Mechanism: Interception and Control</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/#the-callback-mechanism-interception-and-control)) <font size='3ptx'><b>When the ADK framework encounters a point where a callback can run (<font color='brown'>e.g., just before calling the LLM</font>), it checks if you provided a corresponding callback function for that agent</b>. If you did, the framework executes your function.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3db3b-c89f-4df5-954e-ef4bbfbaad4b",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>Context is Key</font></b>: Your callback function isn't called in isolation. The framework provides special context objects (<font color='brown'>`CallbackContext` or `ToolContext`</font>) as arguments. <b>These objects contain vital information about the current state of the agent's execution, including the invocation details, session state, and potentially references to services like artifacts or memory</b>. You use these context objects to understand the situation and interact with the framework. (<font color='brown'>See the dedicated \"Context Objects\" section for full details</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008842a5-ae0d-43d1-b1b0-6feee72c4bb5",
   "metadata": {},
   "source": [
    "<b><font size='3ptx'>Controlling the Flow (The Core Mechanism)</font></b>: The most powerful aspect of callbacks lies in how their return value influences the agent's subsequent actions. This is how you intercept and control the execution flow:\n",
    "\n",
    "1. <b><font size='3ptx'>`return None` (Allow Default Behavior)</font></b>:\n",
    "    - This is the standard way to signal that your callback has finished its work (<font color='brown'>e.g., logging, inspection, minor modifications to mutable input arguments like `llm_request`</font>) and that the ADK agent should <b>proceed with its normal operation</b>.\n",
    "    - For `before_*` callbacks (`before_agent`, `before_model`, `before_tool`), returning None means the next step in the sequence (<font color='brown'>running the agent logic, calling the LLM, executing the tool</font>) will occur.\n",
    "    - For `after_*` callbacks (`after_agent`, `after_model`, `after_tool`), returning None means the result just produced by the preceding step (<font color='brown'>the agent's output, the LLM's response, the tool's result</font>) will be used as is.\n",
    "2. <b><font size='3ptx'>`return <Specific Object>` (Override Default Behavior)</font></b>:\n",
    "    - <b>Returning a specific type of object</b> (<font color='brown'>instead of `None`</font>) <b>is how you override the ADK agent's default behavior</b>. The framework will use the object you return and skip the step that would normally follow or replace the result that was just generated.\n",
    "    - <b>`before_agent_callback` → `types.Content`</b>: Skips the agent's main execution logic (`_run_async_impl` / `_run_live_impl`). The returned `Content` object is immediately treated as the agent's final output for this turn. <b>Useful for handling simple requests directly or enforcing access control</b>.\n",
    "    - <b>`before_model_callback` → `LlmResponse`</b>: Skips the call to the external Large Language Model. The returned `LlmResponse` object is processed as if it were the actual response from the LLM. <b>Ideal for implementing input guardrails, prompt validation, or serving cached responses</b>.\n",
    "    - <b>`before_tool_callback` → `dict`</b>: Skips the execution of the actual tool function (<font color='brown'>or sub-agent</font>). The returned `dict` is used as the result of the tool call, which is then typically passed back to the LLM. <b>Perfect for validating tool arguments, applying policy restrictions, or returning mocked/cached tool results</b>.\n",
    "    - <b>`after_agent_callback` → `types.Content`</b>: <b>Replaces the `Content` that the agent's run logic just produced</b>.\n",
    "    - <b>`after_model_callback` → `LlmResponse`</b>: Replaces the `LlmResponse` received from the LLM. <b>Useful for sanitizing outputs, adding standard disclaimers, or modifying the LLM's response structure</b>.\n",
    "    - <b>`after_tool_callback` → `dict`</b>: Replaces the `dict` result returned by the tool. <b>Allows for post-processing or standardization of tool outputs before they are sent back to the LLM</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b3815-b66b-4731-ac0a-f01d89428f44",
   "metadata": {},
   "source": [
    "<b>Conceptual Code Example (Guardrail)</b>:\n",
    "This example demonstrates the common pattern for a guardrail using `before_model_callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bbfe5c-aa2b-4ce1-81c4-9c1111bda903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models import LlmResponse, LlmRequest\n",
    "from google.adk.runners import Runner\n",
    "from typing import Optional\n",
    "from google.genai import types \n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
    "APP_NAME = \"guardrail_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04796fd-bd89-4287-9ce3-62dde2a8bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Callback Function ---\n",
    "def simple_before_model_modifier(\n",
    "    callback_context: CallbackContext,\n",
    "    llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"Inspects/modifies the LLM request or skips the call.\"\"\"\n",
    "    agent_name = callback_context.agent_name\n",
    "    print(f\"[Callback] Before model call for agent: {agent_name}\")\n",
    "\n",
    "    # Inspect the last user message in the request contents\n",
    "    last_user_message = \"\"\n",
    "    if llm_request.contents and llm_request.contents[-1].role == 'user':\n",
    "      if llm_request.contents[-1].parts:\n",
    "        last_user_message = llm_request.contents[-1].parts[0].text\n",
    "    print(f\"[Callback] Inspecting last user message: '{last_user_message}'\")\n",
    "\n",
    "    # --- Modification Example ---\n",
    "    # Add a prefix to the system instruction\n",
    "    original_instruction = llm_request.config.system_instruction or types.Content(role=\"system\", parts=[])\n",
    "    prefix = \"[Modified by Callback] \"\n",
    "    # Ensure system_instruction is Content and parts list exists\n",
    "    if not isinstance(original_instruction, types.Content):\n",
    "      # Handle case where it might be a string (though config expects Content)\n",
    "      original_instruction = types.Content(role=\"system\", parts=[types.Part(text=str(original_instruction))])\n",
    "    if not original_instruction.parts:\n",
    "      original_instruction.parts.append(types.Part(text=\"\")) # Add an empty part if none exist\n",
    "\n",
    "    # Modify the text of the first part\n",
    "    modified_text = prefix + (original_instruction.parts[0].text or \"\")\n",
    "    original_instruction.parts[0].text = modified_text\n",
    "    llm_request.config.system_instruction = original_instruction\n",
    "    print(f\"[Callback] Modified system instruction to: '{modified_text}'\")\n",
    "\n",
    "    # --- Skip Example ---\n",
    "    # Check if the last user message contains \"BLOCK\"\n",
    "    if \"BLOCK\" in last_user_message.upper():\n",
    "      print(\"[Callback] 'BLOCK' keyword found. Skipping LLM call.\")\n",
    "      # Return an LlmResponse to skip the actual LLM call\n",
    "      return LlmResponse(\n",
    "          content=types.Content(\n",
    "              role=\"model\",\n",
    "              parts=[types.Part(text=\"LLM call was blocked by `before_model_callback`.\")],\n",
    "          )\n",
    "      )\n",
    "    else:\n",
    "      print(\"[Callback] Proceeding with LLM call.\")\n",
    "      # Return None to allow the (modified) request to go to the LLM\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452ce178-f196-438d-94d3-147d2e3133fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LlmAgent and Assign Callback\n",
    "my_llm_agent = LlmAgent(\n",
    "    name=\"ModelCallbackAgent\",\n",
    "    model=GEMINI_2_FLASH,\n",
    "    instruction=\"You are a helpful assistant.\", # Base instruction\n",
    "    description=\"An LLM agent demonstrating before_model_callback\",\n",
    "    before_model_callback=simple_before_model_modifier # Assign the function here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41da533f-2ea5-48ed-9e3d-3e1bb8e41d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session and Runner\n",
    "async def setup_session_and_runner():\n",
    "  session_service = InMemorySessionService()\n",
    "  session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "  runner = Runner(agent=my_llm_agent, app_name=APP_NAME, session_service=session_service)\n",
    "  return session, runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc330670-7632-4316-a453-d624613722e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Interaction\n",
    "async def call_agent_async(query):\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "  session, runner = await setup_session_and_runner()\n",
    "  events = runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
    "\n",
    "  async for event in events:\n",
    "    if event.is_final_response():\n",
    "      final_response = event.content.parts[0].text\n",
    "      print(\"Agent Response: \", final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf493156-5667-4a55-89dc-5545b5b3264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] Before model call for agent: ModelCallbackAgent\n",
      "[Callback] Inspecting last user message: 'write a joke on BLOCK'\n",
      "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
      "\n",
      "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
      "\n",
      " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
      "[Callback] 'BLOCK' keyword found. Skipping LLM call.\n",
      "Agent Response:  LLM call was blocked by `before_model_callback`.\n"
     ]
    }
   ],
   "source": [
    "await call_agent_async(\"write a joke on BLOCK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa90bb-d023-4e86-acc4-d93555842359",
   "metadata": {},
   "source": [
    "By understanding this mechanism of returning `None` versus returning specific objects, you can precisely control the agent's execution path, making callbacks an essential tool for building sophisticated and reliable agents with ADK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabffb33-7231-46d9-a0a5-bfff2be3a9b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Types of Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/types-of-callbacks/#types-of-callbacks)) <font size='3ptx'><b>The framework provides different types of callbacks that trigger at various stages of an agent's execution</b>. Understanding when each callback fires and what context it receives is key to using them effectively.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274849c4-130b-4ccc-82c4-06bbc87c7f4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Agent Lifecycle Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/types-of-callbacks/#agent-lifecycle-callbacks)) <b><font size='3ptx'>These callbacks are available on any agent that inherits from <font color='blue'>BaseAgent</font></font></b> (<font color='brown'>including `LlmAgent`, `SequentialAgent`, `ParallelAgent`, `LoopAgent`, etc</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56ee23-5194-496b-924d-d3841cc04af3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Before Agent Callback</b>\n",
    "<font size='3ptx'><b>When</b></font>: Called immediately before the agent's `_run_async_impl` (or `_run_live_impl`) method is executed. It runs after the agent's <font color='blue'><b>InvocationContext</b></font> is created but before its core logic begins.\n",
    "\n",
    "<font size='3ptx'><b>Purpose</b></font>: Ideal for setting up resources or state needed only for this specific agent's run, performing validation checks on the session state (`callback_context.state`) before execution starts, logging the entry point of the agent's activity, or potentially modifying the invocation context before the core logic uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f1b199-06d2-4acd-836c-9c9acce458e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from collections import namedtuple\n",
       "from functools import cache\n",
       "\n",
       "# ADK Imports\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.agents.callback_context import CallbackContext\n",
       "from google.adk.runners import InMemoryRunner, Runner  # Use InMemoryRunner\n",
       "from google.adk.sessions import Session\n",
       "from google.genai import types # For types.Content\n",
       "from typing import Optional, NamedTuple\n",
       "\n",
       "# Define the model - Use the specific model name requested\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  app_name: str\n",
       "  user_id: str\n",
       "  session_id_run: str\n",
       "  session_id_skip: str\n",
       "  runner: Runner\n",
       "\n",
       "\n",
       "# --- 1. Define the Callback Function ---\n",
       "def check_if_agent_should_run(callback_context: CallbackContext) -> Optional[types.Content]:\n",
       "  \"\"\"\n",
       "  Logs entry and checks 'skip_llm_agent' in session state.\n",
       "  If True, returns Content to skip the agent's execution.\n",
       "  If False or not present, returns None to allow execution.\n",
       "  \"\"\"\n",
       "  agent_name = callback_context.agent_name\n",
       "  invocation_id = callback_context.invocation_id\n",
       "  current_state = callback_context.state.to_dict()\n",
       "\n",
       "  print(f\"\\n[Callback] Entering agent: {agent_name} (Inv: {invocation_id})\")\n",
       "  print(f\"[Callback] Current State: {current_state}\")\n",
       "\n",
       "  # Check the condition in session state dictionary\n",
       "  if current_state.get(\"skip_llm_agent\", False):\n",
       "    print(f\"[Callback] State condition 'skip_llm_agent=True' met: Skipping agent {agent_name}.\")\n",
       "    # Return Content to skip the agent's run\n",
       "    return types.Content(\n",
       "        parts=[types.Part(text=f\"Agent {agent_name} skipped by `before_agent_callback` due to state.\")],\n",
       "        role=\"model\" # Assign model role to the overriding response\n",
       "    )\n",
       "\n",
       "  print(f\"[Callback] State condition not met: Proceeding with agent {agent_name}.\")\n",
       "  # Return None to allow the LlmAgent's normal execution\n",
       "  return None\n",
       "\n",
       "\n",
       "# --- 2. Setup Agent with Callback ---\n",
       "llm_agent_with_before_cb = LlmAgent(\n",
       "    name=\"MyControlledAgent\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are a concise assistant.\",\n",
       "    description=\"An LLM agent demonstrating stateful before_agent_callback\",\n",
       "    before_agent_callback=check_if_agent_should_run # Assign the callback\n",
       ")\n",
       "\n",
       "\n",
       "@cache\n",
       "async def initialize_session_and_runner():\n",
       "  app_name = \"before_agent_demo\"\n",
       "  user_id = \"test_user\"\n",
       "  session_id_run = \"session_will_run\"\n",
       "  session_id_skip = \"session_will_skip\"\n",
       "\n",
       "  # Use InMemoryRunner - it includes InMemorySessionService\n",
       "  runner = InMemoryRunner(agent=llm_agent_with_before_cb, app_name=app_name)\n",
       "  # Get the bundled session service to create sessions\n",
       "  session_service = runner.session_service\n",
       "\n",
       "  # Create session 1: Agent will run (default empty state)\n",
       "  await session_service.create_session(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id=session_id_run\n",
       "      # No initial state means 'skip_llm_agent' will be False in the callback check\n",
       "  )\n",
       "\n",
       "  # Create session 2: Agent will be skipped (state has skip_llm_agent=True)\n",
       "  await session_service.create_session(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id=session_id_skip,\n",
       "      state={\"skip_llm_agent\": True} # Set the state flag here\n",
       "  )\n",
       "\n",
       "  return AgentEnv(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id_run=session_id_run,\n",
       "      session_id_skip=session_id_skip,\n",
       "      runner=runner)\n",
       "\n",
       "\n",
       "async def query(text: str, agent_env, session_id):\n",
       "  async for event in agent_env.runner.run_async(\n",
       "      user_id=agent_env.user_id,\n",
       "      session_id=session_id,\n",
       "      new_message=types.Content(\n",
       "          role=\"user\", parts=[types.Part(text=text)])\n",
       "  ):\n",
       "    # Print final output (either from LLM or callback override)\n",
       "    if event.is_final_response() and event.content:\n",
       "      print(f\"Final Output: [{event.author}] {event.content.parts[0].text.strip()}\")\n",
       "    elif event.is_error():\n",
       "      print(f\"Error Event: {event.error_details}\")\n",
       "\n",
       "\n",
       "async def main():\n",
       "  agent_env = await initialize_session_and_runner()\n",
       "\n",
       "  # --- Scenario 1: Run where callback allows agent execution ---\n",
       "  await query(\n",
       "      'Hello, please respond.',\n",
       "      agent_env,\n",
       "      agent_env.session_id_run)\n",
       "\n",
       "  # --- Scenario 2: Run where callback intercepts and skips agent ---\n",
       "  await query(\n",
       "      \"This message won't reach the LLM.\",\n",
       "      agent_env,\n",
       "      agent_env.session_id_skip)\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(main())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b6d734-1576-4f89-a43c-0b548ec8204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import callback_example1\n",
    "\n",
    "agent_env = await callback_example1.initialize_session_and_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "418ebeb1-1816-44c6-88ca-fa855eb8a1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Callback] Entering agent: MyControlledAgent (Inv: e-936f9530-2eef-4ae4-b22d-d630e0f433b6)\n",
      "[Callback] Current State: {}\n",
      "[Callback] State condition not met: Proceeding with agent MyControlledAgent.\n",
      "Final Output: [MyControlledAgent] Hello! How can I assist you?\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 1: Run where callback allows agent execution ---\n",
    "await callback_example1.query(\n",
    "    'Hello, please respond.',\n",
    "    agent_env,\n",
    "    agent_env.session_id_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f6adc02-a8f0-4154-85f6-9e0a8d1e7504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Callback] Entering agent: MyControlledAgent (Inv: e-09e4f747-403c-4fe2-aaae-626206898b49)\n",
      "[Callback] Current State: {'skip_llm_agent': True}\n",
      "[Callback] State condition 'skip_llm_agent=True' met: Skipping agent MyControlledAgent.\n",
      "Final Output: [MyControlledAgent] Agent MyControlledAgent skipped by `before_agent_callback` due to state.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 2: Run where callback intercepts and skips agent ---\n",
    "await callback_example1.query(\n",
    "    \"This message won't reach the LLM.\",\n",
    "    agent_env,\n",
    "    agent_env.session_id_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d28f9b-cf3a-4542-9161-3ad77ba5973c",
   "metadata": {},
   "source": [
    "<b>Note on the `before_agent_callback` Example</b>:\n",
    "* <b><font size='3ptx'>What it Shows</font></b>: This example demonstrates the `before_agent_callback`. This callback runs right before the agent's main processing logic starts for a given request.\n",
    "* <b><font size='3ptx'>How it Works</font></b>: The callback function (`check_if_agent_should_run`) looks at a flag (`skip_llm_agent`) in the session's state.\n",
    "    - If the flag is True, the callback returns a <b><font color='blue'>types.Content</font></b> object. This tells the ADK framework to skip the agent's main execution entirely and use the callback's returned content as the final response.\n",
    "    - If the flag is False (or not set), the callback returns `None` or an empty object. This tells the ADK framework to proceed with the agent's normal execution (<font color='brown'>calling the LLM in this case</font>).\n",
    "* <b><font size='3ptx'>Expected Outcome</font></b>: You'll see two scenarios:\n",
    "    1. In the session with the `skip_llm_agent: True` state, the agent's LLM call is bypassed, and the output comes directly from the callback (\"Agent... skipped...\").\n",
    "    2. In the session without that state flag, the callback allows the agent to run, and you see the actual response from the LLM (e.g., \"Hello!\").\n",
    "* <b><font size='3ptx'>Understanding Callbacks</font></b>: This highlights how `before_` callbacks act as gatekeepers, allowing you to intercept execution before a major step and potentially prevent it based on checks (<font color='brown'>like state, input validation, permissions</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e6af1-84ff-4498-bdd9-ba690cf952ac",
   "metadata": {},
   "source": [
    "#### <b>After Agent Callback</b>\n",
    "<b><font size='3ptx'>When</font></b>: Called immediately after the agent's `_run_async_impl` (or `_run_live_impl`) method successfully completes. It does not run if the agent was skipped due to `before_agent_callback` returning content or if `end_invocation` was set during the agent's run.\n",
    "\n",
    "<b><font size='3ptx'>Purpose</font></b>: Useful for cleanup tasks, post-execution validation, logging the completion of an agent's activity, modifying final state, or augmenting/replacing the agent's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29aa9040-f604-4db3-a45c-196256fe507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from functools import cache\n",
       "import time\n",
       "\n",
       "# ADK Imports\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.agents.callback_context import CallbackContext\n",
       "from google.adk.runners import InMemoryRunner, Runner  # Use InMemoryRunner\n",
       "from google.genai import types # For types.Content\n",
       "from typing import Optional, NamedTuple\n",
       "\n",
       "# Define the model - Use the specific model name requested\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  app_name: str\n",
       "  user_id: str\n",
       "  session_id_normal: str\n",
       "  session_id_modify: str\n",
       "  runner: Runner\n",
       "\n",
       "\n",
       "# --- 1. Define the Callback Function ---\n",
       "def modify_output_after_agent(callback_context: CallbackContext) -> Optional[types.Content]:\n",
       "  \"\"\"\n",
       "  Logs exit from an agent and checks 'add_concluding_note' in session state.\n",
       "  If True, returns new Content to *replace* the agent's original output.\n",
       "  If False or not present, returns None, allowing the agent's original output to be used.\n",
       "  \"\"\"\n",
       "  agent_name = callback_context.agent_name\n",
       "  invocation_id = callback_context.invocation_id\n",
       "  current_state = callback_context.state.to_dict()\n",
       "\n",
       "  print(f\"\\n[Callback] Exiting agent: {agent_name} (Inv: {invocation_id})\")\n",
       "  print(f\"[Callback] Current State: {current_state}\")\n",
       "\n",
       "  # Example: Check state to decide whether to modify the final output\n",
       "  if current_state.get(\"add_concluding_note\", False):\n",
       "    print(f\"[Callback] State condition 'add_concluding_note=True' met: Replacing agent {agent_name}'s output.\")\n",
       "    # Return Content to *replace* the agent's own output\n",
       "    return types.Content(\n",
       "        parts=[types.Part(text=f\"Concluding note added by `after_agent_callback`, replacing original output.\")],\n",
       "        role=\"model\" # Assign model role to the overriding response\n",
       "    )\n",
       "  else:\n",
       "    print(f\"[Callback] State condition not met: Using agent {agent_name}'s original output.\")\n",
       "    # Return None - the agent's output produced just before this callback will be used.\n",
       "    return None\n",
       "\n",
       "\n",
       "# --- 2. Setup Agent with Callback ---\n",
       "llm_agent_with_after_cb = LlmAgent(\n",
       "    name=\"MySimpleAgentWithAfter\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are a simple agent. Just say 'Processing complete!'\",\n",
       "    description=\"An LLM agent demonstrating after_agent_callback for output modification\",\n",
       "    after_agent_callback=modify_output_after_agent # Assign the callback here\n",
       ")\n",
       "\n",
       "\n",
       "@cache\n",
       "async def initialize_session_and_runner():\n",
       "  app_name = \"before_agent_demo\"\n",
       "  user_id = \"test_user\"\n",
       "  session_id_normal = \"session_run_normally\"\n",
       "  session_id_modify = \"session_modify_output\"\n",
       "\n",
       "  # Use InMemoryRunner - it includes InMemorySessionService\n",
       "  runner = InMemoryRunner(agent=llm_agent_with_after_cb, app_name=app_name)\n",
       "  # Get the bundled session service to create sessions\n",
       "  session_service = runner.session_service\n",
       "\n",
       "  # Create session 1: Agent will run (default empty state)\n",
       "  await session_service.create_session(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id=session_id_normal\n",
       "      # No initial state means 'skip_llm_agent' will be False in the callback check\n",
       "  )\n",
       "\n",
       "  # Create session 2: Agent will be skipped (state has skip_llm_agent=True)\n",
       "  await session_service.create_session(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id=session_id_modify,\n",
       "      state={\"add_concluding_note\": True} # Set the state flag here\n",
       "  )\n",
       "\n",
       "  return AgentEnv(\n",
       "      app_name=app_name,\n",
       "      user_id=user_id,\n",
       "      session_id_normal=session_id_normal,\n",
       "      session_id_modify=session_id_modify,\n",
       "      runner=runner)\n",
       "\n",
       "\n",
       "async def query(text: str, agent_env, session_id):\n",
       "  start_time = time.time()\n",
       "  async for event in agent_env.runner.run_async(\n",
       "      user_id=agent_env.user_id,\n",
       "      session_id=session_id,\n",
       "      new_message=types.Content(\n",
       "          role=\"user\", parts=[types.Part(text=text)])\n",
       "  ):\n",
       "    # Print final output (either from LLM or callback override)\n",
       "    if event.is_final_response() and event.content:\n",
       "      print(f\"Final Output: [{event.author}] {event.content.parts[0].text.strip()}\")\n",
       "    elif event.is_error():\n",
       "      print(f\"Error Event: {event.error_details}\")\n",
       "\n",
       "  spent_time_sec = time.time() - start_time\n",
       "  print(f'[Query] Spent time {spent_time_sec:.02f}s!')\n",
       "\n",
       "\n",
       "async def main():\n",
       "  agent_env = await initialize_session_and_runner()\n",
       "\n",
       "  # --- Scenario 1: Run where callback allows agent's original output ---\n",
       "  print(\n",
       "      \"\\n\" + \"=\"*20 +\n",
       "      f\" SCENARIO 1: Running Agent on Session '{agent_env.session_id_normal}' \" +\n",
       "      \"(Should Use Original Output) \" + \"=\"*20)\n",
       "  await query(\n",
       "      'Process this please.',\n",
       "      agent_env,\n",
       "      agent_env.session_id_normal)\n",
       "\n",
       "  # --- Scenario 2: Run where callback replaces the agent's output ---\n",
       "  print(\n",
       "      \"\\n\" + \"=\"*20 +\n",
       "      f\" SCENARIO 2: Running Agent on Session '{agent_env.session_id_modify}' \" +\n",
       "      \"(Should Replace Output) \" + \"=\"*20)\n",
       "  await query(\n",
       "      'Process this and add note.',\n",
       "      agent_env,\n",
       "      agent_env.session_id_modify)\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(main())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "696fa6ea-9d61-44ee-836e-d851365eb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import callback_example2\n",
    "\n",
    "agent_env = await callback_example2.initialize_session_and_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d96bde82-b2ca-4755-b0d0-3d05e6673828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: [MySimpleAgentWithAfter] Processing complete!\n",
      "\n",
      "[Callback] Exiting agent: MySimpleAgentWithAfter (Inv: e-0c208c70-6670-4c2e-90e3-f0072734a621)\n",
      "[Callback] Current State: {}\n",
      "[Callback] State condition not met: Using agent MySimpleAgentWithAfter's original output.\n",
      "[Query] Spent time 1.60s!\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 1: Run where callback allows agent's original output ---\n",
    "await callback_example2.query(\n",
    "      'Process this please.',\n",
    "      agent_env,\n",
    "      agent_env.session_id_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a9f3c49-28d8-428f-a17f-4d1b5f07ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: [MySimpleAgentWithAfter] Processing complete!\n",
      "\n",
      "[Callback] Exiting agent: MySimpleAgentWithAfter (Inv: e-b0213bf2-c143-47fb-943f-c905627137ce)\n",
      "[Callback] Current State: {'add_concluding_note': True}\n",
      "[Callback] State condition 'add_concluding_note=True' met: Replacing agent MySimpleAgentWithAfter's output.\n",
      "Final Output: [MySimpleAgentWithAfter] Concluding note added by `after_agent_callback`, replacing original output.\n",
      "[Query] Spent time 1.52s!\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 2: Run where callback replaces the agent's output ---\n",
    "await callback_example2.query(\n",
    "      'Process this and add note.',\n",
    "      agent_env,\n",
    "      agent_env.session_id_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca9398-4b8f-4858-a18b-2c18eb03f70e",
   "metadata": {},
   "source": [
    "<b>Note on the after_agent_callback Example:</b>\n",
    "* <b><font size='3ptx'>What it Shows</font></b>: This example demonstrates the `after_agent_callback`. This callback runs right after the agent's main processing logic has finished and produced its result, but before that result is finalized and returned.\n",
    "* <b><font size='3ptx'>How it Works</font></b>: The callback function (`modify_output_after_agent`) checks a flag (`add_concluding_note`) in the session's state.\n",
    "* <b><font size='3ptx'>Expected Outcome</font></b>: You'll see two scenarios:\n",
    "    1. In the session without the `add_concluding_note`: True state, the callback allows the agent's original output (\"Processing complete!\") to be used.\n",
    "    2. In the session with that state flag, the callback intercepts the agent's original output and replaces it with its own message (\"Concluding note added...\").\n",
    "* <b><font size='3ptx'>Understanding Callbacks</font></b>: This highlights how `after_` callbacks allow post-processing or modification. You can inspect the result of a step (<font color='brown'>the agent's run</font>) and decide whether to let it pass through, change it, or completely replace it based on your logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504bc86e-d6fc-430c-951d-58ca2951db3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>LLM Interaction Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/types-of-callbacks/#llm-interaction-callbacks)) <font size='3ptx'><b>These callbacks are specific to `LlmAgent` and provide hooks around the interaction with the Large Language Model.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e412c1f-5f86-465c-858f-430f17f759a2",
   "metadata": {},
   "source": [
    "#### <b>Before Model Callback</b>\n",
    "<b><font size='3ptx'>When</font></b>: Called just before the `generate_content_async` (<font color='brown'>or equivalent</font>) request is sent to the LLM within an `LlmAgent`'s flow.\n",
    "\n",
    "<b><font size='3ptx'>Purpose</font></b>: Allows inspection and modification of the request going to the LLM. Use cases include adding dynamic instructions, injecting few-shot examples based on state, modifying model config, implementing guardrails (<font color='brown'>like profanity filters</font>), or implementing request-level caching.\n",
    "\n",
    "<b><font size='3ptx'>Return Value Effect</font></b>:\n",
    "If the callback returns `None`, the LLM continues its normal workflow. If the callback returns an <b><font color='blue'>LlmResponse</font></b> object, then the call to the LLM is skipped. The returned <b><font color='blue'>LlmResponse</font></b> is used directly as if it came from the model. This is powerful for implementing guardrails or caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49bda37d-d181-496a-8eef-9899bdad50ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from collections import namedtuple\n",
       "from functools import cache\n",
       "import time\n",
       "\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.agents.callback_context import CallbackContext\n",
       "from google.adk.models import LlmResponse, LlmRequest\n",
       "from google.adk.runners import Runner\n",
       "from google.adk.sessions import Session\n",
       "from typing import Optional, NamedTuple\n",
       "from google.genai import types\n",
       "from google.adk.sessions import InMemorySessionService\n",
       "\n",
       "\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "APP_NAME = \"guardrail_app\"\n",
       "USER_ID = \"user_1\"\n",
       "SESSION_ID = \"session_001\"\n",
       "\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  runner: Runner\n",
       "  session: Session\n",
       "\n",
       "\n",
       "# --- Define the Callback Function ---\n",
       "def simple_before_model_modifier(\n",
       "    callback_context: CallbackContext,\n",
       "    llm_request: LlmRequest) -> Optional[LlmResponse]:\n",
       "  \"\"\"Inspects/modifies the LLM request or skips the call.\"\"\"\n",
       "  agent_name = callback_context.agent_name\n",
       "  print(f\"[Callback] Before model call for agent: {agent_name}\")\n",
       "\n",
       "  # Inspect the last user message in the request contents\n",
       "  last_user_message = \"\"\n",
       "  if llm_request.contents and llm_request.contents[-1].role == 'user':\n",
       "    if llm_request.contents[-1].parts:\n",
       "      last_user_message = llm_request.contents[-1].parts[0].text\n",
       "  print(f\"[Callback] Inspecting last user message: '{last_user_message}'\")\n",
       "\n",
       "  # --- Modification Example ---\n",
       "  # Add a prefix to the system instruction\n",
       "  original_instruction = llm_request.config.system_instruction or types.Content(role=\"system\", parts=[])\n",
       "  postfix = \" If the query is a math problem. Respond with 'I do not know.'.\"\n",
       "  # Ensure system_instruction is Content and parts list exists\n",
       "  if not isinstance(original_instruction, types.Content):\n",
       "    # Handle case where it might be a string (though config expects Content)\n",
       "    original_instruction = types.Content(\n",
       "        role=\"system\", parts=[types.Part(text=str(original_instruction))])\n",
       "  if not original_instruction.parts:\n",
       "    original_instruction.parts.append(types.Part(text=\"\")) # Add an empty part if none exist\n",
       "\n",
       "  # Modify the text of the first part\n",
       "  modified_text = (original_instruction.parts[0].text or \"\") + postfix\n",
       "  original_instruction.parts[0].text = modified_text\n",
       "  llm_request.config.system_instruction = original_instruction\n",
       "  print(f\"[Callback] Modified system instruction to: '{modified_text}'\")\n",
       "\n",
       "  # --- Skip Example ---\n",
       "  # Check if the last user message contains \"BLOCK\"\n",
       "  if \"BLOCK\" in last_user_message.upper():\n",
       "    print(\"[Callback] 'BLOCK' keyword found. Skipping LLM call.\")\n",
       "    # Return an LlmResponse to skip the actual LLM call\n",
       "    return LlmResponse(\n",
       "        content=types.Content(\n",
       "            role=\"model\",\n",
       "            parts=[types.Part(text=\"LLM call was blocked by before_model_callback.\")],\n",
       "        )\n",
       "    )\n",
       "  else:\n",
       "    print(\"[Callback] Proceeding with LLM call.\")\n",
       "    # Return None to allow the (modified) request to go to the LLM\n",
       "    return None\n",
       "\n",
       "\n",
       "# Create LlmAgent and Assign Callback\n",
       "my_llm_agent = LlmAgent(\n",
       "    name=\"ModelCallbackAgent\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are a helpful assistant.\", # Base instruction\n",
       "    description=\"An LLM agent demonstrating before_model_callback\",\n",
       "    before_model_callback=simple_before_model_modifier, # Assign the function here\n",
       ")\n",
       "\n",
       "\n",
       "# Session and Runner\n",
       "async def setup_session_and_runner():\n",
       "  session_service = InMemorySessionService()\n",
       "  session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
       "  runner = Runner(agent=my_llm_agent, app_name=APP_NAME, session_service=session_service)\n",
       "  return AgentEnv(runner=runner, session=session)\n",
       "\n",
       "\n",
       "# Agent Interaction\n",
       "async def call_agent_async(query):\n",
       "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
       "  agent_env = await setup_session_and_runner()\n",
       "  events = agent_env.runner.run_async(\n",
       "      user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
       "\n",
       "  async for event in events:\n",
       "    if event.is_final_response():\n",
       "      final_response = event.content.parts[0].text\n",
       "      print(\"Agent Response: \", final_response)\n",
       "\n",
       "\n",
       "async def main():\n",
       "  # --- Scenario 1: Modify system message\n",
       "  await call_agent_async(\"I am John. What is my name?\")\n",
       "  await call_agent_async(\"How much is 1 + 1?\")\n",
       "\n",
       "  # --- Scenario 2: Blocked LLM call\n",
       "  await call_agent_async(\"write a joke on BLOCK\")\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(main())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example3.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43073526-7bb0-49f9-981e-60cbe6b957d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import callback_example3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f205828-a0c4-40ed-8d39-2e008d6d334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] Before model call for agent: ModelCallbackAgent\n",
      "[Callback] Inspecting last user message: 'I am John. What is my name?'\n",
      "[Callback] Modified system instruction to: 'You are a helpful assistant.\n",
      "\n",
      "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
      "\n",
      " The description about you is \"An LLM agent demonstrating before_model_callback\" If the query is a math problem. Respond with 'I do not know.'.'\n",
      "[Callback] Proceeding with LLM call.\n",
      "Agent Response:  Your name is John.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 1.1: Modify system message and process question normally\n",
    "await callback_example3.call_agent_async(\"I am John. What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d33ef582-5ddb-4891-bcb5-6d44b1e29715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] Before model call for agent: ModelCallbackAgent\n",
      "[Callback] Inspecting last user message: 'How much is 1 + 1?'\n",
      "[Callback] Modified system instruction to: 'You are a helpful assistant.\n",
      "\n",
      "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
      "\n",
      " The description about you is \"An LLM agent demonstrating before_model_callback\" If the query is a math problem. Respond with 'I do not know.'.'\n",
      "[Callback] Proceeding with LLM call.\n",
      "Agent Response:  I do not know.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 1.2: Modify system message and response will be impacted by the postfix\n",
    "#                   1 + 1 should be 2. But `I do not know` is forced to be responded.\n",
    "await callback_example3.call_agent_async(\"How much is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c031e53-ed07-4262-a252-1663dead5001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] Before model call for agent: ModelCallbackAgent\n",
      "[Callback] Inspecting last user message: 'Write a joke on BLOCK'\n",
      "[Callback] Modified system instruction to: '[Modified by Callback] You are a helpful assistant.\n",
      "\n",
      "You are an agent. Your internal name is \"ModelCallbackAgent\".\n",
      "\n",
      " The description about you is \"An LLM agent demonstrating before_model_callback\"'\n",
      "[Callback] 'BLOCK' keyword found. Skipping LLM call.\n",
      "Agent Response:  LLM call was blocked by `before_model_callback`.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario 2: Blocked LLM call\n",
    "await call_agent_async(\"Write a joke on BLOCK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b97fcf-40d8-4bfe-b477-9bd91d2be588",
   "metadata": {},
   "source": [
    "#### <b>After Model Callback</b>\n",
    "<b><font size='3ptx'>When</font></b>: Called just after a response (`LlmResponse`) is received from the LLM, before it's processed further by the invoking agent.\n",
    "\n",
    "<b><font size='3ptx'>Purpose</font></b>: Allows inspection or modification of the raw LLM response. Use cases include\n",
    "- logging model outputs,\n",
    "- reformatting responses,\n",
    "- censoring sensitive information generated by the model,\n",
    "- parsing structured data from the LLM response and storing it in callback_context.state\n",
    "- or handling specific error codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aec07c17-5e3f-42e9-89a1-009cc20f865e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from collections import namedtuple\n",
       "import copy\n",
       "from functools import cache\n",
       "import time\n",
       "\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.agents.callback_context import CallbackContext\n",
       "from google.adk.runners import Runner\n",
       "from google.adk.sessions import Session\n",
       "from typing import Optional, NamedTuple\n",
       "from google.genai import types\n",
       "from google.adk.sessions import InMemorySessionService\n",
       "from google.adk.models import LlmResponse\n",
       "\n",
       "\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "APP_NAME = \"guardrail_app\"\n",
       "USER_ID = \"user_1\"\n",
       "SESSION_ID = \"session_001\"\n",
       "\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  runner: Runner\n",
       "  session: Session\n",
       "\n",
       "\n",
       "# --- Define the Callback Function ---\n",
       "def simple_after_model_modifier(\n",
       "    callback_context: CallbackContext,\n",
       "    llm_response: LlmResponse) -> Optional[LlmResponse]:\n",
       "  \"\"\"Inspects/modifies the LLM response after it's received.\"\"\"\n",
       "  agent_name = callback_context.agent_name\n",
       "  print(f\"[Callback] After model call for agent: {agent_name}\")\n",
       "\n",
       "  # --- Inspection ---\n",
       "  original_text = \"\"\n",
       "  if llm_response.content and llm_response.content.parts:\n",
       "    # Assuming simple text response for this example\n",
       "    if llm_response.content.parts[0].text:\n",
       "      original_text = llm_response.content.parts[0].text\n",
       "      print(\n",
       "          \"[Callback] Inspected original response text: \"\n",
       "          f\"'{original_text[:100]}'...\") # Log snippet\n",
       "    elif llm_response.content.parts[0].function_call:\n",
       "      print(\n",
       "          \"[Callback] Inspected response: Contains function call \"\n",
       "          \"'{llm_response.content.parts[0].function_call.name}'. No text modification.\")\n",
       "      return None  # Don't modify tool calls in this example\n",
       "    else:\n",
       "      print(\"[Callback] Inspected response: No text content found.\")\n",
       "      return None\n",
       "  elif llm_response.error_message:\n",
       "    print(\n",
       "        \"[Callback] Inspected response: Contains error \"\n",
       "        f\"'{llm_response.error_message}'. No modification.\")\n",
       "    return None\n",
       "  else:\n",
       "    print(\"[Callback] Inspected response: Empty LlmResponse.\")\n",
       "    return None # Nothing to modify\n",
       "\n",
       "  # --- Modification Example ---\n",
       "  # Replace \"joke\" with \"funny story\" (case-insensitive)\n",
       "  search_term = \"joke\"\n",
       "  replace_term = \"funny story\"\n",
       "  if search_term in original_text.lower():\n",
       "    print(f\"[Callback] Found '{search_term}'. Modifying response.\")\n",
       "    modified_text = original_text.replace(search_term, replace_term)\n",
       "    modified_text = modified_text.replace(\n",
       "        search_term.capitalize(), replace_term.capitalize())  # Handle capitalization\n",
       "\n",
       "    # Create a NEW LlmResponse with the modified content\n",
       "    # Deep copy parts to avoid modifying original if other callbacks exist\n",
       "    modified_parts = [copy.deepcopy(part) for part in llm_response.content.parts]\n",
       "    modified_parts[0].text = modified_text # Update the text in the copied part\n",
       "\n",
       "    new_response = LlmResponse(\n",
       "        content=types.Content(role=\"model\", parts=modified_parts),\n",
       "        # Copy other relevant fields if necessary, e.g., grounding_metadata\n",
       "        grounding_metadata=llm_response.grounding_metadata)\n",
       "    print(f\"[Callback] Returning modified response.\")\n",
       "    return new_response # Return the modified response\n",
       "  else:\n",
       "    print(f\"[Callback] '{search_term}' not found. Passing original response through.\")\n",
       "    # Return None to use the original llm_response\n",
       "    return None\n",
       "\n",
       "\n",
       "# Create LlmAgent and Assign Callback\n",
       "my_llm_agent = LlmAgent(\n",
       "    name=\"AfterModelCallbackAgent\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are a helpful assistant.\",\n",
       "    description=\"An LLM agent demonstrating after_model_callback\",\n",
       "    after_model_callback=simple_after_model_modifier # Assign the function here\n",
       ")\n",
       "\n",
       "\n",
       "# Session and Runner\n",
       "async def setup_session_and_runner():\n",
       "  session_service = InMemorySessionService()\n",
       "  session = await session_service.create_session(\n",
       "      app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
       "  runner = Runner(\n",
       "      agent=my_llm_agent, app_name=APP_NAME, session_service=session_service)\n",
       "  return AgentEnv(runner=runner, session=session)\n",
       "\n",
       "\n",
       "# Agent Interaction\n",
       "async def call_agent_async(query):\n",
       "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
       "  agent_env = await setup_session_and_runner()\n",
       "  events = agent_env.runner.run_async(\n",
       "      user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
       "\n",
       "  async for event in events:\n",
       "    if event.is_final_response():\n",
       "      final_response = event.content.parts[0].text\n",
       "      print(\"Agent Response: \", final_response)\n",
       "\n",
       "\n",
       "async def main():\n",
       "  await call_agent_async('Write two time the word \"joke\"')\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(main())\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e806d4fc-1e26-4a36-9017-a56a436c7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] After model call for agent: AfterModelCallbackAgent\n",
      "[Callback] Inspected original response text: 'joke joke\n",
      "'...\n",
      "[Callback] Found 'joke'. Modifying response.\n",
      "[Callback] Returning modified response.\n",
      "Agent Response:  funny story funny story\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import callback_example4\n",
    "\n",
    "await callback_example4.call_agent_async('Write two time the word \"joke\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3468e6-262f-42f6-a117-c4649e135320",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Tool Execution Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/types-of-callbacks/#tool-execution-callbacks)) <font size='3ptx'><b>These callbacks are also specific to `LlmAgent` and trigger around the execution of tools (<font color='brown'>including `FunctionTool`, `AgentTool`, etc.</font>) that the LLM might request.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898e2da-fa7b-4602-b5a2-c28cb85eb658",
   "metadata": {},
   "source": [
    "#### <b>Before Tool Callback</b>\n",
    "<b><font size='3ptx'>When</font></b>: Called just before a specific tool's `run_async` method is invoked, after the LLM has generated a function call for it.\n",
    "\n",
    "<b><font size='3ptx'>Purpose</font></b>: Allows inspection and modification of tool arguments, performing authorization checks before execution, logging tool usage attempts, or implementing tool-level caching.\n",
    "\n",
    "<b><font size='3ptx'>Return Value Effect</font></b>:\n",
    "1. If the callback returns `None`, the tool's `run_async` method is executed with the (<font color='brown'>potentially modified</font>) args.\n",
    "2. If a dictionary is returned, the tool's `run_async` method is skipped. The returned dictionary is used directly as the result of the tool call. This is useful for caching or overriding tool behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20c06845-6d30-4950-aaf0-8d0c3809a4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from collections import namedtuple\n",
       "from functools import cache\n",
       "import time\n",
       "\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.runners import Runner\n",
       "from typing import Optional\n",
       "from google.genai import types\n",
       "from google.adk.sessions import InMemorySessionService, Session\n",
       "from google.adk.tools import FunctionTool\n",
       "from google.adk.tools.tool_context import ToolContext\n",
       "from google.adk.tools.base_tool import BaseTool\n",
       "from typing import Dict, Any, NamedTuple\n",
       "\n",
       "\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "APP_NAME = \"guardrail_app\"\n",
       "USER_ID = \"user_1\"\n",
       "SESSION_ID = \"session_001\"\n",
       "\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  runner: Runner\n",
       "  session: Session\n",
       "\n",
       "\n",
       "def get_capital_city(country: str) -> str:\n",
       "  \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
       "  print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
       "  country_capitals = {\n",
       "      \"united states\": \"Washington, D.C.\",\n",
       "      \"canada\": \"Ottawa\",\n",
       "      \"france\": \"Paris\",\n",
       "      \"germany\": \"Berlin\",\n",
       "      \"taiwan\": \"taipei\",\n",
       "  }\n",
       "  return country_capitals.get(\n",
       "      country.lower(), f\"Capital not found for {country}\")\n",
       "\n",
       "\n",
       "capital_tool = FunctionTool(func=get_capital_city)\n",
       "\n",
       "\n",
       "def simple_before_tool_modifier(\n",
       "    tool: BaseTool, args: Dict[str, Any],\n",
       "    tool_context: ToolContext) -> Optional[Dict]:\n",
       "  \"\"\"Inspects/modifies tool args or skips the tool call.\"\"\"\n",
       "  agent_name = tool_context.agent_name\n",
       "  tool_name = tool.name\n",
       "  print(f\"[Callback] Before tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
       "  print(f\"[Callback] Original args: {args}\")\n",
       "\n",
       "  if tool_name == 'get_capital_city' and args.get('country', '').lower() == 'canada':\n",
       "    print(\"[Callback] Detected 'Canada'. Modifying args to 'France'.\")\n",
       "    args['country'] = 'France'\n",
       "    print(f\"[Callback] Modified args: {args}\")\n",
       "    return None\n",
       "\n",
       "  # If the tool is 'get_capital_city' and country is 'BLOCK'\n",
       "  if tool_name == 'get_capital_city' and args.get('country', '').upper() == 'BLOCK':\n",
       "    print(\"[Callback] Detected 'BLOCK'. Skipping tool execution.\")\n",
       "    return {\"result\": \"Tool execution was blocked by `before_tool_callback`.\"}\n",
       "\n",
       "  print(\"[Callback] Proceeding with original or previously modified args.\")\n",
       "  return None\n",
       "\n",
       "\n",
       "my_llm_agent = LlmAgent(\n",
       "    name=\"ToolCallbackAgent\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are an agent that can find capital cities. Use the get_capital_city tool.\",\n",
       "    description=\"An LLM agent demonstrating before_tool_callback\",\n",
       "    tools=[capital_tool],\n",
       "    before_tool_callback=simple_before_tool_modifier)\n",
       "\n",
       "\n",
       "# Session and Runner\n",
       "async def setup_session_and_runner():\n",
       "  session_service = InMemorySessionService()\n",
       "  session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
       "  runner = Runner(\n",
       "      agent=my_llm_agent,\n",
       "      app_name=APP_NAME,\n",
       "      session_service=session_service)\n",
       "  return AgentEnv(\n",
       "      session=session,\n",
       "      runner=runner)\n",
       "\n",
       "\n",
       "# Agent Interaction\n",
       "async def call_agent_async(query):\n",
       "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
       "  agent_env = await setup_session_and_runner()\n",
       "  events = agent_env.runner.run_async(\n",
       "      user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
       "\n",
       "  async for event in events:\n",
       "    if event.is_final_response():\n",
       "      final_response = event.content.parts[0].text\n",
       "      print(\"Agent Response: \", final_response)\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(call_agent_async(\"Canada\"))\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example5.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8fe0799-939b-4040-af9b-6419fbf41819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Callback] Before tool call for tool 'get_capital_city' in agent 'ToolCallbackAgent'\n",
      "[Callback] Original args: {'country': 'Canada'}\n",
      "[Callback] Detected 'Canada'. Modifying args to 'France'.\n",
      "[Callback] Modified args: {'country': 'France'}\n",
      "--- Tool 'get_capital_city' executing with country: France ---\n",
      "Agent Response:  I was asked about Canada, but the tool request was for France.  The capital of Canada is Ottawa.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import callback_example5\n",
    "\n",
    "await callback_example5.call_agent_async(\"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da36e1-8002-4a70-9989-01445a2c6744",
   "metadata": {},
   "source": [
    "#### <b>After Tool Callback</b>\n",
    "<b><font size='3ptx'>When</font></b>: Called just after the tool's `run_async` method completes successfully.\n",
    "\n",
    "<b><font size='3ptx'>Purpose</font></b>: Allows inspection and modification of the tool's result before it's sent back to the LLM (<font color='brown'>potentially after summarization</font>). Useful for logging tool results, post-processing or formatting results, or saving specific parts of the result to the session state.\n",
    "\n",
    "<b><font size='3ptx'>Return Value Effect</font></b>:\n",
    "1. If the callback returns `None`, the original `tool_response` is used.\n",
    "2. If a new dictionary is returned, it replaces the original `tool_response`. This allows modifying or filtering the result seen by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51028f91-0c39-4570-aa5f-b2ec91730979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "```python\n",
       "#!/usr/bin/env python\n",
       "import asyncio\n",
       "from collections import namedtuple\n",
       "from copy import deepcopy\n",
       "from functools import cache\n",
       "import time\n",
       "\n",
       "from google.adk.agents import LlmAgent\n",
       "from google.adk.agents.callback_context import CallbackContext\n",
       "from google.adk.runners import Runner\n",
       "from google.adk.sessions import InMemorySessionService, Session\n",
       "from google.adk.tools import FunctionTool\n",
       "from google.adk.tools.tool_context import ToolContext\n",
       "from typing import Any, Dict, Optional, NamedTuple\n",
       "from google.genai import types\n",
       "from google.adk.models import LlmResponse\n",
       "from google.adk.tools.base_tool import BaseTool\n",
       "\n",
       "\n",
       "GEMINI_2_FLASH=\"gemini-2.0-flash\"\n",
       "APP_NAME = \"guardrail_app\"\n",
       "USER_ID = \"user_1\"\n",
       "SESSION_ID = \"session_001\"\n",
       "\n",
       "\n",
       "# Defined the namedtuple\n",
       "class AgentEnv(NamedTuple):\n",
       "  runner: Runner\n",
       "  session: Session\n",
       "\n",
       "\n",
       "# --- Define a Simple Tool Function (Same as before) ---\n",
       "def get_capital_city(country: str) -> Dict[str, Any]:\n",
       "  \"\"\"Retrieves the capital city of a given country.\"\"\"\n",
       "  print(f\"--- Tool 'get_capital_city' executing with country: {country} ---\")\n",
       "  country_capitals = {\n",
       "      \"united states\": \"Washington, D.C.\",\n",
       "      \"canada\": \"Ottawa\",\n",
       "      \"france\": \"Paris\",\n",
       "      \"germany\": \"Berlin\",\n",
       "      \"taiwan\": \"taipei\",\n",
       "  }\n",
       "\n",
       "  return {\n",
       "      \"result\": country_capitals.get(country.lower(), f\"Capital not found for {country}\")}\n",
       "\n",
       "\n",
       "# --- Wrap the function into a Tool ---\n",
       "capital_tool = FunctionTool(func=get_capital_city)\n",
       "\n",
       "\n",
       "# --- Define the Callback Function ---\n",
       "def simple_after_tool_modifier(\n",
       "    tool: BaseTool,\n",
       "    args: Dict[str, Any],\n",
       "    tool_context: ToolContext,\n",
       "    tool_response: Dict\n",
       ") -> Optional[Dict]:\n",
       "  \"\"\"Inspects/modifies the tool result after execution.\"\"\"\n",
       "  agent_name = tool_context.agent_name\n",
       "  tool_name = tool.name\n",
       "  print(f\"[Callback] After tool call for tool '{tool_name}' in agent '{agent_name}'\")\n",
       "  print(f\"[Callback] Args used: {args}\")\n",
       "  print(f\"[Callback] Original tool_response: {tool_response}\")\n",
       "\n",
       "  # Default structure for function tool results is {\"result\": <return_value>}\n",
       "  original_result_value = tool_response.get(\"result\", \"\")\n",
       "  # original_result_value = tool_response\n",
       "\n",
       "  # --- Modification Example ---\n",
       "  # If the tool was 'get_capital_city' and result is 'Washington, D.C.'\n",
       "  if tool_name == 'get_capital_city' and original_result_value == \"Washington, D.C.\":\n",
       "    print(\"[Callback] Detected 'Washington, D.C.'. Modifying tool response.\")\n",
       "\n",
       "    # IMPORTANT: Create a new dictionary or modify a copy\n",
       "    modified_response = deepcopy(tool_response)\n",
       "    modified_response[\"result\"] = f\"{original_result_value} (Note: This is the capital of the USA).\"\n",
       "    modified_response[\"note_added_by_callback\"] = True # Add extra info if needed\n",
       "\n",
       "    print(f\"[Callback] Modified tool_response: {modified_response}\")\n",
       "    return modified_response # Return the modified dictionary\n",
       "\n",
       "  print(\"[Callback] Passing original tool response through.\")\n",
       "  # Return None to use the original tool_response\n",
       "  return None\n",
       "\n",
       "\n",
       "# Create LlmAgent and Assign Callback\n",
       "my_llm_agent = LlmAgent(\n",
       "    name=\"AfterToolCallbackAgent\",\n",
       "    model=GEMINI_2_FLASH,\n",
       "    instruction=\"You are an agent that finds capital cities using the get_capital_city tool. Report the result clearly.\",\n",
       "    description=\"An LLM agent demonstrating after_tool_callback\",\n",
       "    tools=[capital_tool], # Add the tool\n",
       "    after_tool_callback=simple_after_tool_modifier, # Assign the callback\n",
       ")\n",
       "\n",
       "\n",
       "# Session and Runner\n",
       "async def setup_session_and_runner():\n",
       "  session_service = InMemorySessionService()\n",
       "  session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
       "  runner = Runner(\n",
       "      agent=my_llm_agent,\n",
       "      app_name=APP_NAME,\n",
       "      session_service=session_service)\n",
       "  return AgentEnv(\n",
       "      session=session,\n",
       "      runner=runner)\n",
       "\n",
       "\n",
       "# Agent Interaction\n",
       "async def call_agent_async(query):\n",
       "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
       "  agent_env = await setup_session_and_runner()\n",
       "  events = agent_env.runner.run_async(\n",
       "      user_id=USER_ID, session_id=SESSION_ID, new_message=content)\n",
       "\n",
       "  async for event in events:\n",
       "    if event.is_final_response():\n",
       "      final_response = event.content.parts[0].text\n",
       "      print(\"Agent Response: \", final_response)\n",
       "\n",
       "\n",
       "if __name__ == '__main__':\n",
       "  asyncio.run(call_agent_async(\"united states\"))\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_source_code('callback_example6.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cab9b72-2a2c-4ce0-ab01-eb1b5eae992e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool 'get_capital_city' executing with country: united states ---\n",
      "[Callback] After tool call for tool 'get_capital_city' in agent 'AfterToolCallbackAgent'\n",
      "[Callback] Args used: {'country': 'united states'}\n",
      "[Callback] Original tool_response: {'result': 'Washington, D.C.'}\n",
      "[Callback] Detected 'Washington, D.C.'. Modifying tool response.\n",
      "[Callback] Modified tool_response: {'result': 'Washington, D.C. (Note: This is the capital of the USA).', 'note_added_by_callback': True}\n",
      "Agent Response:  The capital city of the United States is Washington, D.C. (Note: This is the capital of the USA.).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import callback_example6\n",
    "\n",
    "\n",
    "await callback_example6.call_agent_async(\"united states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3461da2-5489-41d3-ad7b-1bc18f52dff8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Design Patterns and Best Practices for Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/design-patterns-and-best-practices/#design-patterns-and-best-practices-for-callbacks)) <font size='3ptx'><b>Callbacks offer powerful hooks into the agent lifecycle. </b>Here are common design patterns illustrating how to leverage them effectively in ADK, followed by best practices for implementation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607557c-2cec-4724-bca0-d97130951757",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Design Patterns</font></b>\n",
    "<b><font size='3ptx'>These patterns demonstrate typical ways to enhance or control agent behavior using callbacks:</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56181324-91b9-4c9e-bb03-73f582afe9f6",
   "metadata": {},
   "source": [
    "#### <b>1. Guardrails & Policy Enforcement</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Intercept requests before they reach the LLM or tools to enforce rules.\n",
    "* <b><font size='3ptx'>How</font></b>: Use `before_model_callback` to inspect the `LlmRequest` prompt or `before_tool_callback` to inspect tool arguments. If a policy violation is detected (<font color='brown'>e.g., forbidden topics, profanity</font>), return a predefined response (`LlmResponse` or `dict`) to block the operation and optionally update `context.state` to log the violation.\n",
    "* <b><font size='3ptx'>Example</font></b>: A `before_model_callback` checks `llm_request.contents` for sensitive keywords and returns a standard \"Cannot process this request\" `LlmResponse` if found, preventing the LLM call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ff9dfa-8490-4c8e-bf91-69ecd3bbbf04",
   "metadata": {},
   "source": [
    "#### <b>2. Dynamic State Management</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Read from and write to session state within callbacks to make agent behavior context-aware and pass data between steps.\n",
    "* <b><font size='3ptx'>How</font></b>: Access `callback_context.state` or `tool_context.state`. Modifications (`state['key'] = value`) are automatically tracked in the subsequent `Event.actions.state_delta` for persistence by the <b><font color='blue'>SessionService</font></b>.\n",
    "* <b><font size='3ptx'>Example</font></b>: An `after_tool_callback` saves a `transaction_id` from the tool's result to `tool_context.state['last_transaction_id']`. A later `before_agent_callback` might read `state['user_tier']` to customize the agent's greeting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec318a2d-67c1-4bf4-b887-4ba2d7a86c63",
   "metadata": {},
   "source": [
    "#### <b>3. Logging and Monitoring</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Add detailed logging at specific lifecycle points for observability and debugging.\n",
    "* <b><font size='3ptx'>How</font></b>: Implement callbacks (e.g., `before_agent_callback`, `after_tool_callback`, `after_model_callback`) to print or send structured logs containing information like agent name, tool name, invocation ID, and relevant data from the context or arguments.\n",
    "* <b><font size='3ptx'>Example</font></b>: Log messages like `INFO: [Invocation: e-123] Before Tool: search_api - Args: {'query': 'ADK'}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c09653-c961-4794-b7e6-9f7d50f61ff0",
   "metadata": {},
   "source": [
    "#### <b>4. Caching</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Avoid redundant LLM calls or tool executions by caching results.\n",
    "* <b><font size='3ptx'>How</font></b>: In `before_model_callback` or `before_tool_callback`, generate a cache key based on the request/arguments. Check `context.state` (<font color='brown'>or an external cache</font>) for this key. If found, return the cached `LlmResponse` or result directly, skipping the actual operation. If not found, allow the operation to proceed and use the corresponding `after_` callback (`after_model_callback`, `after_tool_callback`) to store the new result in the cache using the key.\n",
    "* <b><font size='3ptx'>Example</font></b>: `before_tool_callback` for `get_stock_price(symbol)` checks `state[f\"cache:stock:{symbol}\"]`. If present, returns the cached price; otherwise, allows the API call and `after_tool_callback` saves the result to the state key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c2b7b-cb60-4dbb-af9c-97eb5d1513e7",
   "metadata": {},
   "source": [
    "#### <b>5. Request/Response Modification</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Alter data just before it's sent to the LLM/tool or just after it's received.\n",
    "* <b><font size='3ptx'>How</font></b>:\n",
    "    - <b>`before_model_callback`</b>: Modify `llm_request` (<font color='brown'>e.g., add system instructions based on state</font>).\n",
    "    - <b>`after_model_callback`</b>: Modify the returned `LlmResponse` (<font color='brown'>e.g., format text, filter content</font>).\n",
    "    - <b>`before_tool_callback`</b>: Modify the tool `args` dictionary.\n",
    "    - <b>`after_tool_callback`</b>: Modify the `tool_response` dictionary.\n",
    "* <b><font size='3ptx'>Example</font></b>: `before_model_callback` appends \"`User language preference: Spanish`\" to `llm_request.config.system_instruction` if `context.state['lang'] == 'es'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f1dc9-6265-42bb-9f05-61abf6e8a055",
   "metadata": {},
   "source": [
    "#### <b>6. Conditional Skipping of Steps</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Prevent standard operations (<font color='brown'>agent run, LLM call, tool execution</font>) based on certain conditions.\n",
    "* <b><font size='3ptx'>How</font></b>: Return a value from a `before_` callback (`Content` from `before_agent_callback`, `LlmResponse` from `before_model_callback`, `dict` from `before_tool_callback`). The framework interprets this returned value as the result for that step, skipping the normal execution.\n",
    "* <b><font size='3ptx'>Example</font></b>: `before_tool_callback` checks `tool_context.state['api_quota_exceeded']`. If True, it returns `{'error': 'API quota exceeded'}`, preventing the actual tool function from running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e8b0d-eed6-45c8-a349-880de3120c9a",
   "metadata": {},
   "source": [
    "#### <b>7. Tool-Specific Actions (Authentication & Summarization Control)</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Handle actions specific to the tool lifecycle, primarily authentication and controlling LLM summarization of tool results.\n",
    "* <b><font size='3ptx'>How</font></b>: Use `ToolContext` within tool callbacks (`before_tool_callback`, `after_tool_callback`).\n",
    "    - <b>Authentication</b>: Call `tool_context.request_credential(auth_config)` in `before_tool_callback` if credentials are required but not found (<font color='brown'>e.g., via `tool_context.get_auth_response` or state check</font>). This initiates the auth flow.\n",
    "    - <b>Summarization</b>: Set `tool_context.actions.skip_summarization = True` if the raw dictionary output of the tool should be passed back to the LLM or potentially displayed directly, bypassing the default LLM summarization step.\n",
    "* <b><font size='3ptx'>Example</font></b>: A `before_tool_callback` for a secure API checks for an auth token in state; if missing, it calls `request_credential`. An `after_tool_callback` for a tool returning structured JSON might set `skip_summarization = True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe55885-dd86-4ca0-b371-f795abb064b8",
   "metadata": {},
   "source": [
    "#### <b>8. Artifact Handling</b>\n",
    "* <b><font size='3ptx'>Pattern</font></b>: Save or load session-related files or large data blobs during the agent lifecycle.\n",
    "* <b><font size='3ptx'>How</font></b>: Use `callback_context.save_artifact` / `await tool_context.save_artifact` to store data (<font color='brown'>e.g., generated reports, logs, intermediate data</font>). Use `load_artifact` to retrieve previously stored artifacts. Changes are tracked via `Event.actions.artifact_delta`.\n",
    "* <b><font size='3ptx'>Example</font></b>: An `after_tool_callback` for a \"generate_report\" tool saves the output file using `await tool_context.save_artifact(\"report.pdf\", report_part)`. A `before_agent_callback` might load a configuration artifact using `callback_context.load_artifact(\"agent_config.json\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6aa730-ebe0-4c62-958d-2c3047531939",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Best Practices for Callbacks</font></b>\n",
    "([source](https://google.github.io/adk-docs/callbacks/design-patterns-and-best-practices/#best-practices-for-callbacks))\n",
    "* <b><font size='3ptx'>Keep Focused</font></b>: Design each callback for a single, well-defined purpose (e.g., just logging, just validation). Avoid monolithic callbacks.\n",
    "* <b><font size='3ptx'>Mind Performance</font></b>: Callbacks execute synchronously within the agent's processing loop. <font color='darkred'>Avoid long-running or blocking operations</font> (<font color='brown'>network calls, heavy computation</font>). Offload if necessary, but be aware this adds complexity.\n",
    "* <b><font size='3ptx'>Handle Errors Gracefully</font></b>: Use `try...except/catch` blocks within your callback functions. Log errors appropriately and decide if the agent invocation should halt or attempt recovery. Don't let callback errors crash the entire process.\n",
    "* <b><font size='3ptx'>Manage State Carefully</font></b>:\n",
    "    - <b>Be deliberate about reading from and writing to `context.state`</.b>. Changes are immediately visible within the current invocation and persisted at the end of the event processing.\n",
    "    - <b>Use specific state keys</b> rather than modifying broad structures to avoid unintended side effects.\n",
    "    - <b>Consider using state prefixes</b> (`State.APP_PREFIX`, `State.USER_PREFIX`, `State.TEMP_PREFIX`) for clarity, especially with persistent `SessionService` implementations.\n",
    "* <b><font size='3ptx'>Consider Idempotency</font></b>: If a callback performs actions with external side effects (<font color='brown'>e.g., incrementing an external counter</font>), design it to be idempotent (<font color='brown'>safe to run multiple times with the same input</font>) if possible, to handle potential retries in the framework or your application.\n",
    "* <b><font size='3ptx'>Test Thoroughly</font></b>: Unit test your callback functions using mock context objects. Perform integration tests to ensure callbacks function correctly within the full agent flow.\n",
    "* <b><font size='3ptx'>Ensure Clarity</font></b>: Use descriptive names for your callback functions. Add clear docstrings explaining their purpose, when they run, and any side effects (<font color='brown'>especially state modifications</font>).\n",
    "* <b><font size='3ptx'>Use Correct Context Type</font></b>: Always use the specific context type provided (`CallbackContext` for agent/model, `ToolContext` for tools) to ensure access to the appropriate methods and properties.\n",
    "\n",
    "By applying these patterns and best practices, you can effectively use callbacks to create more robust, observable, and customized agent behaviors in ADK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
