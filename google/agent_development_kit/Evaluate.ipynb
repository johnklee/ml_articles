{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d99c706-4966-4f62-899b-cf4d9f2797d1",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Why Evaluate Agents</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/)) <font size='3ptx'><b>In traditional software development, unit tests and integration tests provide confidence that code functions as expected and remains stable through changes.</b> These tests provide a clear \"pass/fail\" signal, guiding further development. However, <b>LLM agents introduce a level of variability that makes traditional testing approaches insufficient</b>.</font>\n",
    "\n",
    "Due to the probabilistic nature of models, deterministic \"pass/fail\" assertions are often unsuitable for evaluating agent performance. Instead, <b>we need qualitative evaluations of both the final output and the agent's trajectory - the sequence of steps taken to reach the solution. This involves assessing the quality of the agent's decisions, its reasoning process, and the final result</b>.\n",
    "\n",
    "This may seem like a lot of extra work to set up, but the investment of automating evaluations pays off quickly. If you intend to progress beyond prototype, this is a highly recommended best practice.\n",
    "![flow](https://google.github.io/adk-docs/assets/evaluate_agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cc41b-291b-49f6-892a-557eb0f729b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>Preparing for Agent Evaluations</font></b>\n",
    "Before automating agent evaluations, define clear objectives and success criteria:\n",
    "* <b><font size='3ptx'>Define Success</font></b>: What constitutes a successful outcome for your agent?\n",
    "* <b><font size='3ptx'>Identify Critical Tasks</font></b>: What are the essential tasks your agent must accomplish?\n",
    "* <b><font size='3ptx'>Choose Relevant Metrics</font></b>: What metrics will you track to measure performance?\n",
    "\n",
    "These considerations will guide the creation of evaluation scenarios and enable effective monitoring of agent behavior in real-world deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d08ea-7370-4e5a-bd60-8fca02d57497",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>What to Evaluate?</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#what-to-evaluate)) <font size='3ptx'><b>To bridge the gap between a proof-of-concept and a production-ready AI agent, a robust and automated evaluation framework is essential.</b> Unlike evaluating generative models, where the focus is primarily on the final output, agent evaluation requires a deeper understanding of the decision-making process.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9f857-071a-4e79-ab5c-b95fc156977e",
   "metadata": {},
   "source": [
    "Agent evaluation can be broken down into two components:\n",
    "1. <b><font size='3ptx'>Evaluating Trajectory and Tool Use</font></b>: Analyzing the steps an agent takes to reach a solution, including its choice of tools, strategies, and the efficiency of its approach.\n",
    "2. <b><font size='3ptx'>Evaluating the Final Response</font></b>: Assessing the quality, relevance, and correctness of the agent's final output.\n",
    "\n",
    "<b>The trajectory is just a list of steps the agent took before it returned to the user</b>. We can compare that against the list of steps we expect the agent to have taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1a131-342b-46e7-ab91-c9ffd538783d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Evaluating trajectory and tool use</font></b>\n",
    "<font size='3ptx'><b>Before responding to a user, an agent typically performs a series of actions, which we refer to as a 'trajectory.'</b> It might compare the user input with session history to disambiguate a term, or lookup a policy document, search a knowledge base or invoke an API to save a ticket. We call this a ‘trajectory’ of actions.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340aa1db-ec09-4533-892d-09b387acbf01",
   "metadata": {},
   "source": [
    "Evaluating an agent's performance requires comparing its actual trajectory to an expected, or ideal, one. This comparison can reveal errors and inefficiencies in the agent's process. <b>The expected trajectory represents the ground truth -- the list of steps we anticipate the agent should take</b>.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "# Trajectory evaluation will compare\n",
    "expected_steps = [\"determine_intent\", \"use_tool\", \"review_results\", \"report_generation\"]\n",
    "actual_steps = [\"determine_intent\", \"use_tool\", \"review_results\", \"report_generation\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb44694-a7a3-4da0-aad7-a5f9af3e47e9",
   "metadata": {},
   "source": [
    "Several ground-truth-based trajectory evaluations exist:\n",
    "1. <b><font size='3ptx'>Exact match</font></b>: Requires a perfect match to the ideal trajectory.\n",
    "2. <b><font size='3ptx'>In-order match</font></b>: Requires the correct actions in the correct order, allows for extra actions.\n",
    "3. <b><font size='3ptx'>Any-order match</font></b>: Requires the correct actions in any order, allows for extra actions.\n",
    "4. <b><font size='3ptx'>Precision</font></b>: Measures the relevance/correctness of predicted actions.\n",
    "5. <b><font size='3ptx'>Recall</font></b>: Measures how many essential actions are captured in the prediction.\n",
    "6. <b><font size='3ptx'>Single-tool use</font></b>: Checks for the inclusion of a specific action.\n",
    "\n",
    "<b>Choosing the right evaluation metric depends on the specific requirements and goals of your agent.</b> For instance, in high-stakes scenarios, an exact match might be crucial, while in more flexible situations, an in-order or any-order match might suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2a38f-57e9-4f10-8192-04d64213f6a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <b><font color='darkblue'>How Evaluation works with the ADK</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#how-evaluation-works-with-the-adk)) <font size='3ptx'><b>The ADK offers two methods for evaluating agent performance against predefined datasets and evaluation criteria.</b> While conceptually similar, they differ in the amount of data they can process, which typically dictates the appropriate use case for each.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c09ea-3d12-456d-8fe7-9c758ab8f66c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>First approach: Using a test file</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#first-approach-using-a-test-file)) <font size='3ptx'><b>This approach involves creating individual test files, each representing a single, simple agent-model interaction (a `session`). It's most effective during active agent development, serving as a form of unit testing.</b> Each test file contains a single session, which may consist of multiple turns. A turn represents a single interaction between the user and the agent.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccb9d5-be4e-4408-8f6e-99c754f7669b",
   "metadata": {},
   "source": [
    "Each turn includes:\n",
    "* <b><font size='3ptx'>User Content</font></b>: The user issued query.\n",
    "* <b><font size='3ptx'>Expected Intermediate Tool Use Trajectory</font></b>: The tool calls we expect the agent to make in order to respond correctly to the user query.\n",
    "* <b><font size='3ptx'>Expected Intermediate Agent Responses</font></b>: These are the natural language responses that the agent (<font color='brown'>or sub-agents</font>) generates as it moves towards generating a final answer. These natural language responses are usually an artifact of an multi-agent system, where your root agent depends on sub-agents to achieve a goal. These intermediate responses, may or may not be of interest to the end user, but for a developer/owner of the system, are of critical importance, as they <b>give you the confidence that the agent went through the right path to generate final response</b>.\n",
    "* <b><font size='3ptx'>Final Response</font></b>: The expected final response from the agent.\n",
    "\n",
    "You can give the file any name for example `evaluation.test.json`.The framework only checks for the `.test.json` suffix, and the preceding part of the filename is not constrained. Here is a test file with a few examples:\n",
    "\n",
    "<b><font color='darkred'>NOTE</font></b>: The test files are now backed by a formal Pydantic data model. The two key schema files are [Eval Set](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_set.py) and [Eval Case](https://github.com/google/adk-python/blob/main/src/google/adk/evaluation/eval_case.py).\n",
    "\n",
    "```json\n",
    "# Do note that some fields are removed for sake of making this doc readable.\n",
    "{\n",
    "  \"eval_set_id\": \"home_automation_agent_light_on_off_set\",\n",
    "  \"name\": \"\",\n",
    "  \"description\": \"This is an eval set that is used for unit testing `x` behavior of the Agent\",\n",
    "  \"eval_cases\": [\n",
    "    {\n",
    "      \"eval_id\": \"eval_case_id\",\n",
    "      \"conversation\": [\n",
    "        {\n",
    "          \"invocation_id\": \"b7982664-0ab6-47cc-ab13-326656afdf75\", # Unique identifier for the invocation.\n",
    "          \"user_content\": { # Content provided by the user in this invocation. This is the query.\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"Turn off device_2 in the Bedroom.\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": \"user\"\n",
    "          },\n",
    "          \"final_response\": { # Final response from the agent that acts as a reference of benchmark.\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"I have set the device_2 status to off.\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": \"model\"\n",
    "          },\n",
    "          \"intermediate_data\": {\n",
    "            \"tool_uses\": [ # Tool use trajectory in chronological order.\n",
    "              {\n",
    "                \"args\": {\n",
    "                  \"location\": \"Bedroom\",\n",
    "                  \"device_id\": \"device_2\",\n",
    "                  \"status\": \"OFF\"\n",
    "                },\n",
    "                \"name\": \"set_device_info\"\n",
    "              }\n",
    "            ],\n",
    "            \"intermediate_responses\": [] # Any intermediate sub-agent responses.\n",
    "          },\n",
    "        }\n",
    "      ],\n",
    "      \"session_input\": { # Initial session input.\n",
    "        \"app_name\": \"home_automation_agent\",\n",
    "        \"user_id\": \"test_user\",\n",
    "        \"state\": {}\n",
    "      },\n",
    "    }\n",
    "  ],\n",
    "}\n",
    "```\n",
    "\n",
    "Test files can be organized into folders. Optionally, a folder can also include a <font color='olive'>test_config.json</font> file that specifies the evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc9474-da46-4b86-90f4-396bcbc370fe",
   "metadata": {},
   "source": [
    "#### <b>How to migrate test files not backed by the Pydantic schema?</b>\n",
    "<b><font color='darkred'>NOTE</font></b>: If your test files don't adhere to EvalSet schema file, then this section is relevant to you.\n",
    "\n",
    "Please use `AgentEvaluator.migrate_eval_data_to_new_schema` to migrate your existing `*.test.json` files to the Pydantic backed schema.\n",
    "\n",
    "The utility takes your current test data file and an optional initial session file, and generates a single output json file with data serialized in the new format. Given that the new schema is more cohesive, both the old test data file and initial session file can be ignored (<font color='brown'>or removed.</font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef84393-2eb2-42ef-a1af-e7e45e6b9ba6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Second approach: Using An Evalset File</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#second-approach-using-an-evalset-file)) <font size='3ptx'><b>The evalset approach utilizes a dedicated dataset called an \"evalset\" for evaluating agent-model interactions. Similar to a test file, the evalset contains example interactions.</b> However, an evalset can contain multiple, potentially lengthy sessions, making it ideal for simulating complex, multi-turn conversations. Due to its ability to represent complex sessions, the evalset is well-suited for integration tests. These tests are typically run less frequently than unit tests due to their more extensive nature.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9db2e-7433-4cab-8962-fb4cad861b2a",
   "metadata": {},
   "source": [
    "<b>An evalset file contains multiple \"evals,\" each representing a distinct session. Each eval consists of one or more \"turns,\" which include the user query, expected tool use, expected intermediate agent responses, and a reference response</b>. These fields have the same meaning as they do in the test file approach. Each eval is identified by a unique name. Furthermore, each eval includes an associated initial session state.\n",
    "\n",
    "<b>Creating evalsets manually can be complex, therefore UI tools are provided to help capture relevant sessions and easily convert them into evals within your evalset</b>. Learn more about using the web UI for evaluation below. Here is an example evalset containing two sessions:\n",
    "\n",
    "```json\n",
    "# Do note that some fields are removed for sake of making this doc readable.\n",
    "{\n",
    "  \"eval_set_id\": \"eval_set_example_with_multiple_sessions\",\n",
    "  \"name\": \"Eval set with multiple sessions\",\n",
    "  \"description\": \"This eval set is an example that shows that an eval set can have more than one session.\",\n",
    "  \"eval_cases\": [\n",
    "    {\n",
    "      \"eval_id\": \"session_01\",\n",
    "      \"conversation\": [\n",
    "        {\n",
    "          \"invocation_id\": \"e-0067f6c4-ac27-4f24-81d7-3ab994c28768\",\n",
    "          \"user_content\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"What can you do?\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": \"user\"\n",
    "          },\n",
    "          \"final_response\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "\n",
    "                \"text\": \"I can roll dice of different sizes and check if numbers are prime.\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": null\n",
    "          },\n",
    "          \"intermediate_data\": {\n",
    "            \"tool_uses\": [],\n",
    "            \"intermediate_responses\": []\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "      \"session_input\": {\n",
    "        \"app_name\": \"hello_world\",\n",
    "        \"user_id\": \"user\",\n",
    "        \"state\": {}\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      \"eval_id\": \"session_02\",\n",
    "      \"conversation\": [\n",
    "        {\n",
    "          \"invocation_id\": \"e-92d34c6d-0a1b-452a-ba90-33af2838647a\",\n",
    "          \"user_content\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"Roll a 19 sided dice\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": \"user\"\n",
    "          },\n",
    "          \"final_response\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"I rolled a 17.\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": null\n",
    "          },\n",
    "          \"intermediate_data\": {\n",
    "            \"tool_uses\": [],\n",
    "            \"intermediate_responses\": []\n",
    "          },\n",
    "        },\n",
    "        {\n",
    "          \"invocation_id\": \"e-bf8549a1-2a61-4ecc-a4ee-4efbbf25a8ea\",\n",
    "          \"user_content\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"Roll a 10 sided dice twice and then check if 9 is a prime or not\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": \"user\"\n",
    "          },\n",
    "          \"final_response\": {\n",
    "            \"parts\": [\n",
    "              {\n",
    "                \"text\": \"I got 4 and 7 from the dice roll, and 9 is not a prime number.\\n\"\n",
    "              }\n",
    "            ],\n",
    "            \"role\": null\n",
    "          },\n",
    "          \"intermediate_data\": {\n",
    "            \"tool_uses\": [\n",
    "              {\n",
    "                \"id\": \"adk-1a3f5a01-1782-4530-949f-07cf53fc6f05\",\n",
    "                \"args\": {\n",
    "                  \"sides\": 10\n",
    "                },\n",
    "                \"name\": \"roll_die\"\n",
    "              },\n",
    "              {\n",
    "                \"id\": \"adk-52fc3269-caaf-41c3-833d-511e454c7058\",\n",
    "                \"args\": {\n",
    "                  \"sides\": 10\n",
    "                },\n",
    "                \"name\": \"roll_die\"\n",
    "              },\n",
    "              {\n",
    "                \"id\": \"adk-5274768e-9ec5-4915-b6cf-f5d7f0387056\",\n",
    "                \"args\": {\n",
    "                  \"nums\": [\n",
    "                    9\n",
    "                  ]\n",
    "                },\n",
    "                \"name\": \"check_prime\"\n",
    "              }\n",
    "            ],\n",
    "            \"intermediate_responses\": [\n",
    "              [\n",
    "                \"data_processing_agent\",\n",
    "                [\n",
    "                  {\n",
    "                    \"text\": \"I have rolled a 10 sided die twice. The first roll is 5 and the second roll is 3.\\n\"\n",
    "                  }\n",
    "                ]\n",
    "              ]\n",
    "            ]\n",
    "          },\n",
    "        }\n",
    "      ],\n",
    "      \"session_input\": {\n",
    "        \"app_name\": \"hello_world\",\n",
    "        \"user_id\": \"user\",\n",
    "        \"state\": {}\n",
    "      },\n",
    "    }\n",
    "  ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c094b8-e0f3-427e-8281-792d08871d37",
   "metadata": {},
   "source": [
    "#### <b>How to migrate eval set files not backed by the Pydantic schema?</b>\n",
    "<b><font color='darkred'>NOTE</font></b>: If your eval set files don't adhere to EvalSet schema file, then this section is relevant to you.\n",
    "\n",
    "Based on who is maintaining the eval set data, there are two routes:\n",
    "1. <font size='3ptx'><b>Eval set data maintained by ADK UI</b> If you use ADK UI to maintain your Eval set data then no action is needed from you.\n",
    "2. <font size='3ptx'><b>Eval set data is developed and maintained manually and used in ADK eval CLI</b></font> A migration tool is in the works, until then the ADK eval CLI command will continue to support data in the old format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efcf8f-e096-4f4e-8490-974129251eaa",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Evaluation Criteria</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#evaluation-criteria)) <b><font size='3ptx'>The evaluation criteria define how the agent's performance is measured against the evalset.</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5319f0-0ea3-42df-b763-e9c481e7e50d",
   "metadata": {},
   "source": [
    "The following metrics are supported:\n",
    "* <b>`tool_trajectory_avg_score`</b>: This metric compares the agent's actual tool usage during the evaluation against the expected tool usage defined in the `expected_tool_use` field. Each matching tool usage step receives a score of 1, while a mismatch receives a score of 0. The final score is the average of these matches, representing the accuracy of the tool usage trajectory.\n",
    "* <b>`response_match_score`</b>: This metric compares the agent's final natural language response to the expected final response, stored in the `reference` field. We use the [**ROUGE**](https://en.wikipedia.org/wiki/ROUGE_(metric)) metric to calculate the similarity between the two responses.\n",
    "\n",
    "If no evaluation criteria are provided, the following default configuration is used:\n",
    "* <b>`tool_trajectory_avg_score`</b>: Defaults to 1.0, requiring a 100% match in the tool usage trajectory.\n",
    "* <b>`response_match_score`</b>: Defaults to 0.8, allowing for a small margin of error in the agent's natural language responses.\n",
    "\n",
    "Here is an example of a <font color='olive'>test_config.json</font> file specifying custom evaluation criteria:\n",
    "```python\n",
    "{\n",
    "  \"criteria\": {\n",
    "    \"tool_trajectory_avg_score\": 1.0,\n",
    "    \"response_match_score\": 0.8\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd817afe-c952-4eea-9c82-2580b48813ca",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How to run Evaluation with the ADK</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#how-to-run-evaluation-with-the-adk)) <b><font size='3ptx'>As a developer, you can evaluate your agents using the ADK in the following ways:</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085af24-87c0-484c-9b82-d23ae24a8371",
   "metadata": {},
   "source": [
    "1. <b><font size='3ptx'>Web-based UI (`adk web`)</font></b>: Evaluate agents interactively through a web-based interface.\n",
    "2. <b><font size='3ptx'>Programmatically (`pytest`)</font></b>: Integrate evaluation into your testing pipeline using pytest and test files.\n",
    "3. <b><font size='3ptx'>Command Line Interface (`adk eval`)</font></b>: Run evaluations on an existing evaluation set file directly from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435365bf-ef55-4247-b0f9-246a66f3a7c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>1. `adk web` - Run Evaluations via the Web UI</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#1-adk-web-run-evaluations-via-the-web-ui)) <b><font size='3ptx'>The web UI provides an interactive way to evaluate agents, generate evaluation datasets, and inspect agent behavior in detail.</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4dff75a-f397-49f4-bae2-cde307d600ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-adk[eval]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06389a-2f62-4566-a558-813cec920712",
   "metadata": {},
   "source": [
    "#### <b>Step 1: Create and Save a Test Case</b>\n",
    "1. Start the web server by running: `adk web <path_to_your_agents_folder>`.\n",
    "```shell\n",
    "$ tree -I __pycache__ evaluate_transform/\n",
    "evaluate_transform/\n",
    "├── agent.py\n",
    "└── prompt.py\n",
    "\n",
    "$ adk web\n",
    "```\n",
    "\n",
    "2. In the web interface, select an agent and interact with it to create a session:\n",
    "![select agent](images/evaluate_1.png)\n",
    "\n",
    "3. Navigate to the `Eval` tab on the right side of the interface.\n",
    "4. Create a new eval set or select an existing one.\n",
    "5. Click \"Add current session\" to save the conversation as a new evaluation case.\n",
    "![select agent](images/evaluate_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22c760-7912-4f71-aa30-81f58784f57e",
   "metadata": {},
   "source": [
    "#### <b>Step 2: View and Edit Your Test Case</b>\n",
    "Once a case is saved, you can click its ID in the list to inspect it. To make changes, click the **Edit current eval case** icon (pencil). This interactive view allows you to:\n",
    "- <b><font size='3ptx'>Modify</font></b> agent text responses to refine test scenarios.\n",
    "- <b><font size='3ptx'>Delete</font></b> individual agent messages from the conversation.\n",
    "- <b><font size='3ptx'>Delete</font></b> the entire evaluation case if it's no longer needed\n",
    "\n",
    "![flow](https://google.github.io/adk-docs/assets/adk-eval-case.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82f5d4-e982-4b4a-aa80-22e454ee7cbf",
   "metadata": {},
   "source": [
    "#### <b>Step 3: Run the Evaluation with Custom Metrics</b>\n",
    "1. Select one or more test cases from your evalset.\n",
    "2. Click **Run Evaluation**. An **EVALUATION METRIC** dialog will appear.\n",
    "3. In the dialog, use the sliders to configure the thresholds for:\n",
    "     - **Tool trajectory avg score**\n",
    "     - **Response match score**\n",
    "4. Click **Start** to run the evaluation using your custom criteria. The evaluation history will record the metrics used for each run.\n",
    "\n",
    "![flow](https://google.github.io/adk-docs/assets/adk-eval-config.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ad7fd-b025-4930-8ead-09af4fe296a2",
   "metadata": {},
   "source": [
    "#### <b>Step 4: Analyze Results</b>\n",
    "After the run completes, you can analyze the results:\n",
    "* <b><font size='3ptx'>Analyze Run Failures</font></b>: Click on any `Pass` or `Fail` result. For failures, you can hover over the `Fail` label to see a side-by-side comparison of the **Actual vs. Expected Output** and the scores that caused the failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0499919-03bc-430b-ab9e-3b8cee2d0715",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>Debugging with the Trace View</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#debugging-with-the-trace-view)) <font size='3ptx'><b>The ADK web UI includes a powerful `Trace` tab for debugging agent behavior.</b> This feature is available for any agent session, not just during evaluation.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2941e5e3-5639-451a-9942-c84e6978c2da",
   "metadata": {},
   "source": [
    "**The `Trace` tab provides a detailed and interactive way to inspect your agent's execution flow.** Traces are automatically grouped by user message, making it easy to follow the chain of events.\n",
    "\n",
    "Each trace row is interactive:\n",
    "* <b><font size='3ptx'>Hovering</font></b> over a trace row highlights the corresponding message in the chat window.\n",
    "* <b><font size='3ptx'>Clicking</font></b> on a trace row opens a detailed inspection panel with four tabs:\n",
    "    - <b>Event</b>: The raw event data.\n",
    "    - <b>Request</b>: The request sent to the model.\n",
    "    - <b>Response</b>: The response received from the model.\n",
    "    - <b>Graph</b>: A visual representation of the tool calls and agent logic flow.\n",
    "\n",
    "![flow1](https://google.github.io/adk-docs/assets/adk-trace1.gif)\n",
    "\n",
    "![flow2](https://google.github.io/adk-docs/assets/adk-trace2.gif)\n",
    "\n",
    "Blue rows in the trace view indicate that an event was generated from that interaction. Clicking on these blue rows will open the bottom event detail panel, providing deeper insights into the agent's execution flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441434ce-d020-44f2-9696-5202ca8eb0f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <b><font color='darkgreen'>2. `pytest` - Run Tests Programmatically</font></b>\n",
    "<font size='3ptx'><b>You can also use pytest to run test files as part of your integration tests.</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839949b1-16ff-46c3-a6c0-11609991460a",
   "metadata": {},
   "source": [
    "<b>Example Command</b>\n",
    "```shell\n",
    "$ pytest tests/integration/\n",
    "```\n",
    "\n",
    "<b>Example Test Code</b>\n",
    "\n",
    "Here is an example of a pytest test case that runs a single test file:\n",
    "- <b><font color='violet'>tests/integration/test_transform.py</font></b>\n",
    "```python\n",
    "from google.adk.evaluation.agent_evaluator import AgentEvaluator\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_with_single_test_file():\n",
    "  \"\"\"Test the agent's basic ability via a session file.\"\"\"\n",
    "  test_json_path = (\n",
    "      \"tests/integration/fixture/transform/evalsete613dc.evalset.json\")\n",
    "  await AgentEvaluator.evaluate(\n",
    "      agent_module=\"evaluate_transform\",\n",
    "      eval_dataset_file_path_or_dir=test_json_path,\n",
    "  )\n",
    "```\n",
    "\n",
    "This approach allows you to integrate agent evaluations into your CI/CD pipelines or larger test suites. If you want to specify the initial session state for your tests, you can do that by storing the session details in a file and passing that to `AgentEvaluator.evaluate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd82acb-0b99-481d-9432-1ab7eb1de985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.12.9, pytest-8.4.1, pluggy-1.6.0\n",
      "rootdir: /usr/local/google/home/johnkclee/Github/ml_articles/google/agent_development_kit\n",
      "plugins: langsmith-0.4.4, devtools-0.12.2, asyncio-1.1.0, anyio-4.9.0\n",
      "asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "\u001b[32m.\u001b[0m\u001b[32m                                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 17.44s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/integration/test_transform.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00dbe3-9dba-4100-b561-935e11991f11",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>3. `adk eval` - Run Evaluations via the CLI</font></b>\n",
    "([source](https://google.github.io/adk-docs/evaluate/#3-adk-eval-run-evaluations-via-the-cli)) <font size='3ptx'><b>You can also run evaluation of an eval set file through the command line interface (CLI).</b> This runs the same evaluation that runs on the UI, but it helps with automation, i.e. you can add this command as a part of your regular build generation and verification process.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0beb7-1409-4c0a-b4d6-d634261193ce",
   "metadata": {},
   "source": [
    "Here is the command:\n",
    "```shell\n",
    "adk eval \\\n",
    "    <AGENT_MODULE_FILE_PATH> \\\n",
    "    <EVAL_SET_FILE_PATH> \\\n",
    "    [--config_file_path=<PATH_TO_TEST_JSON_CONFIG_FILE>] \\\n",
    "    [--print_detailed_results]\n",
    "```\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7e5236-33ce-40e8-aa39-46bfaafcf7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using evaluation criteria: {'tool_trajectory_avg_score': 1.0, 'response_match_score': 0.8}\n",
      "Running Eval: evalsete613dc:casedc24df\n",
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n",
      "Result: ✅ Passed\n",
      "\n",
      "*********************************************************************\n",
      "Eval Run Summary\n",
      "evalsete613dc:\n",
      "  Tests passed: 1\n",
      "  Tests failed: 0\n"
     ]
    }
   ],
   "source": [
    "!adk eval \\\n",
    "    evaluate_transform/  \\\n",
    "    tests/integration/fixture/transform/evalsete613dc.evalset.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87cf00-088e-419f-ad48-9bf2725d1532",
   "metadata": {},
   "source": [
    "Here are the details for each command line argument:\n",
    "* <b>`AGENT_MODULE_FILE_PATH`</b>: The path to the `__init__.py` file that contains a module by the name \"`agent`\". \"`agent`\" module contains a `root_agent`.\n",
    "* <b>`EVAL_SET_FILE_PATH`</b>: The path to evaluations file(s). You can specify one or more eval set file paths. For each file, all evals will be run by default. If you want to run only specific evals from a eval set, first create a comma separated list of eval names and then add that as a suffix to the eval set file name, demarcated by a colon `:` .\n",
    "    - For example: `sample_eval_set_file.json:eval_1,eval_2,eval_3`\n",
    "This will only run `eval_1`, `eval_2` and `eval_3` from `sample_eval_set_file.json`\n",
    "* <b>`CONFIG_FILE_PATH`</b>: The path to the config file.\n",
    "* <b>`--print_detailed_results`</b>: Prints detailed results on the console."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
