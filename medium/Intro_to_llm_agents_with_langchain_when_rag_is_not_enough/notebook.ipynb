{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc51f46b-5ece-467e-864d-e69e331eda9d",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://towardsdatascience.com/intro-to-llm-agents-with-langchain-when-rag-is-not-enough-7d8c08145834)) <b><font size='3ptx'>This article is a written form of a tutorial I conducted two weeks ago with Neurons Lab. If you prefer a narrative walkthrough, you can find the [YouTube video here](https://www.youtube.com/watch?v=uVkS05qPhik)</font></b>\n",
    "\n",
    "As always, you can find [the code on GitHub](https://github.com/Rachnog/intro_to_llm_agents), and here are separate Colab Notebooks:\n",
    "* <b>[Planning and reasoning](#sect2)</b> ([colab](https://colab.research.google.com/drive/1SplDwEIbVfo9zNt6JOK0gJlV0wCNuz0F?usp=sharing))\n",
    "* <b>[Different types of memories](#sect3)</b> ([colab](https://colab.research.google.com/drive/13b_pD27aqcNXYI2M7fBxK1fRIO2pygZJ?usp=sharing))\n",
    "* <b>[Various types of tools](#sect4)</b> ([colab](https://colab.research.google.com/drive/1-VpwkmSvzA-zQ_iVK-xjOQf5kA-Lzwg9?usp=sharing))\n",
    "* <b>[Building complete agents](#sect5)</b> ([colab](https://colab.research.google.com/drive/1aC9AUNNYYz36atE8BUJH4fZIfknHa9Pk?usp=sharing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde36577-16d6-4487-835a-0be9f3461335",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Agenda</font></b>\n",
    "* <b><font size='3ptx'><a href='#sect1'>Introduction to the agents</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2'>Planning and reasoning</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect3'>Different types of memories</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect4'>Various types of tools</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect5'>Building complete agents</a></font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004390cb-68f8-479b-ad54-a209a64c4679",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Importing necessary packages</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "31a12e97-eb5b-4176-b0eb-4d550b7f9ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.chat_models.openai import ChatOpenAI\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eb53a-9fb0-4b30-b617-3f3fa168439f",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <b><font color='darkblue'>Introduction to the agents</font></b>\n",
    "<b><font size='3ptx'>Let’s begin the lecture by exploring various examples of LLM agents.</font></b>\n",
    "* <font size='3ptx'><b><a href='#sect1_1'>Step 1: Planning</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_2'>Step 2: Memory</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_3'>Step 3: Tools</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_4'>Step 4: All together</a></b></font>\n",
    "\n",
    "![fig1](images/fig1.PNG)\n",
    "\n",
    "While the topic is widely discussed, few are actively utilizing agents; often, what we perceive as agents are simply large language models. Let’s consider such a simple task as searching for football game results and saving them as a CSV file. We can compare several available tools:\n",
    "* **GPT-4 with search and plugins**: as you will find in the [chat history here](https://chat.openai.com/share/2ecd61a9-dbd9-4287-aa75-14618106a34c), GPT-4 failed to do the task due to code errors.\n",
    "* **AutoGPT through** https://evo.ninja/ at least could generate some kind of CSV (not ideal though).\n",
    "* **AgentGPT** through https://agentgpt.reworkd.ai/: decided to treat this task as a synthetic data generator which is not what we asked about, check the [chat history here](https://agentgpt.reworkd.ai/agent?id=clsyfh1t101t9jv08yaof890k).\n",
    "\n",
    "**Since the available tools are not great, let’s learn from the first principles of how to build agents from scratch.** I am using amazing [Lilian’s blog article](https://lilianweng.github.io/posts/2023-06-23-agent/) as a structure reference but adding more examples on my own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493c236-a141-499e-9618-00eec3fb1486",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <b><font color='darkgreen'>Step 1: Planning</font></b>\n",
    "<b><font size='3ptx'>You might have come across various techniques aimed at improving the performance of large language models, such as [offering tips](https://twitter.com/literallydenis/status/1730965217125839142) or even jokingly threatening them.</font></b>\n",
    "\n",
    "![fig2](images/fig2.PNG)\n",
    "\n",
    "**One popular technique is called “[chain of thought,](https://www.promptingguide.ai/techniques/cot)” where the model is asked to think step by step, enabling self-correction**. This approach has evolved into more advanced versions like the “[**chain of thought with self-consistency**](https://www.promptingguide.ai/techniques/consistency)” and the generalized “[**tree of thoughts,**](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac)” (ToT) <b>where multiple thoughts are created, re-evaluated, and consolidated to provide an output</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f933427-08b7-43f2-830d-0c33f3d1c290",
   "metadata": {},
   "source": [
    "**In this tutorial, I am using heavily [Langsmith](https://www.langchain.com/langsmith), a platform for productionizing LLM applications**. For example, while building the tree of thoughts prompts, I save my sub-prompts in the prompts repository and load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be4ca9b-93ea-4859-8f5e-56080223313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18dcac-7aa5-47fe-a3aa-855b3c8ee35f",
   "metadata": {},
   "source": [
    "You can see in [this notebook](https://github.com/Rachnog/intro_to_llm_agents/blob/main/1_planning.ipynb) the result of such reasoning, <b>the point I want to make here is the right process for defining your reasoning steps and versioning them in such an LLMOps system like [Langsmith](https://www.langchain.com/langsmith)</b>.\n",
    "\n",
    "Also, you can see other examples of popular reasoning techniques in public repositories like ReAct or Self-ask with search:\n",
    "\n",
    "```python\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "```\n",
    "\n",
    "Other notable approaches are:\n",
    "* **Reflexion** ([Shinn & Labash 2023](https://arxiv.org/abs/2303.11366)) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "* **Chain of Hindsight** ([CoH; Liu et al. 2023](https://arxiv.org/abs/2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deebe339-0d67-47d4-b259-d10a49eeb129",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <b><font color='darkgreen'>Step 2: Memory</font></b> ([back](#sect1))\n",
    "![fig3](images/fig3.PNG)\n",
    "\n",
    "* **Sensory Memory**: This component of memory captures immediate sensory inputs, like what we see, hear or feel. In the context of prompt engineering and AI models, a prompt serves as a transient input, similar to a momentary touch or sensation. It’s the initial stimulus that triggers the model’s processing.\n",
    "* **Short-Term Memory**: Short-term memory holds information temporarily, typically related to the ongoing task or conversation. In prompt engineering, this equates to retaining the recent chat history. This memory enables the agent to maintain context and coherence throughout the interaction, ensuring that responses align with the current dialogue. In code, you typically add it as conversation history:\n",
    "```python\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "tools = [retriever_tool]\n",
    "agent = create_openai_functions_agent(\n",
    "    llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804ad80-e19c-455a-967d-953f9543e5a9",
   "metadata": {},
   "source": [
    "* **Long-Term Memory**: Long-term memory stores both factual knowledge and procedural instructions. In AI models, this is represented by the data used for training and fine-tuning. Additionally, long-term memory supports the operation of RAG frameworks, allowing agents to access and integrate learned information into their responses. It’s like the comprehensive knowledge repository that agents draw upon to generate informed and relevant outputs. In code, you typically add it as a vectorized database:\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "loader = WebBaseLoader(\"https://neurons-lab.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb52f31-6757-4db7-86fa-316bc0909ec6",
   "metadata": {},
   "source": [
    "<a id='sect1_3'></a>\n",
    "### <b><font color='darkgreen'>Step 3: Tools</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>ChatGPT [Plugins](https://openai.com/blog/chatgpt-plugins) and OpenAI API [function calling](https://platform.openai.com/docs/guides/gpt/function-calling) are good examples of LLMs augmented with tool use capability working in practice.\n",
    "\n",
    "![fig4](images/fig4.PNG)\n",
    "\n",
    "* **Built-in Langchain tools**: Langchain has a [plenty of built-in tools](https://python.langchain.com/docs/integrations/tools/) ranging from internet search and Arxiv toolkit to Zapier and Yahoo Finance. For this simple tutorial, we will experiment with the internet search provided by [**Tavily**](https://tavily.com/):\n",
    "```python\n",
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "agent_chain = initialize_agent(\n",
    "    [retriever_tool, tavily_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe77876-f59f-405a-972e-ae8e06e4007e",
   "metadata": {},
   "source": [
    "* **Custom tools**: it’s also very easy to define your own tools. Let’s dissect the simple example of a tool that calculates the length of the string. You need to use the <b><font color='violet'>@tool</font> decorator to make Langchain know about it. Then, don’t forget about the type of input and the output. But the most important part will be the function comment between \"\"\" \"\"\" — this is how your agent will know what this tool does and will compare this description to descriptions of the other tools:\n",
    "```python\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def calculate_length_tool(a: str) -> int:\n",
    "    \"\"\"The function calculates the length of the input string.\"\"\"\n",
    "    return len(a)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "agent_chain = initialize_agent(\n",
    "    [retriever_tool, tavily_tool, calculate_length_tool],\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73d3c0-0f2d-43a5-97c7-c4dc12605b32",
   "metadata": {},
   "source": [
    "You can find examples of how it works [in this script](https://github.com/Rachnog/intro_to_llm_agents/blob/main/3_tools.ipynb), **but you also can see an error** — it doesn't pull the correct description of the Neurons Lab company and despite calling the right custom function of length calculation, the final result is wrong. Let’s try to fix it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301cf28-4049-4abb-9f60-5543c83341fe",
   "metadata": {},
   "source": [
    "<a id='sect1_4'></a>\n",
    "### <b><font color='darkgreen'>Step 4: All together</font></b> ([back](#sect1))\n",
    "<b><font size='3ptx'>I am providing a clean version of combining all the pieces of architecture together in this script.</font></b>\n",
    "\n",
    "Notice, how we can easily decompose and define separately:\n",
    "* All kinds of **tools** (<font color='brown'>search, custom tools, etc</font>)\n",
    "* All kinds of **memories** (<font color='brown'>**sensory** as a prompt, **short-term** as runnable message history, and as a sketchpad within the prompt, and **long-term** as a retrieval from the vector database</font>)\n",
    "* Any kind of **planning strategy** (<font color='brown'>as a part of a prompt pulled from the LLMOps system</font>)\n",
    "\n",
    "The final definition of the agent will look as simple as this:\n",
    "```python\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9db24c-50d5-4ce4-a5ec-63dcb6e19c01",
   "metadata": {},
   "source": [
    "As you can see in the [outputs of the script](https://github.com/Rachnog/intro_to_llm_agents/blob/main/4_agents.ipynb) (<font color='brown'>or you can run it yourself</font>), it solves the issue in the previous part related to tools. What changed? We defined a **complete architecture**, where short-term memory plays a crucial role. Our agent obtained **message history and a sketchpad as a part of the reasoning structure** which allowed it to pull the correct website description and calculate its length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e3720-cd3b-4bd9-8ba5-2512a7e7d35d",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <b><font color='darkblue'>Planning and reasoning</font></b> ([back](#sect0))\n",
    "([source colab](https://colab.research.google.com/drive/1SplDwEIbVfo9zNt6JOK0gJlV0wCNuz0F?usp=sharing)) <b><font size='3ptx'>Here we are going to run through several common planning and reasoning framework to see how they work.</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b49c7cd-7458-493c-9066-3f2f8bb2ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a96d64-ce9d-4f7c-b1b3-2793f25e36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "cot_step1 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step1\")\n",
    "cot_step2 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step2\")\n",
    "cot_step3 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step3\")\n",
    "cot_step4 = hub.pull(\"rachnogstyle/nlw_jan24_cot_step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "008eace2-0f90-4772-bd70-d258eadced0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step1,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step2,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step3,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=model),\n",
    "    prompt=cot_step4,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c51dd412-fca0-4974-841a-c11d57ad30d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output = overall_chain(\n",
    "    {\n",
    "        \"input\": \"How to become a successful software engineer?\",\n",
    "        \"perfect_factors\": \"It is difficult for a company to hire an engineer without experience.\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02bbfd44-2aac-4f13-95bc-43aa093c7712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'perfect_factors', 'ranked_solutions'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b187504-04a9-4d98-b2d5-4033ac43d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking: \n",
      "\n",
      "1. Participating in a semester-long co-op program with a software development firm\n",
      "2. Securing a summer internship at a tech company\n",
      "3. Taking on a part-time internship during the school year\n",
      "\n",
      "Justification:\n",
      "\n",
      "1. Participating in a semester-long co-op program with a software development firm ranks highest because it offers a longer and more immersive experience in a professional setting, allowing for deeper learning and skill development. It also provides the opportunity to build strong relationships with industry professionals and potentially secure a job offer after graduation.\n",
      "\n",
      "2. Securing a summer internship at a tech company is ranked second as it still provides valuable hands-on experience in a shorter time frame. It allows students to gain exposure to the industry and build their resume, potentially leading to future internship or job opportunities.\n",
      "\n",
      "3. Taking on a part-time internship during the school year is ranked last as it may be more challenging to balance internship responsibilities with academic commitments. However, it still offers valuable experience and networking opportunities for students who are able to manage their time effectively.\n",
      "\n",
      "Final thoughts:\n",
      "\n",
      "It is important for students to carefully consider their goals, availability, and preferences when pursuing internships or co-op opportunities during college. Researching and networking with industry professionals can help students identify the best opportunities for their career development. Additionally, students should be prepared to overcome obstacles and adapt to unexpected outcomes in order to make the most of their internship experiences.\n"
     ]
    }
   ],
   "source": [
    "print(output['ranked_solutions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8874-9689-4ef0-9950-d8c93a538d3c",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>ReAct prompt overview</font></b>\n",
    "Let's review [**ReAct**](https://www.google.com/url?q=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fagents%2Fagent_types%2Freact) prompt as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96a62f8c-ef26-496f-b650-ca79609322b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72cc61df-5e7d-4de2-a20f-6657e4e918e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aad6d8-6c9a-4aca-bc23-96c53de1be98",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Self-ask with search</font></b>\n",
    "Let's review [self-ask with search](https://www.google.com/url?q=https%3A%2F%2Fpython.langchain.com%2Fdocs%2Fmodules%2Fagents%2Fagent_types%2Fself_ask_with_search) as it's defined in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "684da760-701e-4023-9642-52a503945a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95a7b587-03ac-40d0-a7a6-1040240ea00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "Question: {input}\n",
      "Are followup questions needed here:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc73af-2717-4136-8755-430f44df5ebb",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Approaches to read next</font></b>\n",
    "* **Reflexion** ([Shinn & Labash 2023](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2303.11366)) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills.\n",
    "* **Chain of Hindsight** ([CoH; Liu et al. 2023](https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2302.02676)) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5077d-e978-4451-abeb-32486981ea9d",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <b><font color='darkblue'>Different types of memories</font></b> ([back](#sect0))\n",
    "![fig3](images/fig3.PNG)\n",
    "\n",
    "- **Sensory Memory:** This component of memory captures immediate sensory inputs, like what we see, hear, or feel. In the context of prompt engineering and AI models, a prompt serves as a transient input, similar to a momentary touch or sensation. It's the initial stimulus that triggers the model's processing.\n",
    "\n",
    "- **Short-Term Memory:** Short-term memory holds information temporarily, typically related to the ongoing task or conversation. In prompt engineering, this equates to retaining the recent chat history. This memory enables the agent to maintain context and coherence throughout the interaction, ensuring that responses align with the current dialogue.\n",
    "\n",
    "- **Long-Term Memory:** Long-term memory stores both factual knowledge and procedural instructions. In AI models, this is represented by the data used for training and fine-tuning. Additionally, long-term memory supports the operation of RAG frameworks, allowing agents to access and integrate learned information into their responses. It's like the comprehensive knowledge repository that agents draw upon to generate informed and relevant outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e05ac8-5b62-440d-9cbb-00c2c6ac3784",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Adding long-term memory</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8ca0f2d-98a3-4053-8e08-4a58af9484cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cca7ce9-b8ac-4d1b-84d9-9ab21fc33bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://neurons-lab.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cec55be2-0a6e-4ddd-9aea-e7431b2a24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3defebca-2cfe-4f86-bcfd-2b0d7e67dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retriever.get_relevant_documents(\"What are the projects in healthcare?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ba816bb-fc5b-41bb-ad0d-32fcb0ad9e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creative Practice Solutions : Developing an AI-Driven Medical Transcription & Billing System                                                \\n\\n\\nHealthTech\\n\\n\\n\\nExplore story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMagellan X: Creating a predictive maintenance solution in shipping with Magellan X                                                \\n\\n\\nCleantech\\n\\n\\n\\nExplore story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\niPlena: Transforming user experience in physiotherapy                                                \\n\\n\\nHealthTech\\n\\n\\n\\nExplore story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHealthTech client: Building a Remote Patient Monitoring system for health providers                                                \\n\\n\\nHealthTech\\n\\n\\n\\nExplore story\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCleanTech client: Developing Machine learning models to predict PV output and consumption                                                \\n\\n\\nCleantech\\n\\n\\n\\nExplore story'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a5d88-e0e6-4a26-957d-948cbbbc4ea9",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Adding short-term memory</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff7467ab-c0d9-46a9-9e1d-fbd21b10b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cebc4e0-af11-460a-90d4-9d34b72bce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6d0b3e34-2575-4c50-8c76-e193cc6fcaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85fa50e3-e48a-4e38-9e3b-e73ed4763f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91b29fc1-158e-40d1-a026-c923ae07a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "tools = [retriever_tool]\n",
    "agent = create_openai_functions_agent(\n",
    "    llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "message_history = ChatMessageHistory()\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a7bcc00-650c-43d0-9e45-ad32fc20c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 2bcd31df-b73c-4aa1-9cf2-30a9807e0bb5 not found for run e5edba71-d536-4229-af62-cb114d1e7eeb. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello John Lee! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Hi! I'm John Lee\",\n",
       " 'chat_history': [],\n",
       " 'output': 'Hello John Lee! How can I assist you today?'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Hi! I'm John Lee\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18a7a37e-db80-4fd8-81c6-86cae5c07e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e37d290f-879f-4537-b397-a33a40328939 not found for run 521670d7-f86f-4d83-ac7b-875f1396fbbe. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYes, you mentioned that your name is John Lee. How can I help you, John?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\"input\": \"Do you know what's my name?\"},\n",
    "    # This is needed because in most real world scenarios, a session id is needed\n",
    "    # It isn't really used here because we are using a simple in memory ChatMessageHistory\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "469eb72e-46f4-437d-8b17-20511e053f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'chat_history', 'output'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d95ee5b-83cb-4263-bbf1-fe1ec7612a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hi! I'm John Lee\"),\n",
       " AIMessage(content='Hello John Lee! How can I assist you today?')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa7d4b5f-4333-4b4b-b45c-89e6226bd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you mentioned that your name is John Lee. How can I help you, John?\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b8945-3401-44b0-9c1f-c3b9a6551759",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <b><font color='darkblue'>Various types of tools</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>First such tool-augmented architecture started in 2022 with A21Labs</font></b>\n",
    "* <b><a href='#sect4_1'>Memory as a retrieval tool</a></b>\n",
    "* <b><a href='#sect4_2'>Langchain tools</a></b>\n",
    "* <b><a href='#sect4_3'>Custom tools and reasoning</a></b>\n",
    "\n",
    "![fig4](images/fig4.PNG)\n",
    "\n",
    "**These modules can be neural** (e.g. deep learning models) **or symbolic** (e.g. math calculator, currency converter, weather API). ChatGPT [Plugins](https://www.google.com/url?q=https%3A%2F%2Fopenai.com%2Fblog%2Fchatgpt-plugins) and OpenAI [API function](https://www.google.com/url?q=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fgpt%2Ffunction-calling) calling are good examples of LLMs augmented with tool use capability working in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4060770d-d9a0-4eb6-af65-9301b6173073",
   "metadata": {},
   "source": [
    "<a id='sect4_1'></a>\n",
    "### <b><font color='darkgreen'>Memory as a retrieval tool</font></b>\n",
    "* [langchain.tools.retriever.create_retriever_tool](https://api.python.langchain.com/en/latest/tools/langchain.tools.retriever.create_retriever_tool.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23226740-d96f-4dac-9306-cd0f12671f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "61be516e-99d4-4079-8563-967490c1cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://neurons-lab.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d979dc52-cb0f-479f-94e7-fc3e7b7f5aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7150956e-1f58-4e48-bb56-e52be6800ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_description = \"Search for information about Neurons Lab. For any questions about Neurons Lab, you must use this tool!\"\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,  # The retriever to use for the retrieval\n",
    "    \"neurons_lab_search\",  # The name for the tool. This will be passed to the language model, so should be unique and somewhat descriptive.\n",
    "    tools_description,  # The description for the tool. This will be passed to the language model, so should be descriptive.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb113a1e-809f-4679-aa49-ca323ae24df4",
   "metadata": {},
   "source": [
    "<a id='sect4_2'></a>\n",
    "### <b><font color='darkgreen'>Langchain tools</font></b> ([back](#sect4))\n",
    "More tools here https://python.langchain.com/docs/integrations/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3afde21-6c74-4649-86df-7f1d831921c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1094c4ba-155b-4c71-bbc2-16654bb7a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73a404ab-07a5-4551-b2c6-e84aa871717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "agent_chain = initialize_agent(\n",
    "    [retriever_tool, tavily_tool],\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "81aa04c3-5f82-4d38-b140-93d6ce31f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/johnkclee/Github/ml_articles/env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use the Neurons Lab search tool to find information about the services they offer.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"neurons_lab_search\",\n",
      "  \"action_input\": \"services offered\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mGrow your business & attract clients \n",
      "Do you also seek development opportunities or funding? Thanks to our advanced-tier AWS partnership and access to a vast VC network, we can aid your company in obtaining grants and other assistance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Talk to our expert team from Europe\n",
      "Get in touch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Talk to our expert team from USA\n",
      "Get in touch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Articles = mass of insights /\n",
      "volume of experience\n",
      "\n",
      "ALL POSTINGS\n",
      "\n",
      "                                    1 — 6                                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AI for portfolio management: from Markowitz to Reinforcement Learning\n",
      "\n",
      "                                                    Jul 16, 2023|11 min read                                                \n",
      "\n",
      "\n",
      "FinTech\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accelerate Remote Patient Monitoring and Care Management with IoT Medical Devices on the AWS Cloud\n",
      "\n",
      "Jul 26, 2023|6 min read                                                \n",
      "\n",
      "\n",
      "Customer Experience\n",
      "\n",
      "\n",
      "HealthTech\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Predictive maintenance for smart warranty service of PV Inverters\n",
      "\n",
      "                                                    Jul 26, 2023|6 min read                                                \n",
      "\n",
      "\n",
      "Predictive maintenance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We collaborate with companies across a broad range of industries to create innovative solutions including HealthTech, CleanTech and Retail. Care to join?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yes, I am interested\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get in touch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Services\n",
      "\n",
      "\n",
      "AI feasibility analysis\n",
      "\n",
      "\n",
      "AI solution engineering\n",
      "\n",
      "\n",
      "AI operations management\n",
      "\n",
      "\n",
      "Generative AI Solutions\n",
      "\n",
      "\n",
      "GenAI Workshop + PoC\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Co-labs\n",
      "\n",
      "\n",
      "Company\n",
      "\n",
      "\n",
      "Articles\n",
      "\n",
      "\n",
      "Talents\n",
      "\n",
      "\n",
      "Get in touch\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "*Traditionally, a recipe for any secret sauce is never written down. It’s only shared face to face with those closest to us.\n",
      "\n",
      "\n",
      "\n",
      "Privacy Notice\n",
      " Cookie Notice\n",
      "\n",
      "Creative Practice Solutions : Developing an AI-Driven Medical Transcription & Billing System                                                \n",
      "\n",
      "\n",
      "HealthTech\n",
      "\n",
      "\n",
      "\n",
      "Explore story\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Magellan X: Creating a predictive maintenance solution in shipping with Magellan X                                                \n",
      "\n",
      "\n",
      "Cleantech\n",
      "\n",
      "\n",
      "\n",
      "Explore story\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iPlena: Transforming user experience in physiotherapy                                                \n",
      "\n",
      "\n",
      "HealthTech\n",
      "\n",
      "\n",
      "\n",
      "Explore story\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HealthTech client: Building a Remote Patient Monitoring system for health providers                                                \n",
      "\n",
      "\n",
      "HealthTech\n",
      "\n",
      "\n",
      "\n",
      "Explore story\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CleanTech client: Developing Machine learning models to predict PV output and consumption                                                \n",
      "\n",
      "\n",
      "Cleantech\n",
      "\n",
      "\n",
      "\n",
      "Explore story\n",
      "\n",
      "Release & scale faster\n",
      "We make implementation ultra-fast by deploying and customizing pre-developed solutions in NLP, computer vision, time series, and Recommender systems.\n",
      "We also share our proprietary handbooks of best practices to help your team hit the ground running. Collaboration is the only way to effectively implement innovation.\n",
      "\n",
      "\n",
      "Work with experts\n",
      "Our expertise lies at the intersection of AI, advanced science, and business. Using a Talent Pool of over 500 highly skilled individuals, we quickly assemble teams of PhD-level applied scientists, recognized DS/ML/AI Engineers, and MLOps who specialize in developing solutions for your industry.\n",
      "\n",
      "\n",
      "Grow your business & attract clients \n",
      "Do you also seek development opportunities or funding? Thanks to our advanced-tier AWS partnership and access to a vast VC network, we can aid your company in obtaining grants and other assistance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe Neurons Lab offers a range of services including AI feasibility analysis, AI solution engineering, AI operations management, Generative AI Solutions, and GenAI Workshop + PoC. They also have Co-labs for collaboration. Additionally, they work with companies in various industries such as HealthTech, CleanTech, and Retail to create innovative solutions. If you are interested in their services, you can get in touch with their expert team from Europe or the USA. They can also assist in seeking development opportunities or funding through their advanced-tier AWS partnership and access to a vast VC network.\n",
      "\n",
      "Thought: I have gathered information about the services offered by Neurons Lab.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The Neurons Lab offers services such as AI feasibility analysis, AI solution engineering, AI operations management, Generative AI Solutions, and GenAI Workshop + PoC. They also collaborate with companies in various industries and can assist in seeking development opportunities or funding through their partnerships and network.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run the agent\n",
    "output = agent_chain.run(\n",
    "    \"What are the Neurons Lab services\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10f2f57b-c3c9-46a2-b2e2-5e02f2c62b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Neurons Lab offers services such as AI feasibility analysis, AI solution engineering, AI operations management, Generative AI Solutions, and GenAI Workshop + PoC. They also collaborate with companies in various industries and can assist in seeking development opportunities or funding through their partnerships and network.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f0d07-69a0-41c1-acbe-c60feed4b293",
   "metadata": {},
   "source": [
    "<a id='sect4_3'></a>\n",
    "### <b><font color='darkgreen'>Custom tools and reasoning</font></b> ([back](#sect4))\n",
    "* Here is a full guide: https://python.langchain.com/docs/modules/agents/tools/custom_tools\n",
    "* Reasoning: https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc0b4eb1-ea5d-44b0-85b0-29cc3e371bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ae64c84-3c1f-44c8-a53c-c38671a229a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate_magic_num(a: int) -> float:\n",
    "    \"\"\"The function calculates the magic number of input integer `a`.\"\"\"\n",
    "    return int(a) * 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8be6cce7-246f-4afc-9da2-d402fb2b69d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('calculate_magic_num(a: int) -> float - The function calculates the magic number of input integer `a`.',\n",
       " {'a': {'title': 'A', 'type': 'integer'}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_magic_num.description, calculate_magic_num.args,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24239c5e-e477-4b3c-b738-3ad8c86310cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "agent_chain = initialize_agent(\n",
    "    [retriever_tool, tavily_tool, calculate_magic_num],\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4d2df32-a822-4ddb-9729-5970ce6ddb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to use the calculate_magic_num function to find the magic number for the input number 12.\n",
      "Action: calculate_magic_num\n",
      "Action Input: 12\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m25\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 25\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run the agent\n",
    "output = agent_chain.run(\n",
    "    \"Give me the magic number calculated by number 12.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d64447df-20d3-48c1-b423-342b17e1be0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9481fc24-3650-4d08-bfcc-17782e3eba5d",
   "metadata": {},
   "source": [
    "<a id='sect5'></a>\n",
    "## <b><font color='darkblue'>Building complete agents</font></b> ([back](#sect0))\n",
    "<b><font size='3ptx'>Now it is time to put all things together as an agent.</font></b>\n",
    "* <font size='3ptx'><b>[Tools](#sect5_1)</b></font>\n",
    "* <font size='3ptx'><b>[Memory](#sect5_2)</b></font>\n",
    "* <font size='3ptx'><b>[Agent](#sect5_3)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b4ce9-91b2-4770-bc1a-bdf4d528d63d",
   "metadata": {},
   "source": [
    "<a id='sect5_1'></a>\n",
    "### <b><font color='darkgreen'>Tools</font></b> ([back](#sect5))\n",
    "Here we define some tools to support agent for our customized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45be4f59-0bc5-45cb-851a-9bf0731e5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "51b6f97b-0933-4422-b052-1ac13d4ca1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in searching tool\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eff1f49a-2f50-46ed-ac84-c0c2bccfe390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized tools\n",
    "@tool\n",
    "def calculate_magic_number(a: str) -> int:\n",
    "    \"\"\"The function calculates the magic number of input string as integer.\"\"\"\n",
    "    return int(a) * 2 + 3\n",
    "\n",
    "@tool\n",
    "def calculate_uppercase_tool(a: str) -> int:\n",
    "    \"\"\"The function calculates the number of uppercase characters in the input string.\"\"\"\n",
    "    return sum(1 for c in a if c.isupper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d1105-1b1c-4411-ac16-dbccfc678151",
   "metadata": {},
   "source": [
    "<a id='sect5_2'></a>\n",
    "### <b><font color='darkgreen'>Memory</font></b> ([back](#sect5))\n",
    "Let's enable our agent to be capable of both short-term and long-term meory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb244d0e-84ed-4f55-b97e-20b58b10b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1854b43-2fcd-4d61-b797-44132ef4257a",
   "metadata": {},
   "source": [
    "#### <b>Long-term memory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ef786d0-92eb-4fc0-bb11-3aa195011efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://neurons-lab.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b93758fe-943c-4d9e-b32d-1fc608343eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"neurons_lab_search\",\n",
    "    \"Search for information about Neurons Lab. For any questions about Neurons Lab, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5d686-a162-46c3-aacc-d2624e3f0739",
   "metadata": {},
   "source": [
    "#### <b>Short-term memory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8fb032bb-51c3-468d-ba83-aa3878597f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ee516b76-3599-40cd-8928-0b9f60d7926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6ad4e-1322-4f19-baed-a97fefc58326",
   "metadata": {},
   "source": [
    "#### <b>Sensory memory</b>\n",
    "https://smith.langchain.com/hub/hwchase17/openai-functions-agent\n",
    "\n",
    "In the context of humans, sensory memory is a very brief, almost instantaneous, buffer for sensory information. It acts like a fleeting snapshot of the world experienced through our senses (sight, sound, touch, etc.). It holds this information just long enough for the brain to decide what to pay attention to and process further.\n",
    "\n",
    "When it comes to large language models (LLMs) like me, the concept of sensory memory isn't directly applicable in the same way it is to humans. This is because we lack physical senses and don't experience the world through sight, sound, or touch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "517108b8-68ed-49dc-98af-7a742476ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "17f479a2-9b95-41ec-9e11-6d6f5d0caad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "66868512-b2b9-408f-9aa0-01d45f83df83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653aafa3-d19b-4ef3-bc14-d54764e67a1d",
   "metadata": {},
   "source": [
    "<a id='sect5_3'></a>\n",
    "### <b><font color='darkgreen'>Agent</font></b> ([back](#sect5))\n",
    "It's time to glue tools, memory along with an angent for our usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c69a3-7c04-4ad2-a9a7-4a1adc693a11",
   "metadata": {},
   "source": [
    "#### <b>All tools together</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "884fdd82-c5ac-4d77-a37d-115aefa8775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tool (neurons_lab_search): Search for information about Neurons Lab. For any questions about Neurons Lab, you must use this tool!\n",
      "- Tool (tavily_search_results_json): A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "- Tool (calculate_magic_number): calculate_magic_number(a: str) -> int - The function calculates the magic number of input string as integer.\n",
      "- Tool (calculate_uppercase_tool): calculate_uppercase_tool(a: str) -> int - The function calculates the number of uppercase characters in the input string.\n"
     ]
    }
   ],
   "source": [
    "tools = [retriever_tool, tavily_tool, calculate_magic_number, calculate_uppercase_tool]\n",
    "for t in tools:\n",
    "    print(f'- Tool ({t.name}): {t.description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb3544-eb08-4760-a95a-98d82c0e07b2",
   "metadata": {},
   "source": [
    "#### <b>Defining an agent with tools and memory</b>\n",
    "* [**API**:langchain.agents.openai_functions_agent.base.create_openai_functions_agent](https://api.python.langchain.com/en/latest/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html)\n",
    "* [**class**:langchain.agents.agent.AgentExecutor](https://api.python.langchain.com/en/latest/agents/langchain.agents.agent.AgentExecutor.html)\n",
    "* [**class**:langchain_core.runnables.history.RunnableWithMessageHistory](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cf7e035f-dc66-4449-ad42-04d15ff44406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "070cce54-fb82-43e5-b311-2981f5cb53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_with_chat_history = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    lambda session_id: message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99b887-4019-4179-a350-1ac8eba45d81",
   "metadata": {},
   "source": [
    "#### <b>Running an agent</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f8e7cf4-9396-4750-beae-53ca69a33622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run fe2f1d7d-e306-4f03-8134-85ab5392ea05 not found for run 37440ff8-b52b-48de-b096-2aa0e2a80b1b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello John Lee! It's nice to meet you. How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Hello, my name is John Lee!\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "04ee2ee8-8e42-46d5-8a2a-33ba192b96fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'chat_history', 'output'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6c513bb9-cd33-45f7-b3f3-0d0887cd0e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John Lee! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "72c2aec9-423b-46ea-a3c4-dc6ff9f838a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 8910e34f-67f8-4988-969e-5cb16689c5b3 not found for run 39ad7d94-fe1b-4e86-9b3f-21c9e48a90fb. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `calculate_uppercase_tool` with `{'a': 'John Lee'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2\u001b[0m\u001b[32;1m\u001b[1;3mThere are 2 uppercase characters in your name \"John Lee\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Please find the upper case characters in my name.\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "296ee4ba-778a-459a-aaec-36ff6827d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 uppercase characters in your name \"John Lee\".\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0a570cef-a471-42b3-a3cb-986762fd2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 3de8e223-80b3-4e44-bec6-281013d0cc93 not found for run 0e763e97-ab5c-4f53-b109-3ea0780f46aa. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `calculate_uppercase_tool` with `{'a': 'John Lee'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `calculate_magic_number` with `{'a': '2'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m7\u001b[0m\u001b[32;1m\u001b[1;3mThe magic number calculated using the number of uppercase characters in your name \"John Lee\" is 7.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = agent_with_chat_history.invoke(\n",
    "    {\n",
    "        \"input\": \"Use the number of upper case character of my name to calculate the magic number.\"\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"<foo>\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7ef0963-24e7-4416-8bbd-bb4b324ab406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magic number calculated using the number of uppercase characters in your name \"John Lee\" is 7.\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bc6cd-65a2-4ad4-9eff-fb7db50d97cf",
   "metadata": {},
   "source": [
    "#### <b>Structuring outputs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d8541c2c-913b-49ea-b070-f2e16439c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "845c452c-5c66-41a8-ad6c-646a0bfe56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class DesiredStructure(BaseModel):\n",
    "    question: str = Field(description=\"the question asked\")\n",
    "    numerical_answer: int = Field(description=\"the number extracted from the answer, text excluded\")\n",
    "    text_answer: str = Field(description=\"the text part of the answer, numbers excluded\")\n",
    "parser = PydanticOutputParser(pydantic_object=DesiredStructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0d072030-c8d2-4b78-b8d1-bdc100c77c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "prompt_and_model = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "90c527d8-bd3e-4838-afca-3f1597f902a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = prompt_and_model.invoke({\n",
    "    \"query\": \"Calculate the number of character in word 'Google' and double the number.\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7133a96c-e9d2-47e9-b124-ae7609cdefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"question\": \"Calculate the number of character in word 'Google' and double the number.\",\n",
      "  \"numerical_answer\": 12,\n",
      "  \"text_answer\": \"The word 'Google' has 6 characters. Doubling it gives 12.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63052b7a-0fbc-4e46-85f0-0b401d4d73fa",
   "metadata": {},
   "source": [
    "#### <b>others</b>\n",
    "Next stes: evaluation and benchmarks https://python.langchain.com/docs/langsmith/walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a177e-c16c-400c-8397-8012962f71ad",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Medium - Implementing the Tree of Thoughts in LangChain’s Chain](https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac)\n",
    "* [The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use **RunnableSequence**](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator/issues/998)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
