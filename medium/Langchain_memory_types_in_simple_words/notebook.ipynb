{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0ae4fd-f7e3-4fe0-b6fb-94277e32cb75",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([article source](https://medium.com/@rahulpant.me/langchain-memory-types-in-simple-words-9fc142003567)) <b><font size='3ptx'>Langchain is becoming the secret sauce which helps in LLM’s easier path to production. In this article we delve into the different types of memory / remembering power the LLMs can have by using langchain.</font></b>\n",
    "![concept](images/1.PNG)\n",
    "\n",
    "<b>We also look at a sample code and output to explain these memory type.</b>\n",
    "\n",
    "We have been dealing with chatbots since many years, and from the earlier days of fixed question and robotic answers the chatbots are consistently becoming more ‘human-like’. LangChain’s suite of memory types offers a fascinating glimpse into the future of these interactions.\n",
    "\n",
    "From the short-term recall of [**ConversationBufferMemory**](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html), **which ensures a chatbot remembers your favourite pizza toppings, to the complex web of relationships maintained by Conversation Knowledge Graph Memory**, Lanchchain is revolutionizing the way we think about and interact with AI.\n",
    "\n",
    "We will look into the following memory types in this article:\n",
    "* Conversation Buffer Memory\n",
    "* Conversation Buffer Window Memory\n",
    "* Conversation Summary Memory\n",
    "* Conversation Summary Buffer Memory\n",
    "* Conversation Token Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c465a5-e320-40db-ac60-6950d435acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q --upgrade google-generativeai langchain-google-genai chromadb pypdf ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67fc6202-9ec8-4dcc-b2bb-cdb2713a14fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.memory.buffer import ConversationBufferMemory\n",
    "from langchain.memory import (\n",
    "    ConversationBufferWindowMemory, ConversationSummaryMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory)\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# loads the .env file\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed3b11-4bc3-4298-9edf-95b3a17f89ab",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>ConversationBufferMemory</font>: Buffer for storing conversation memory</b>\n",
    "<b><font size='3ptx'>As the name suggests, this keeps in memory the conversation history to help contextualize the answer to the next user question.</font></b>\n",
    "\n",
    "While this sounds very useful, one <b><font color='darkred'>drawback is that it keeps all of history</font></b> (<font color='brown'>upto the max limit of specific LLM</font>) <b><font color='darkred'>and for every questions passes the whole previous discussion</font></b> (<font color='brown'>as tokens</font>) <b><font color='darkred'>to LLM API</font></b>. This can have significant cost impact as API costs are based on number of tokens processed and also the latency impact as conversation grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f3a260-506d-475b-8712-d8cc11466b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0,convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edee34b8-12f4-4245-8890-cb0a41a4f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    memory=memory, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb62066-ffc1-4454-8aef-651803684ed5",
   "metadata": {},
   "source": [
    "Let's ask our first question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1b4a28-98c2-476c-a77a-e37e0ecafb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is John.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello John, my name is Gemini. It's nice to meet you.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is John.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e0829-85ba-4b76-b3a0-eb2683c5d611",
   "metadata": {},
   "source": [
    "For our second question, the previous chat history will be appended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c101e013-5424-4d92-af49-41a95ad14835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is John.\n",
      "AI: Hello John, my name is Gemini. It's nice to meet you.\n",
      "Human: I am a software engineer. Tell me what's amazing about being a software engineer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Being a software engineer is an amazing career for many reasons. First, it is a highly in-demand field, with a growing number of jobs available each year. This means that software engineers are likely to have job security and the opportunity to work on a variety of projects. Second, software engineering is a creative field that allows engineers to use their skills to solve problems and create new products. Third, software engineering is a well-paying field, with salaries that are typically higher than the average for other professions. Finally, software engineering is a rewarding field that allows engineers to make a real difference in the world.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I am a software engineer. Tell me what's amazing about being a software engineer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70da4b-092d-4416-a969-60eae90af717",
   "metadata": {},
   "source": [
    "Then let's ask the information we disclose in the first quesiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4af3acf-9f39-4eb1-b664-63a5e247d5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is John.\n",
      "AI: Hello John, my name is Gemini. It's nice to meet you.\n",
      "Human: I am a software engineer. Tell me what's amazing about being a software engineer.\n",
      "AI: Being a software engineer is an amazing career for many reasons. First, it is a highly in-demand field, with a growing number of jobs available each year. This means that software engineers are likely to have job security and the opportunity to work on a variety of projects. Second, software engineering is a creative field that allows engineers to use their skills to solve problems and create new products. Third, software engineering is a well-paying field, with salaries that are typically higher than the average for other professions. Finally, software engineering is a rewarding field that allows engineers to make a real difference in the world.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'John'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb18ef-968d-4ff0-b58b-807c86851e29",
   "metadata": {},
   "source": [
    "The LLM could correctly answer the question and know my name is `John`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb2f7b-e702-4f33-a4c7-47d3900cf937",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>ConversationBufferWindowMemory</font>: Buffer for storing conversation memory inside a limited size window.</b>\n",
    "<b><font size='3ptx'>In our conversations we usually do not need all last 5–10 conversation history but definitely the last few. </font></b>\n",
    "\n",
    "<b>This type of memory helps define “K”, the number of last few conversations it should remember</b>. It simply tells the LLM, remember the last few discussions and forget all of the rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5f68ca3-8d01-4068-9eda-b493661f04ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: What's your plan today?\\nAI: Have a coffee and enjoy the day.\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "## Adding COnversational in Memory as an example\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What's your plan today?\"},\n",
    "                    {\"output\": \"Have a coffee and enjoy the day.\"})\n",
    "\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False)\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b6430-9ef0-4e98-83f0-9b61850c11bc",
   "metadata": {},
   "source": [
    "From the output, because of `k=1`, we would only keep the last intput/output in the history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f599db-49e0-424f-b0d6-a362b11e9992",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>[ConversationSummaryMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.summary.ConversationSummaryMemory.html)</font>: Conversation summarizer to chat memory.</b>\n",
    "<b><font size='3ptx'>Instead of remembering the exact conversation, can we summarize the previous conversation context and hence help the LLM in answering the upcoming question?</font></b>\n",
    "\n",
    "<b>This is how Summary Memory helps. It keeps on summarizing the previous context and maintains it for use in next discussion.</b>\n",
    "\n",
    "In the below example we create a bit of historical conversation and a scenario. The summary memory instead of remembering the exact conversation remembers a summary and answers using that context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5041380-0c2a-412b-8bdb-45e96b3f384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcf5ca15-670d-4ff5-b0da-6a5c2cd4ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory= ConversationSummaryMemory(llm=llm, max_token_limit=100)\n",
    "\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f67f899-6d4d-4bc1-b971-e8c3bd1d38a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"The human says hello to the AI. The AI asks what's up. The human says they're just hanging out. The human asks what is on the schedule today. The AI says there is a meeting at 8am with the product team, time to work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e43155-49ef-41b7-a434-46d4689a3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    memory=memory, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2bfdecf-2614-431f-9136-a220a4e3fbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human says hello to the AI. The AI asks what's up. The human says they're just hanging out. The human asks what is on the schedule today. The AI says there is a meeting at 8am with the product team, time to work on the LangChain project from 9am-12pm, and lunch at an Italian restaurant with a customer at Noon. The human asks what cool demo in laptop can he show. The AI says it does not have that information in its context.\n",
      "Human: What time is the meeting with the product team?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"What time is the meeting with the product team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "935dac0a-1517-4cc2-8651-2daa80f00a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8am'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12bba3-ebf2-4413-a8f0-676864a4d717",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>[ConversationTokenBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html): Conversation chat memory with token limit.</font></b>\n",
    "<b><font size='3ptx'>Instead of “k” conversations being remembered in [ConversationBufferWindowMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.token_buffer.ConversationTokenBufferMemory.html), in this case we want to remember last set of discussion based on “max token limit”.</font></b>\n",
    "\n",
    "As seen in below example different token limit remembers different size of discussion depending on token limit defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cec56a07-923d-4769-b9bb-abcbde6c93ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=25)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97b61001-f2e0-4269-924a-8dd2bb9ab0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory=ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b0e7c-c2da-468b-aeb5-cb4b39ba29b8",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>[ConversationSummaryBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.summary_buffer.ConversationSummaryBufferMemory.html): Buffer with summarizer for storing conversation memory.</font></b>\n",
    "<b><font size='3ptx'>While summary is good, we know that recent/last conversation has high correlation to upcoming query and a mix of summary of old conversation with a buffer memory of last few conversation would be a good combination.</font></b>\n",
    "\n",
    "This type of memory exactly does that. You can set the token limit which define how much historical conversation to be kept and how much to summarize. Higher the token size more the exact conversation history kept as-is. ([more](https://python.langchain.com/v0.1/docs/modules/memory/types/summary_buffer/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "948353c2-84c3-4911-9f7f-84b2b18dc730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human says \"hi\" and the AI responds with \"whats up\".\\nHuman: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8afda8-d55c-4a20-8c53-471551399093",
   "metadata": {},
   "source": [
    "We can also get the history as a list of messages (this is useful if you are using this with a chat model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31f6eebf-6f78-4cff-9ee4-59f2c7328388",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm, max_token_limit=10, return_messages=True\n",
    ")\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56481ba1-95d7-4d13-b6b4-9ea08cfb3903",
   "metadata": {},
   "source": [
    "We can also utilize the predict_new_summary method directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f4b9f85-14a4-4fe3-a30d-ed822fece8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='not much you'), AIMessage(content='not much')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47769bc8-f524-423e-9ce4-7fcbbe4d9c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human and the AI both say they are not doing much.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8839908-487e-46fa-8f59-ee5a1ec6dc5e",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>More</font></b>\n",
    "<b><font size='3ptx'>Some other memory types also exists which I am yet to explore namely...</font></b>\n",
    "* [**ConversationEntityMemory**](https://api.python.langchain.com/en/latest/memory/langchain.memory.entity.ConversationEntityMemory.html): This extracts and stores key entities (like people, places etc.) from the discussion.\n",
    "* [**Conversation Knowledge Graph Memory**](https://api.python.langchain.com/en/latest/memory/langchain.memory.kg.ConversationKGMemory.html) : Uses a knowledge graph to store information and relationships between entities.\n",
    "* [**VectorStore-Backed Memory**](https://js.langchain.com/v0.1/docs/modules/memory/types/vectorstore_retriever_memory/): Uses vector embeddings to store and retrieve information based on semantic similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63abfd7-e114-4cf3-bc6a-da3e63bc65c2",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Pinecone - Conversational Memory for LLMs with Langchain](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/)\n",
    "> Conversational memory is how a chatbot can respond to multiple queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
    "* [Langchain doc - Memory](https://python.langchain.com/v0.1/docs/modules/memory/)\n",
    "> Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation...\n",
    "* [DeepLearning.AI - LangChain for LLM Application Development: Memory](https://learn.deeplearning.ai/courses/langchain/lesson/3/memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
