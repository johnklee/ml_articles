{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://towardsdatascience.com/ensemble-learning-stacking-blending-voting-b37737c4f483), [github](https://github.com/FernandoLpz/Stacking-Blending-Voting-Ensembles)) <font size='3ptx'>**If you want to increase the effectiveness of your ML model, maybe you should consider Ensemble Learning**</font>\n",
    "![1.jpeg](images/1.jpeg)\n",
    "<br/>\n",
    "\n",
    "We have heard the phrase “unity is strength”, whose meaning can be transferred to different areas of life. Sometimes correct answers to a specific problem are supported by several sources and not just one. **This is what Ensemble Learning tries to do, that is, to put together a group of ML models to improve solutions to specific problems.**\n",
    "\n",
    "Throughout this blog, we will learrn what Ensemble Learning is, what are the types of Ensembles that exist and we will specifically address Voting and Stacking Ensembles. Therefore, this blog will be divided into the following sections:\n",
    "* <font size='3ptx'>[**What is Ensemble Learning?**](#sect1)</font>\n",
    "* <font size='3ptx'>[**Stacking**](#sect2)</font>\n",
    "* <font size='3ptx'>[**Blending**](#sect3)</font>\n",
    "* <font size='3ptx'>[**Voting**](#sect4)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>What is Ensemble Learning?</font>\n",
    "[**Ensemble Learning**](https://en.wikipedia.org/wiki/Ensemble_learning) refers to the use of ML algorithms jointly to solve classification and/or regression problems mainly. These algorithms can be the same type (homogeneous Ensemble Learning) or different types (heterogeneous Ensemble Learning). **Ensemble Learning performs a strategic combination of various experts or ML models in order to improve the effectiveness obtained using a single weak model** \\[[1](http://www.scholarpedia.org/article/Ensemble_learning), [2](https://tjzhifei.github.io/links/EMFA.pdf)]. Figure 1 provides a visual overview regarding the comparison of a model that does not implement Ensemble Learning and a model that does implement Ensemble Learning.\n",
    "![2](images/2.jpeg)\n",
    "<br/>\n",
    "\n",
    "There are different types of Ensemble Learning techniques which differ mainly by the type of models used (<font color='brown'>homogeneous or heterogeneous models</font>), the data sampling (<font color='brown'>with or without replacement, k-fold, etc.</font>) and the decision function (<font color='brown'>voting, average, meta model, etc</font>). Therefore, Ensemble Learning techniques can be classified as:\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking\n",
    "\n",
    "In addition to these three main categories, two important variations emerge: **<font color='darkblue'>Voting</font>** (<font color='brown'>which is a complement of Bagging</font>) and **<font color='darkblue'>Blending</font>** (<font color='brown'>a subtype of Stacking</font>). Although Voting and Blending are a complement and a subtype of Bagging and Stacking respectively, these techniques are often found as direct types of Ensemble Learning.\n",
    "\n",
    "In this blog we will specifically address the Stacking, Blending and Voting techniques, let’s go for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Stacking</font>\n",
    "**Better known as Stacking Generalization, it is a method introduced by [David H. Wolpert in 1992](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)** \\[3] **where the key is to reduce the generalization error of different generalizers** (<font color='brown'>i.e. ML models</font>). The general idea of the Stacking Generalization method is the generation of a Meta-Model. Such **a Meta-Model is made up of the predictions of a set of ML base models** (<font color='brown'>i.e. weak learners</font>) through the k-fold cross validation technique. Finally, the Meta-Model is trained with an additional ML model (<font color='brown'>which is commonly known as the “final estimator” or “final learner”</font>).\n",
    "\n",
    "**The Stacking Generalization method is commonly composed of 2 training stages, better known as “level 0” and “level 1”.** It is important to mention that it can be added as many levels as necessary. However, in practice it is common to use only 2 levels. The aim of the first stage (<font color='brown'>level 0</font>) is to generate the training data for the meta-model, this is carried out by implementing k-fold cross validation for each “weak learner” defined in the first stage. The predictions of each one of these“weak learners” are “stacked” in order to build such such “new training set” (<font color='brown'>the meta-model</font>). **The aim of the second stage** (<font color='brown'>level 1</font>) **is to train the meta-model, such training is carried out through an already determined “<font color='darkblue'>final learner</font>”.**\n",
    "\n",
    "In figure 2 we see a graphical description of an architecture of a Stacking Generalization Classifier that is composed of 3 base models (<font color='brown'>weak learners</font>) and a final estimator.\n",
    "![3](images/3.jpeg)\n",
    "<br/>\n",
    "**Perfect, so far we already know how the <font color='darkblue'>Stacking Generalization</font> technique works**. Now let’s see a small example of how we would do this in code (<font color='brown'>it is important to mention that this technique can be implemented directly from [scikit-learn](https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html), however, in order to make the explanation more demonstrative, let’s see how we do it from scratch</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.k = 5\n",
    "\n",
    "    def load_data(self):\n",
    "        x, y = load_breast_cancer(return_X_y=True)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.3, random_state=23)\n",
    "    \n",
    "    def StackingClassifier(self):\n",
    "        # Define weak learners\n",
    "        weak_learners = [\n",
    "            ('dt', DecisionTreeClassifier()),\n",
    "            ('knn', KNeighborsClassifier()),\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('gb', GradientBoostingClassifier()),\n",
    "            ('gn', GaussianNB())\n",
    "        ]\n",
    "        \n",
    "        # Finaler learner or meta model\n",
    "        final_learner = LogisticRegression()\n",
    "\n",
    "        train_meta_model = None\n",
    "        test_meta_model = None\n",
    "\n",
    "        # Start stacking\n",
    "        for clf_id, clf in weak_learners:\n",
    "            # Predictions for each classifier based on k-fold\n",
    "            predictions_clf = self.k_fold_cross_validation(clf)\n",
    "            \n",
    "            # Predictions for test set for each classifier based on train of level 0\n",
    "            test_predictions_clf = self.train_level_0(clf)\n",
    "            \n",
    "            # Stack predictions which will form \n",
    "            # the inputa data for the data model\n",
    "            if isinstance(train_meta_model, np.ndarray):\n",
    "                train_meta_model = np.vstack((train_meta_model, predictions_clf))\n",
    "            else:\n",
    "                train_meta_model = predictions_clf\n",
    "\n",
    "            # Stack predictions from test set\n",
    "            # which will form test data for meta model\n",
    "            if isinstance(test_meta_model, np.ndarray):\n",
    "                test_meta_model = np.vstack((test_meta_model, test_predictions_clf))\n",
    "            else:\n",
    "                test_meta_model = test_predictions_clf\n",
    "        \n",
    "        # Transpose train_meta_model\n",
    "        train_meta_model = train_meta_model.T\n",
    "\n",
    "        # Transpose test_meta_model\n",
    "        test_meta_model = test_meta_model.T\n",
    "        \n",
    "        # Training level 1\n",
    "        self.train_level_1(final_learner, train_meta_model, test_meta_model)\n",
    "\n",
    "    def k_fold_cross_validation(self, clf):        \n",
    "        predictions_clf = None\n",
    "\n",
    "        # Number of samples per fold\n",
    "        batch_size = int(len(self.x_train) / self.k)\n",
    "\n",
    "        # Stars k-fold cross validation\n",
    "        for fold in range(self.k):\n",
    "\n",
    "            # Settings for each batch_size\n",
    "            if fold == (self.k - 1):\n",
    "                test = self.x_train[(batch_size * fold):, :]\n",
    "                batch_start = batch_size * fold\n",
    "                batch_finish = self.x_train.shape[0]\n",
    "            else:\n",
    "                test = self.x_train[(batch_size * fold): (batch_size * (fold + 1)), :]\n",
    "                batch_start = batch_size * fold\n",
    "                batch_finish = batch_size * (fold + 1)\n",
    "            \n",
    "            # test & training samples for each fold iteration\n",
    "            fold_x_test = self.x_train[batch_start:batch_finish, :]\n",
    "            fold_x_train = self.x_train[[index for index in range(self.x_train.shape[0]) if index not in range(batch_start, batch_finish)], :]\n",
    "\n",
    "            # test & training targets for each fold iteration\n",
    "            fold_y_test = self.y_train[batch_start:batch_finish]\n",
    "            fold_y_train = self.y_train[[index for index in range(self.x_train.shape[0]) if index not in range(batch_start, batch_finish)]]\n",
    "\n",
    "            # Fit current classifier\n",
    "            clf.fit(fold_x_train, fold_y_train)\n",
    "            fold_y_pred = clf.predict(fold_x_test)\n",
    "\n",
    "            # Store predictions for each fold_x_test\n",
    "            if isinstance(predictions_clf, np.ndarray):\n",
    "                predictions_clf = np.concatenate((predictions_clf, fold_y_pred))\n",
    "            else:\n",
    "                predictions_clf = fold_y_pred\n",
    "\n",
    "        return predictions_clf\n",
    "\n",
    "    def train_level_0(self, clf):\n",
    "        # Train in full real training set\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        # Get predictions from full real test set\n",
    "        y_pred = clf.predict(self.x_test)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def train_level_1(self, final_learner, train_meta_model, test_meta_model):\n",
    "        # Train is carried out with final learner or meta model\n",
    "        final_learner.fit(train_meta_model, self.y_train)\n",
    "        # Getting train and test accuracies from meta_model\n",
    "        print(f\"Train accuracy: {final_learner.score(train_meta_model,  self.y_train)}\")\n",
    "        print(f\"Test accuracy: {final_learner.score(test_meta_model, self.y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above class use [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) to load dataset \"[**breast cancer wisconsin dataset**](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)\". Let's see how to execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9597989949748744\n",
      "Test accuracy: 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "es = Ensemble()\n",
    "es.load_data()\n",
    "es.StackingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s analyze the key parts, we are defining 5 classifiers and stored in `weak_learners` (<font color='brown'>weak learners</font>) that will be the base models of our stack (<font color='brown'>which are trained at level 0</font>). We define the final classifier `final_learner` by using [**LogisticRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (<font color='brown'>which is the meta-model classifier</font>). Now, level 0 training begins with the for loop defined in line `for clf_id, clf in weak_learners`. As we can see, in line `predictions_clf = self.k_fold_cross_validation(clf)` we are receiving the predictions of k-fold cross validation and “stacking” these predictions (<font color='brown'>the which are forming the training data of the meta-model</font>). On line `test_predictions_clf = self.train_level_0(clf)` we are receiving the predictions from the test set which are “stacked” to form the meta-model test data. Finally, in line `self.train_level_1(final_learner, train_meta_model, test_meta_model)` we carry out the level 1 training, that is, the meta-model training.\n",
    "\n",
    "Well, so far we already know how the Stacking Generalization technique works. **As we mentioned, one of the key parts of this method is the use of the k-fold cross validation for the generation of the meta-model training data. However, there is a variation, we can omit k-fold cross validation and only use “one-holdout set”, this small but significant variation is called “<font color='darkblue'>Blending</font>”.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Blending</font>\n",
    "**<font color='darkblue'>Blending</font> is a technique derived from Stacking Generalization.** The only difference is that in Blending, the k-fold cross validation technique is not used to generate the training data of the meta-model. Blending implements “one-holdout set”, that is, a small portion of the training data (<font color='brown'>validation</font>) to make predictions which will be “stacked” to form the training data of the meta-model. Also, predictions are made from the test data to form the meta-model test data.\n",
    "\n",
    "In figure 3 we can see a Blending architecture using 3 base models (<font color='brown'>weak learners</font>) and a final classifier. The blue boxes represent that portion of the training data that is used to generate predictions (<font color='brown'>yellow boxes</font>) to form the meta-model. The green boxes represent the test data which is used to generate predictions to form the meta-model test data (<font color='brown'>purple boxes</font>).\n",
    "![3](images/4.jpeg)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that you’re familiar with the Blending architecture, let’s see how we do this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        x, y = load_breast_cancer(return_X_y=True)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.15, random_state=23)\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = train_test_split(\n",
    "            self.x_train, \n",
    "            self.y_train, \n",
    "            test_size=0.3, \n",
    "            random_state=23\n",
    "        )\n",
    "        \n",
    "    def go(self):\n",
    "        self.load_data()\n",
    "        self.BlendingClassifier()\n",
    "    \n",
    "    def BlendingClassifier(self):\n",
    "\n",
    "        # Define weak learners\n",
    "        weak_learners = [('dt', DecisionTreeClassifier()),\n",
    "                        ('knn', KNeighborsClassifier()),\n",
    "                        ('rf', RandomForestClassifier()),\n",
    "                        ('gb', GradientBoostingClassifier()),\n",
    "                        ('gn', GaussianNB())]\n",
    "        \n",
    "        # Finaler learner or meta model\n",
    "        final_learner = LogisticRegression()\n",
    "\n",
    "        train_meta_model = None\n",
    "        test_meta_model = None\n",
    "\n",
    "        # Start stacking\n",
    "        for clf_id, clf in weak_learners:\n",
    "            \n",
    "            # Predictions for each classifier based on k-fold\n",
    "            val_predictions, test_predictions = self.train_level_0(clf)\n",
    "            \n",
    "            # Stack predictions which will form \n",
    "            # the inputa data for the data model\n",
    "            if isinstance(train_meta_model, np.ndarray):\n",
    "                train_meta_model = np.vstack((train_meta_model, val_predictions))\n",
    "            else:\n",
    "                train_meta_model = val_predictions\n",
    "\n",
    "            # Stack predictions from test set\n",
    "            # which will form test data for meta model\n",
    "            if isinstance(test_meta_model, np.ndarray):\n",
    "                test_meta_model = np.vstack((test_meta_model, test_predictions))\n",
    "            else:\n",
    "                test_meta_model = test_predictions\n",
    "        \n",
    "        # Transpose train_meta_model\n",
    "        train_meta_model = train_meta_model.T\n",
    "\n",
    "        # Transpose test_meta_model\n",
    "        test_meta_model = test_meta_model.T\n",
    "        \n",
    "        # Training level 1\n",
    "        self.train_level_1(final_learner, train_meta_model, test_meta_model)\n",
    "\n",
    "\n",
    "    def train_level_0(self, clf):\n",
    "        # Train with base x_train\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        \n",
    "        # Generate predictions for the holdout set (validation)\n",
    "        # These predictions will build the input for the meta model\n",
    "        val_predictions = clf.predict(self.x_val)\n",
    "        \n",
    "        # Generate predictions for original test set\n",
    "        # These predictions will be used to test the meta model\n",
    "        test_predictions = clf.predict(self.x_test)\n",
    "\n",
    "        return val_predictions, test_predictions\n",
    "\n",
    "    def train_level_1(self, final_learner, train_meta_model, test_meta_model):\n",
    "        # Train is carried out with final learner or meta model\n",
    "        final_learner.fit(train_meta_model, self.y_val)\n",
    "       \n",
    "        # Getting train and test accuracies from meta_model\n",
    "        print(f\"Train accuracy: {final_learner.score(train_meta_model,  self.y_val)}\")\n",
    "        print(f\"Test accuracy: {final_learner.score(test_meta_model, self.y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s analyze the key parts of this model. We are defining the 5 base classifiers and stored them in `weak_learners` that we will use (<font color='brown'>weak learners</font>), we define the final classifier `final_learner`, as in the previous example, we will use [**LogisticRegression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Level 0 training begins on line `for clf_id, clf in weak_learners:`. As we can see, on line `val_predictions, test_predictions = self.train_level_0(clf)` we are receiving the predictions of the validation set (<font color='brown'>which will form the training data of the meta-model</font>) and the predictions of the test data (<font color='brown'>the which will form the meta-model test data</font>). Also, we are “stacking” the predictions of each base classifier. Finally, on line `self.train_level_1(final_learner, train_meta_model, test_meta_model)` we are moving to level 1 training, and that is it!\n",
    "\n",
    "As we can see, the **Blending** architecture is slightly simpler and more compact than Stack Generalization. **Omitting k-fold cross validation can make us optimize the processing time.**\n",
    "\n",
    "Great, by now you already know the Stacked Generalization architecture and how it works as well as the variation that arises from it (<font color='brown'>Blending</font>). **The million dollar question remains: which technique is better? When should I apply Stacking or Blending? Well, that will depend 100% on the task you are trying to solve, the amount of data you have as well as the computing power and memory available.**\n",
    "\n",
    "Finally, let’s talk about an Ensemble Learning technique that is simple, intuitive and that can sometimes be a good option, let’s talk about Voting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9448275862068966\n",
      "Test accuracy: 0.9651162790697675\n"
     ]
    }
   ],
   "source": [
    "es = Ensemble()\n",
    "es.go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>Voting</font>\n",
    "This type of ensemble is one of the most intuitive and easy to understand. The <font color='darkblue'>**Voting Classifier**</font> is a homogeneous and heterogeneous type of Ensemble Learning, that is, the base classifiers can be of the same or different type. As mentioned earlier, **this type of ensemble also works as an extension of bagging** (<font color='brown'>e.g. Random Forest</font>).\n",
    "\n",
    "**The architecture of a Voting Classifier is made up of a number “n” of ML models, whose predictions are valued in two different ways: hard and soft.** In hard mode, the winning prediction is the one with “the most votes”. In Figure 2 we see an example of how the Voting Classifier works in hard mode.\n",
    "![5](images/5.jpeg)\n",
    "<br/>\n",
    "\n",
    "On the other hand, **the Voting Classifier in soft mode considers the probabilities thrown by each ML model, these probabilities will be weighted and averaged, consequently the winning class will be the one with the highest weighted and averaged probability**. In Figure 3 we see an example of how the Voting Classifier works in the soft mode.\n",
    "![6](images/6.jpeg)\n",
    "<br/>\n",
    "Ok, now that we know how the Voiting Classifier works, let’s see how to do this in code. On this occasion, since it is a simple and intuitive ensemble technique (<font color='brown'>compared to Stacking or Blending</font>), let’s make use of the function provided by scikit-learn for the implementation of Voting, let’s do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "class Ensemble:\n",
    "    def __init__(self):\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_data(self):\n",
    "        x, y = load_breast_cancer(return_X_y=True)\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(x, y, test_size=0.25, random_state=23)\n",
    "\n",
    "    def go(self):\n",
    "        self.load_data()\n",
    "        # vc, decision_tree, knn, logistic_regression = self.__VotingClassifier__()\n",
    "       \n",
    "        # Getting train and test accuracies from meta_model\n",
    "        for name, clf in self.__VotingClassifier__():\n",
    "            print(f\"{name} Train accuracy: {clf.score(self.x_train,  self.y_train)}\")\n",
    "            print(f\"{name} Test accuracy: {clf.score(self.x_test, self.y_test)}\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __Classifiers__(name=None):\n",
    "        # See for reproducibility\n",
    "        random_state = 23\n",
    "        \n",
    "        if name == 'decision_tree':\n",
    "            return DecisionTreeClassifier(random_state=random_state)\n",
    "        if name == 'kneighbors':\n",
    "            return KNeighborsClassifier()\n",
    "        if name == 'logistic_regression':\n",
    "            return LogisticRegression(random_state=random_state)\n",
    "\n",
    "    def __DecisionTreeClassifier__(self):\n",
    "        \n",
    "        # Decision Tree Classifier\n",
    "        decision_tree = Ensemble.__Classifiers__(name='decision_tree')\n",
    "        \n",
    "        # Train Decision Tree\n",
    "        decision_tree.fit(self.x_train, self.y_train)\n",
    "        return decision_tree\n",
    "\n",
    "    def __KNearestNeighborsClassifier__(self):\n",
    "        \n",
    "        # K-Nearest Neighbors Classifier\n",
    "        knn = Ensemble.__Classifiers__(name='kneighbors')\n",
    "        \n",
    "        # Train K-Nearest Neighbos\n",
    "        knn.fit(self.x_train, self.y_train)\n",
    "        return knn\n",
    "\n",
    "    def __LogisticRegression__(self):\n",
    "        \n",
    "        # Decision Tree Classifier\n",
    "        logistic_regression = Ensemble.__Classifiers__(name='logistic_regression')\n",
    "        \n",
    "        # Init Grid Search\n",
    "        logistic_regression.fit(self.x_train, self.y_train)\n",
    "        return logistic_regression\n",
    "    \n",
    "    def __VotingClassifier__(self):\n",
    "\n",
    "        # Instantiate classifiers\n",
    "        decision_tree = Ensemble.__Classifiers__(name='decision_tree')\n",
    "        knn = Ensemble.__Classifiers__(name='kneighbors')\n",
    "        logistic_regression = Ensemble.__Classifiers__(name='logistic_regression')\n",
    "\n",
    "        # Voting Classifier initialization\n",
    "        vc = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('decision_tree', decision_tree), \n",
    "                ('knn', knn), \n",
    "                ('logistic_regression', logistic_regression)], \n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Init Grid Search\n",
    "        vc.fit(self.x_train, self.y_train)\n",
    "        return (\n",
    "            (\"voting\", vc), \n",
    "            ('decision_tree', self.__DecisionTreeClassifier__()), \n",
    "            ('knn', self.__KNearestNeighborsClassifier__()), \n",
    "            ('logistic_regression', self.__LogisticRegression__())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voting Train accuracy: 0.9835680751173709\n",
      "voting Test accuracy: 0.965034965034965\n",
      "==================================================\n",
      "decision_tree Train accuracy: 1.0\n",
      "decision_tree Test accuracy: 0.958041958041958\n",
      "==================================================\n",
      "knn Train accuracy: 0.9460093896713615\n",
      "knn Test accuracy: 0.9300699300699301\n",
      "==================================================\n",
      "logistic_regression Train accuracy: 0.9507042253521126\n",
      "logistic_regression Test accuracy: 0.9440559440559441\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "es = Ensemble()\n",
    "es.go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we are creating a class which will contain different classifiers which are: Decision Tree, K-Nearest Neighbors, Logistic Regression and [**Voting Classifier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html). To compare the effectiveness between “weak classifiers” and the Ensemble, we will make use of the “[**breast_cancer**](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data)” toy dataset. We will use each classifier with its default values.\n",
    "\n",
    "**As we can see, the Test accuracy of the Voting Classifier is slightly better than that of the weak classifiers**. It is very important to mention that, although Voting Classifier is a great alternative to improve the accuracy of your models, it may not always be the best option due to various factors, including processing time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
