{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c720bf-ac60-4faa-b69e-4af3912e2462",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://cobusgreyling.medium.com/using-langchain-with-model-context-protocol-mcp-e89b87ee3c4c)) <b><font size='3ptx'>The [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) is an open-source protocol developed by Anthropic, focusing on safe and interpretable Generative AI systems. MCP emerged from the need to address a key limitation with Large Language Model (LLM) applications, that being their isolation from external data sources and tools.</font></b>\n",
    "\n",
    "<b>One of the key focus areas of LLM-based applications has been the aspect of Data Delivery</b>. Getting data to the LLM for inference, this has been the objective of RAG implementations, Fine-tuning and how also the objective of MCP.\n",
    "\n",
    "MCP‚Äôs primary purpose is to standardise how LLM-based applications connect to diverse systems, as seen in the image below:\n",
    "![MCP servers](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cAZLUf-GB9jXmOHD_ThaEA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8022f-a426-464e-a79a-d1f65e70b16a",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Eliminating Custom Integration</font></b>\n",
    "<b><font size='3ptx'>There is this challenge with AI Agents to delivery data to the AI Agent, or in other words; integrate the AI Agent/LLM-based application to external data sources.</font></b>\n",
    "\n",
    "There is been numerous attempts to integrate somehow seamless by leveraging GUI‚Äôs, web browser and web search. All of these avenues have advantages and disadvantages.\n",
    "![ex](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*A4Sjk8hqd8TDEEyVaXahcw.gif)\n",
    "\n",
    "<b>MCP has the potential to function as a universal interface, think of it as the virtual / software version of USB-C for AI.</b> Enabling seamless, secure and scalable data exchange between LLMs/AI Agents and external resources.\n",
    "\n",
    "<b>MCP uses a client-server architecture where MCP hosts</b> (AI applications) <b>communicate with MCP servers</b> (data/tool providers). Developers can use MCP to build reusable, modular connectors, with pre-built servers available for popular platforms, creating a community-driven ecosystem.\n",
    "\n",
    "MCP‚Äôs open-source nature encourages innovation, allowing developers to extend its capabilities while maintaining security through features like granular permissions. <b>Ultimately, MCP aims to transform AI Agents from isolated chatbots into context-aware, interoperable systems deeply integrated into digital environments</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8d009-c6ca-446a-a3e1-63e843c004a4",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Step by Step Instructions</font></b>\n",
    "<b><font size='3ptx'>Anthropic‚Äôs Model Context Protocol (MCP) is an open source protocol to connect LLMs with context, tools, and prompts. It has a growing number of ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò¥ for connecting to various tools or data sources. Here, we show how to connect any MCP server to LangGraph agents & use MCP tools‚Ä¶</font></b>\n",
    "\n",
    "If you are like me, getting a prototype work, no matter how simple it is, brings an immense sense of clarity and understanding; at leat in my own mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a05772-a6db-47f2-8ea0-8ad493a157c8",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>What is the Model Context Protocol (MCP)?</font></b>\n",
    "([source](https://apidog.com/blog/fastmcp/)) <b><font size='3ptx'>Before diving deeper into FastMCP, it's essential to grasp the core concepts of MCP itself.</font></b>\n",
    "\n",
    "MCP defines a standardized way for LLM applications (clients) to interact with external systems (servers). An MCP server can expose several key components:\n",
    "* **Tools:** These are essentially functions that an LLM can request the server to execute. Think of them like POST endpoints in a traditional API. They perform actions, potentially interact with other systems (databases, APIs, hardware), and return results. For example, a tool could send an email, query a database, or perform a calculation.\n",
    "* **Resources:** These expose data that an LLM can read or retrieve. Similar to GET endpoints, resources provide information to enrich the LLM's context. This could be anything from configuration files and user profiles to real-time data streams.\n",
    "* **Prompts:** These are reusable templates for structuring interactions with the LLM. They help guide the conversation and ensure consistent outputs for specific tasks.\n",
    "* **Context:** Servers can provide contextual information, including instructions on how to best interact with the available tools and resources.\n",
    "\n",
    "<b>MCP aims to create a robust and secure ecosystem where LLMs can reliably access and utilize external capabilities</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b886659-56ff-45e6-8e24-15c67df485ca",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Why Choose FastMCP?</font></b>\n",
    "<b><font size='3ptx'>While you could implement the MCP specification directly using lower-level SDKs, FastMCP offers compelling advantages, particularly for Python developers:</font></b>\n",
    "* **üöÄ Fast Development:** Its high-level interface significantly reduces the amount of code needed, accelerating the development process. Often, defining a tool or resource is as simple as decorating a standard Python function.\n",
    "* **üçÄ Simplicity:** FastMCP abstracts away the complex details of server setup, protocol handling, content types, and error management, minimizing boilerplate.\n",
    "* **üêç Pythonic:** Designed with Python best practices in mind, it feels natural and intuitive for developers familiar with the language, leveraging features like type hints and decorators.\n",
    "* **üîç Complete:** FastMCP aims to provide a comprehensive implementation of the core MCP specification, ensuring compatibility and access to the protocol's full potential.\n",
    "\n",
    "FastMCP version 1 proved highly successful and is now integrated into the official MCP Python SDK. Version 2 builds upon this foundation, introducing advanced features focused on simplifying server interactions, such as flexible clients, server proxying, and composition patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417bf2e-208f-4335-a5d0-c7c6f0bab031",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>How to Install FastMCP</font></b>\n",
    "Getting FastMCP set up in your Python environment is straightforward:\n",
    "```shell\n",
    "$ pip install fastmcp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37830a2-806b-4d97-956a-4043398f4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastmcp==2.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep 'fastmcp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddfc48-20a8-4136-ba4f-aefc51960f13",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Virtual environment</font></b>\n",
    "It is good practice to create a virtual environment to install and run code; the command below creates the virtual environment called `MCP_Demo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c585c8-dc34-4a36-89d2-4d0d2dc2561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m venv MCP_Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb96d45-5d4a-45b2-8e9f-017caf3fbbe3",
   "metadata": {},
   "source": [
    "Then run this command to activate (enter) the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2c4337-dc65-4dd7-85c2-523e2c025465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source MCP_Demo/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cb776-d0db-4f12-ae07-9d1ac701054a",
   "metadata": {},
   "source": [
    "You will see your command prompt is updated with (`MCP_Demo`).\n",
    "\n",
    "Then run the following lines of code in sequence to install the required packages:\n",
    "```shell\n",
    "$ pip install langchain-mcp-adapters\n",
    "$ pip install langchain-mcp-adapters\n",
    "$ export GEMINI_API_KEY=<your_api_key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1fe42a-49ac-4d0d-ae80-3676fcec97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-mcp-adapters==0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep 'langchain-mcp-adapters'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17d480-3ce3-4422-b568-ceb9a340e06b",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Server implementation</font></b>\n",
    "<b><font size='3ptx'>An LLM MCP (Model Control Plane) Server is a backend component that manages and routes requests to large language models.</font></b>\n",
    "\n",
    "The MCP server acts as a gateway between user applications and the model infrastructure, ensuring efficient, secure, and scalable access to LLM capabilities. It's especially useful in systems that serve multiple models or need fine-grained control over inference workflows.\n",
    "\n",
    "- **`math_server.py`**:\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# name or identifier of the LLM app or service\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "  \"\"\"Add two numbers\"\"\"\n",
    "  return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "  \"\"\"Multiply two numbers\"\"\"\n",
    "  return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # \"stdio\" tells the MCP server to read requests from standard input (stdin) and write responses to standard output (stdout).\n",
    "  mcp.run(transport=\"stdio\")\n",
    "```\n",
    "\n",
    "Run the server with the following command:\n",
    "```shell\n",
    "$ python math_server.py\n",
    "```\n",
    "\n",
    "While the MCP server is running in the one tab, let's open a new tab to develop MCP client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa0be0-4a5e-497b-b3ea-ec55fa59c4d2",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Client</font></b>\n",
    "<b><font size='3ptx'>An LLM MCP Client is a component or library that communicates with an LLM MCP (Model Control Plane) Server to send prompts and receive model-generated responses.</font></b>\n",
    "\n",
    "It abstracts the details of the transport layer (e.g., stdio, http, or websocket) and provides a simple interface for applications to interact with large language models. \n",
    "\n",
    "- **`mcp_client.py`**:\n",
    "\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import asyncio\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "executing_path = Path(working_directory) / Path(\"math_server.py\")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[executing_path],\n",
    ")\n",
    "\n",
    "async def run_agent():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "\n",
    "            # Get tools\n",
    "            tools = await load_mcp_tools(session)\n",
    "\n",
    "            # Create and run the agent\n",
    "            agent = create_react_agent(model, tools)\n",
    "            agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "            return agent_response\n",
    "\n",
    "# Run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    result = asyncio.run(run_agent())\n",
    "    for message in result['messages']:\n",
    "      print(message.pretty_print())\n",
    "      print('\\n')\n",
    "```\n",
    "\n",
    "From above sample code, it leverage function [**`langchain.agents.react.agent.create_react_agent`**](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to bind the tools with LLM mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5f9010-ffe4-40c9-89af-b33ed8ceedf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/john/Gitrepos/ml_articles/medium/Using_Langchain_with_model-context-protocol_mcp/math_server.py\n"
     ]
    }
   ],
   "source": [
    "# Confirm we got correct execution path:\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "executing_path = Path(working_directory) / Path(\"math_server.py\")\n",
    "\n",
    "print(executing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3778726-c83d-4525-b2c9-8ebcba4d9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm our model API key setting is correct:\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0ef6c-c0ad-45b2-b1b4-542024d8d33d",
   "metadata": {},
   "source": [
    "Run the client with the command: `python mcp_client.py`\n",
    "\n",
    "The client will run once and end with the output below:\n",
    "```\n",
    "================================ Human Message =================================\n",
    "\n",
    "what's (3 + 5) x 12?\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  add (179d68f6-144c-4dcb-afd0-c8595deba9c6)\n",
    " Call ID: 179d68f6-144c-4dcb-afd0-c8595deba9c6\n",
    "  Args:\n",
    "    a: 3.0\n",
    "    b: 5.0\n",
    "None\n",
    "\n",
    "\n",
    "================================= Tool Message =================================\n",
    "Name: add\n",
    "\n",
    "8\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  multiply (0f6dfdda-3830-463f-9cfe-2b7c5f084a64)\n",
    " Call ID: 0f6dfdda-3830-463f-9cfe-2b7c5f084a64\n",
    "  Args:\n",
    "    a: 8.0\n",
    "    b: 12.0\n",
    "None\n",
    "\n",
    "\n",
    "================================= Tool Message =================================\n",
    "Name: multiply\n",
    "\n",
    "96\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "(3 + 5) x 12 is 96.\n",
    "None\n",
    "```\n",
    "\n",
    "MCP is a convenient way of integrating AI Agents with information and services supplying context and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4404461-7544-4f28-9bd5-755b5bdd1493",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>How to Use FastMCP: Building Your First Server</font></b>\n",
    "Now, let's dive into the practical aspects of using FastMCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45c6917-fd47-47c7-9d73-a75f16b6968b",
   "metadata": {},
   "source": [
    "#### <b>1. Creating a Basic Server Instance:</b>\n",
    "The core of any FastMCP application is the <b><font color='blue'>FastMCP</font></b> class. You start by creating an instance of this class.\n",
    "\n",
    "- **`my_fastmcp_server.py`**:\n",
    "\n",
    "```python\n",
    "from fastmcp import FastMCP\n",
    "import asyncio # We'll need this later for the client\n",
    "\n",
    "# Instantiate the server, giving it a name\n",
    "mcp = FastMCP(name=\"My First MCP Server\")\n",
    "\n",
    "print(\"FastMCP server object created.\")\n",
    "```\n",
    "\n",
    "The <b><font color='blue'>FastMCP</font></b> constructor accepts several helpful arguments:\n",
    "* **name (str, optional)**: A human-readable name for your server (defaults to \"FastMCP\"). Useful for identification in logs or client applications.\n",
    "* **instructions (str, optional)**: A description guiding clients on how to interact with the server, explaining its purpose or highlighting key functionalities.\n",
    "* **lifespan (callable, optional)**: An async context manager for handling server startup and shutdown logic (<font color='brown'>e.g., initializing database connections</font>).\n",
    "* **tags (set[str], optional)**: Tags to categorize the server itself.\n",
    "* **`**settings`**: You can pass keyword arguments corresponding to ServerSettings (like port, host, log_level) directly to the constructor for configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c4dd4-2aad-47bb-9015-ba1aa77461cb",
   "metadata": {},
   "source": [
    "#### <b>2. Adding Components:</b>\n",
    "An empty server isn't very useful. Let's add the core MCP components.\n",
    "\n",
    "**Adding a Tool**: Tools are functions exposed to the client. Use the `@mcp.tool()` decorator. FastMCP uses Python type hints to define the expected input parameters and return type for the client.\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py (continued)\n",
    "@mcp.tool()\n",
    "def greet(name: str) -> str:\n",
    "    \"\"\"Returns a simple greeting.\"\"\"\n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "print(\"Tools 'greet' and 'add' added.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683528b9-aac0-487b-bfd8-3b6b19b70fd8",
   "metadata": {},
   "source": [
    "**Adding a Resource**: Resources expose data via a URI. Use the `@mcp.resource()` decorator, providing the URI string.\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py (continued)\n",
    "APP_CONFIG = {\"theme\": \"dark\", \"version\": \"1.1\", \"feature_flags\": [\"new_dashboard\"]}\n",
    "\n",
    "@mcp.resource(\"data://config\")\n",
    "def get_config() -> dict:\n",
    "    \"\"\"Provides the application configuration.\"\"\"\n",
    "    return APP_CONFIG\n",
    "\n",
    "print(\"Resource 'data://config' added.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a62882-b07c-4f25-b08c-04267e9da474",
   "metadata": {},
   "source": [
    "**Adding a Resource Template:** These are like dynamic resources where parts of the URI act as parameters.\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py (continued)\n",
    "USER_PROFILES = {\n",
    "    101: {\"name\": \"Alice\", \"status\": \"active\"},\n",
    "    102: {\"name\": \"Bob\", \"status\": \"inactive\"},\n",
    "}\n",
    "\n",
    "@mcp.resource(\"users://{user_id}/profile\")\n",
    "def get_user_profile(user_id: int) -> dict:\n",
    "    \"\"\"Retrieves a user's profile by their ID.\"\"\"\n",
    "    # The {user_id} from the URI is automatically passed as an argument\n",
    "    return USER_PROFILES.get(user_id, {\"error\": \"User not found\"})\n",
    "\n",
    "print(\"Resource template 'users://{user_id}/profile' added.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f6256-a0b1-44a7-9c28-7e9547c3a917",
   "metadata": {},
   "source": [
    "**Adding a Prompt:** Prompts define reusable interaction patterns.\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py (continued)\n",
    "@mcp.prompt(\"summarize\")\n",
    "async def summarize_prompt(text: str) -> list[dict]:\n",
    "    \"\"\"Generates a prompt to summarize the provided text.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant skilled at summarization.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Please summarize the following text:\\n\\n{text}\"}\n",
    "    ]\n",
    "\n",
    "print(\"Prompt 'summarize' added.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf7312-34fd-4d3d-9aab-6121596084e1",
   "metadata": {},
   "source": [
    "#### <b>3. Testing the Server (In-Process):</b>\n",
    "Before running the server externally, you can test its components directly within the same Python script using the <b><font color='blue'>Client</font></b>. This is useful for quick checks and unit testing.\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py (continued)\n",
    "from fastmcp import Client # Import the client\n",
    "\n",
    "async def test_server_locally():\n",
    "    print(\"\\n--- Testing Server Locally ---\")\n",
    "    # Point the client directly at the server object\n",
    "    client = Client(mcp)\n",
    "\n",
    "    # Clients are asynchronous, so use an async context manager\n",
    "    async with client:\n",
    "        # Call the 'greet' tool\n",
    "        greet_result = await client.call_tool(\"greet\", {\"name\": \"FastMCP User\"})\n",
    "        print(f\"greet result: {greet_result}\")\n",
    "\n",
    "        # Call the 'add' tool\n",
    "        add_result = await client.call_tool(\"add\", {\"a\": 5, \"b\": 7})\n",
    "        print(f\"add result: {add_result}\")\n",
    "\n",
    "        # Read the 'config' resource\n",
    "        config_data = await client.read_resource(\"data://config\")\n",
    "        print(f\"config resource: {config_data}\")\n",
    "\n",
    "        # Read a user profile using the template\n",
    "        user_profile = await client.read_resource(\"users://101/profile\")\n",
    "        print(f\"User 101 profile: {user_profile}\")\n",
    "\n",
    "        # Get the 'summarize' prompt structure (doesn't execute the LLM call here)\n",
    "        prompt_messages = await client.get_prompt(\"summarize\", {\"text\": \"This is some text.\"})\n",
    "        print(f\"Summarize prompt structure: {prompt_messages}\")\n",
    "\n",
    "# Run the local test function\n",
    "# asyncio.run(test_server_locally())\n",
    "# Commented out for now, we'll focus on running the server next\n",
    "```\n",
    "\n",
    "Note the use of `async` and `await`. <b>FastMCP clients operate asynchronously, requiring an `async` function and using `async with client`: to manage the client's lifecycle</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512e7e3-9ec9-4454-8d7b-c53e4427f441",
   "metadata": {},
   "source": [
    "#### <b>4. Running the Server:</b>\n",
    "To make your MCP server accessible to external clients (like an LLM application), you need to run it. There are two primary ways:\n",
    "\n",
    "**Standard Python Execution** (Recommended for Compatibility):\n",
    "Add the following `if __name__ == \"__main__\":` block to your <font color='olive'>my_fastapi_mcp_server.py</font> file. This is the standard Python practice for making a script executable:\n",
    "\n",
    "```python\n",
    "# my_fastmcp_server.py(at the end of the file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Starting FastMCP Server via __main__ ---\")\n",
    "    # This starts the server, typically using the stdio transport by default\n",
    "    mcp.run()\n",
    "```\n",
    "\n",
    "To run the server, execute the script from your terminal:\n",
    "\n",
    "```shell\n",
    "$ python my_fastapi_mcp_server.py\n",
    "```\n",
    "\n",
    "This command starts the MCP server, listening for client connections using the default stdio (<font color='brown'>standard input/output</font>) transport mechanism. This method ensures your server runs consistently for various clients that expect to execute a Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ce2a9c-6584-4e92-9aae-a160c0e0afa3",
   "metadata": {},
   "source": [
    "**Using the FastMCP CLI:**\n",
    "\n",
    "FastMCP provides a command-line interface for running servers, which offers more flexibility and control, especially regarding transport options.\n",
    "\n",
    "```shell\n",
    "# Run the server using stdio (default)\n",
    "$ fastmcp run my_fastmcp_server.py:mcp\n",
    "\n",
    "# Run the server using Server-Sent Events (SSE) on port 8080\n",
    "$ fastmcp run my_fastmcp_server.py:mcp --transport sse --port 8080 --host 0.0.0.0\n",
    "\n",
    "# Run with a different log level\n",
    "$ fastmcp run my_fastmcp_server.py:mcp --transport sse --log-level DEBUG\n",
    "```\n",
    "\n",
    "Key points about the CLI:\n",
    "\n",
    "* **`my_fastmcp_server.py:mcp`**: Specifies the file (my_server.py) and the FastMCP server object within that file (mcp). If you omit :mcp, FastMCP will try to automatically find an object named mcp, app, or server.\n",
    "* **The `if __name__ == \"__main__\"`**: block is not required when using fastmcp run; the CLI directly finds and runs the specified server object.\n",
    "* **`--transport`**: Selects the communication protocol (stdio, sse). SSE is common for web-based interactions.\n",
    "* **`--port, --host, --log-level`**: Configure transport and logging settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69583e-42cf-4b09-8394-f4f1bdd29fa9",
   "metadata": {},
   "source": [
    "#### <b>5. Interacting with a Running Server (Client):</b>\n",
    "Once your server is running (either via `python my_fastmcp_server.py` or `fastmcp run`), you can create a separate client script to interact with it.\n",
    "\n",
    "* **`my_fastmcp_client.py`**:\n",
    "\n",
    "```python\n",
    "from fastmcp import Client\n",
    "import asyncio\n",
    "\n",
    "async def interact_with_server():\n",
    "    print(\"--- Creating Client ---\")\n",
    "\n",
    "    # Option 1: Connect to a server run via `my_fastmcp_server.py` (uses stdio)\n",
    "    # client = Client(\"my_fastmcp_server.py\")\n",
    "\n",
    "    # Option 2: Connect to a server run via `fastmcp run ... --transport sse --port 8080`\n",
    "    client = Client(\"http://127.0.0.1:8080/sse/\") # Use the correct URL/port\n",
    "\n",
    "    print(f\"Client configured to connect to: {client}\")\n",
    "\n",
    "    try:\n",
    "        async with client:\n",
    "            print(\"--- Client Connected ---\")\n",
    "            # Call the 'greet' tool\n",
    "            greet_result = await client.call_tool(\"greet\", {\"name\": \"Remote Client\"})\n",
    "            print(f\"greet result: {greet_result}\")\n",
    "\n",
    "            # Read the 'config' resource\n",
    "            config_data = await client.read_resource(\"data://config\")\n",
    "            print(f\"config resource: {config_data}\")\n",
    "\n",
    "            # Read user profile 102\n",
    "            profile_102 = await client.read_resource(\"users://102/profile\")\n",
    "            print(f\"User 102 profile: {profile_102}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        print(\"--- Client Interaction Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(interact_with_server())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ffe094-d6c4-4ea5-bf6a-c30daaeb8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import Client\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddea5cec-d9bc-43a3-a10c-88528348404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"http://127.0.0.1:8080/sse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b31899-cca8-4c11-b27f-f685ba02ab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object Client.close at 0x74bfb827dc00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55335f99-b273-4437-8fb5-5d57fb08aa19",
   "metadata": {},
   "source": [
    "Run this client script while the server is running in another terminal:\n",
    "\n",
    "```shell\n",
    "$ python my_fastmcp_client.py\n",
    "```\n",
    "\n",
    "The client will connect to the running server (<font color='brown'>make sure the `Client(...)` target matches how the server is running ‚Äì file path for stdio, URL for sse</font>), execute the tool calls and resource reads, and print the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340ebad-913c-4c0a-8aa6-bbc4ca3b9c34",
   "metadata": {},
   "source": [
    "#### <b>6. Server Configuration (ServerSettings):</b>\n",
    "You can fine-tune server behavior using <b><font color='blue'>ServerSettings</font></b>. Settings can be applied in order of precedence:\n",
    "1. Keyword arguments during `FastMCP` initialization (highest precedence).\n",
    "2. Environment variables (prefixed with `FASTMCP_SERVER_`, e.g., `FASTMCP_SERVER_PORT=8888`).\n",
    "3. Values loaded from a `.env` file in the working directory.\n",
    "4. Default values (lowest precedence).\n",
    "\n",
    "\n",
    "Example configuring during initialization:\n",
    "\n",
    "```python\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "mcp_configured = FastMCP(\n",
    "    name=\"ConfiguredServer\",\n",
    "    port=8080,  # Sets the default SSE port\n",
    "    host=\"127.0.0.1\", # Sets the default SSE host\n",
    "    log_level=\"DEBUG\", # Sets the logging level\n",
    "    on_duplicate_tools=\"warn\" # Warn if tools with the same name are registered (options: 'error', 'warn', 'ignore')\n",
    ")\n",
    "\n",
    "# Access settings via the .settings attribute\n",
    "print(f\"Configured Port: {mcp_configured.settings.port}\") # Output: 8080\n",
    "print(f\"Duplicate Tool Policy: {mcp_configured.settings.on_duplicate_tools}\") # Output: warn\n",
    "```\n",
    "\n",
    "Key configuration options include `host`, `port`, `log_level`, and `policies` for handling duplicate component names (`on_duplicate_tools`, `on_duplicate_resources`, `on_duplicate_prompts`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4eb0-4c75-4be9-82ac-3265edeb1c05",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>What Else You Can Do with FastMCP?</font></b>\n",
    "FastMCP also supports more advanced use cases:\n",
    "* **Composition**: Combine multiple FastMCP servers. `main.mount(\"sub\", sub_server)` creates a live link, while `main.import_server(sub_server)` copies components. This aids modularity.\n",
    "* **Proxying**: Use `FastMCP.from_client(client)` to create a FastMCP server instance that acts as a proxy for another MCP server (<font color='brown'>local or remote</font>). This is useful for bridging transports (<font color='brown'>e.g., exposing a remote SSE server via local stdio</font>) or adding a unified frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983c83e-7f89-4c4b-836f-f0e721f271bc",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Gemini API quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "* [**LangChain doc:** ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) \n",
    "* [A Beginner's Guide to Use FastMCP](https://apidog.com/blog/fastmcp/)\n",
    "* [**Github - langchain-mcp-adapters**](https://github.com/langchain-ai/langchain-mcp-adapters)\n",
    "> ![ui](https://github.com/langchain-ai/langchain-mcp-adapters/raw/main/static/img/mcp.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
