{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0c720bf-ac60-4faa-b69e-4af3912e2462",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://cobusgreyling.medium.com/using-langchain-with-model-context-protocol-mcp-e89b87ee3c4c)) <b><font size='3ptx'>The [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) is an open-source protocol developed by Anthropic, focusing on safe and interpretable Generative AI systems. MCP emerged from the need to address a key limitation with Large Language Model (LLM) applications, that being their isolation from external data sources and tools.</font></b>\n",
    "\n",
    "<b>One of the key focus areas of LLM-based applications has been the aspect of Data Delivery</b>. Getting data to the LLM for inference, this has been the objective of RAG implementations, Fine-tuning and how also the objective of MCP.\n",
    "\n",
    "MCP‚Äôs primary purpose is to standardise how LLM-based applications connect to diverse systems, as seen in the image below:\n",
    "![MCP servers](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cAZLUf-GB9jXmOHD_ThaEA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8022f-a426-464e-a79a-d1f65e70b16a",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Eliminating Custom Integration</font></b>\n",
    "<b><font size='3ptx'>There is this challenge with AI Agents to delivery data to the AI Agent, or in other words; integrate the AI Agent/LLM-based application to external data sources.</font></b>\n",
    "\n",
    "There is been numerous attempts to integrate somehow seamless by leveraging GUI‚Äôs, web browser and web search. All of these avenues have advantages and disadvantages.\n",
    "![ex](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*A4Sjk8hqd8TDEEyVaXahcw.gif)\n",
    "\n",
    "<b>MCP has the potential to function as a universal interface, think of it as the virtual / software version of USB-C for AI.</b> Enabling seamless, secure and scalable data exchange between LLMs/AI Agents and external resources.\n",
    "\n",
    "<b>MCP uses a client-server architecture where MCP hosts</b> (AI applications) <b>communicate with MCP servers</b> (data/tool providers). Developers can use MCP to build reusable, modular connectors, with pre-built servers available for popular platforms, creating a community-driven ecosystem.\n",
    "\n",
    "MCP‚Äôs open-source nature encourages innovation, allowing developers to extend its capabilities while maintaining security through features like granular permissions. <b>Ultimately, MCP aims to transform AI Agents from isolated chatbots into context-aware, interoperable systems deeply integrated into digital environments</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8d009-c6ca-446a-a3e1-63e843c004a4",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Step by Step Instructions</font></b>\n",
    "<b><font size='3ptx'>Anthropic‚Äôs Model Context Protocol (MCP) is an open source protocol to connect LLMs with context, tools, and prompts. It has a growing number of ùò¥ùò¶ùò≥ùò∑ùò¶ùò≥ùò¥ for connecting to various tools or data sources. Here, we show how to connect any MCP server to LangGraph agents & use MCP tools‚Ä¶</font></b>\n",
    "\n",
    "If you are like me, getting a prototype work, no matter how simple it is, brings an immense sense of clarity and understanding; at leat in my own mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a05772-a6db-47f2-8ea0-8ad493a157c8",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>What is the Model Context Protocol (MCP)?</font></b>\n",
    "([source](https://apidog.com/blog/fastmcp/)) <b><font size='3ptx'>Before diving deeper into FastMCP, it's essential to grasp the core concepts of MCP itself.</font></b>\n",
    "\n",
    "MCP defines a standardized way for LLM applications (clients) to interact with external systems (servers). An MCP server can expose several key components:\n",
    "* **Tools:** These are essentially functions that an LLM can request the server to execute. Think of them like POST endpoints in a traditional API. They perform actions, potentially interact with other systems (databases, APIs, hardware), and return results. For example, a tool could send an email, query a database, or perform a calculation.\n",
    "* **Resources:** These expose data that an LLM can read or retrieve. Similar to GET endpoints, resources provide information to enrich the LLM's context. This could be anything from configuration files and user profiles to real-time data streams.\n",
    "* **Prompts:** These are reusable templates for structuring interactions with the LLM. They help guide the conversation and ensure consistent outputs for specific tasks.\n",
    "* **Context:** Servers can provide contextual information, including instructions on how to best interact with the available tools and resources.\n",
    "\n",
    "<b>MCP aims to create a robust and secure ecosystem where LLMs can reliably access and utilize external capabilities</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b886659-56ff-45e6-8e24-15c67df485ca",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Why Choose FastMCP?</font></b>\n",
    "<b><font size='3ptx'>While you could implement the MCP specification directly using lower-level SDKs, FastMCP offers compelling advantages, particularly for Python developers:</font></b>\n",
    "* **üöÄ Fast Development:** Its high-level interface significantly reduces the amount of code needed, accelerating the development process. Often, defining a tool or resource is as simple as decorating a standard Python function.\n",
    "* **üçÄ Simplicity:** FastMCP abstracts away the complex details of server setup, protocol handling, content types, and error management, minimizing boilerplate.\n",
    "* **üêç Pythonic:** Designed with Python best practices in mind, it feels natural and intuitive for developers familiar with the language, leveraging features like type hints and decorators.\n",
    "* **üîç Complete:** FastMCP aims to provide a comprehensive implementation of the core MCP specification, ensuring compatibility and access to the protocol's full potential.\n",
    "\n",
    "FastMCP version 1 proved highly successful and is now integrated into the official MCP Python SDK. Version 2 builds upon this foundation, introducing advanced features focused on simplifying server interactions, such as flexible clients, server proxying, and composition patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ddfc48-20a8-4136-ba4f-aefc51960f13",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Virtual environment</font></b>\n",
    "It is good practice to create a virtual environment to install and run code; the command below creates the virtual environment called `MCP_Demo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c585c8-dc34-4a36-89d2-4d0d2dc2561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m venv MCP_Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb96d45-5d4a-45b2-8e9f-017caf3fbbe3",
   "metadata": {},
   "source": [
    "Then run this command to activate (enter) the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd2c4337-dc65-4dd7-85c2-523e2c025465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source MCP_Demo/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109cb776-d0db-4f12-ae07-9d1ac701054a",
   "metadata": {},
   "source": [
    "You will see your command prompt is updated with (`MCP_Demo`).\n",
    "\n",
    "Then run the following lines of code in sequence to install the required packages:\n",
    "```shell\n",
    "$ pip install langchain-mcp-adapters\n",
    "$ pip install langchain-mcp-adapters\n",
    "$ export GEMINI_API_KEY=<your_api_key>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1fe42a-49ac-4d0d-ae80-3676fcec97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-mcp-adapters==0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep 'langchain-mcp-adapters'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17d480-3ce3-4422-b568-ceb9a340e06b",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Server implementation</font></b>\n",
    "<b><font size='3ptx'>An LLM MCP (Model Control Plane) Server is a backend component that manages and routes requests to large language models.</font></b>\n",
    "\n",
    "The MCP server acts as a gateway between user applications and the model infrastructure, ensuring efficient, secure, and scalable access to LLM capabilities. It's especially useful in systems that serve multiple models or need fine-grained control over inference workflows.\n",
    "\n",
    "- **`math_server.py`**:\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# name or identifier of the LLM app or service\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "  \"\"\"Add two numbers\"\"\"\n",
    "  return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "  \"\"\"Multiply two numbers\"\"\"\n",
    "  return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # \"stdio\" tells the MCP server to read requests from standard input (stdin) and write responses to standard output (stdout).\n",
    "  mcp.run(transport=\"stdio\")\n",
    "```\n",
    "\n",
    "Run the server with the following command:\n",
    "```shell\n",
    "$ python math_server.py\n",
    "```\n",
    "\n",
    "While the MCP server is running in the one tab, let's open a new tab to develop MCP client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aa0be0-4a5e-497b-b3ea-ec55fa59c4d2",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Client</font></b>\n",
    "<b><font size='3ptx'>An LLM MCP Client is a component or library that communicates with an LLM MCP (Model Control Plane) Server to send prompts and receive model-generated responses.</font></b>\n",
    "\n",
    "It abstracts the details of the transport layer (e.g., stdio, http, or websocket) and provides a simple interface for applications to interact with large language models. \n",
    "\n",
    "- **`mcp_client.py`**:\n",
    "\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import asyncio\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "executing_path = Path(working_directory) / Path(\"math_server.py\")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[executing_path],\n",
    ")\n",
    "\n",
    "async def run_agent():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "\n",
    "            # Get tools\n",
    "            tools = await load_mcp_tools(session)\n",
    "\n",
    "            # Create and run the agent\n",
    "            agent = create_react_agent(model, tools)\n",
    "            agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "            return agent_response\n",
    "\n",
    "# Run the async function\n",
    "if __name__ == \"__main__\":\n",
    "    result = asyncio.run(run_agent())\n",
    "    for message in result['messages']:\n",
    "      print(message.pretty_print())\n",
    "      print('\\n')\n",
    "```\n",
    "\n",
    "From above sample code, it leverage function [**`langchain.agents.react.agent.create_react_agent`**](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to bind the tools with LLM mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c5f9010-ffe4-40c9-89af-b33ed8ceedf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/john/Gitrepos/ml_articles/medium/Using_Langchain_with_model-context-protocol_mcp/math_server.py\n"
     ]
    }
   ],
   "source": [
    "# Confirm we got correct execution path:\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "executing_path = Path(working_directory) / Path(\"math_server.py\")\n",
    "\n",
    "print(executing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3778726-c83d-4525-b2c9-8ebcba4d9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm our model API key setting is correct:\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0ef6c-c0ad-45b2-b1b4-542024d8d33d",
   "metadata": {},
   "source": [
    "Run the client with the command: `python mcp_client.py`\n",
    "\n",
    "The client will run once and end with the output below:\n",
    "```\n",
    "================================ Human Message =================================\n",
    "\n",
    "what's (3 + 5) x 12?\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  add (179d68f6-144c-4dcb-afd0-c8595deba9c6)\n",
    " Call ID: 179d68f6-144c-4dcb-afd0-c8595deba9c6\n",
    "  Args:\n",
    "    a: 3.0\n",
    "    b: 5.0\n",
    "None\n",
    "\n",
    "\n",
    "================================= Tool Message =================================\n",
    "Name: add\n",
    "\n",
    "8\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "Tool Calls:\n",
    "  multiply (0f6dfdda-3830-463f-9cfe-2b7c5f084a64)\n",
    " Call ID: 0f6dfdda-3830-463f-9cfe-2b7c5f084a64\n",
    "  Args:\n",
    "    a: 8.0\n",
    "    b: 12.0\n",
    "None\n",
    "\n",
    "\n",
    "================================= Tool Message =================================\n",
    "Name: multiply\n",
    "\n",
    "96\n",
    "None\n",
    "\n",
    "\n",
    "================================== Ai Message ==================================\n",
    "\n",
    "(3 + 5) x 12 is 96.\n",
    "None\n",
    "```\n",
    "\n",
    "MCP is a convenient way of integrating AI Agents with information and services supplying context and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f983c83e-7f89-4c4b-836f-f0e721f271bc",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [Gemini API quickstart](https://ai.google.dev/gemini-api/docs/quickstart)\n",
    "* [**LangChain doc:** ChatGoogleGenerativeAI](https://python.langchain.com/docs/integrations/chat/google_generative_ai/) \n",
    "* [A Beginner's Guide to Use FastMCP](https://apidog.com/blog/fastmcp/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
