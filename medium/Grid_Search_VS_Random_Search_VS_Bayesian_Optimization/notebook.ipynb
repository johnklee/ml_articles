{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708ba255",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([course link](https://towardsdatascience.com/grid-search-vs-random-search-vs-bayesian-optimization-2e68f57c3c46)) <b><font size='3ptx'>Which hyperparameter tuning method is best?</font></b> <b>Hyperparameter tuning is an essential step in developing robust predictive models. After all, sticking with default parameters prevents models from achieving peak performance.</b>\n",
    "\n",
    "This begs the question: what method is most fitting for finding the optimal hyperparameters for a given model?\n",
    "\n",
    "Here, we delve into 3 popular approaches for hyperparameter tuning and determine which one is superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4b901",
   "metadata": {},
   "source": [
    "<a id='sect0'></a>\n",
    "### <font color='darkgreen'>Agenda</font>\n",
    "* <b><font size='3ptx'><a href='#sect1'>Introduction of 3 popular approaches for hyperparameter tuning</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2'>Case Study</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect3'>Which method is the best?</a></font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c61dc",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>Introduction of 3 popular approaches for hyperparameter tuning</font>\n",
    "* <b><a href='#sect1_1'>Grid search</a></b>\n",
    "* <b><a href='#sect1_2'>Random search</a></b>\n",
    "* <b><a href='#sect1_3'>Bayesian Optimization</a></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b468f",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <font color='darkgreen'>Grid search</font>\n",
    "<b><font size='3ptx'>The grid search is the most common hyperparameter tuning approach given its simple and straightforward procedure</font>. It is an `uninformed search method`, which means that it does not learn from its previous iterations.</b>\n",
    "\n",
    "<b>Using this method entails testing every unique combination of hyperparameters in the search space to determine the combination that yields the best performance.</b> It’s easy to see the benefits of such a brute-force method; what better way to find the best solution than to try all of them out?\n",
    "\n",
    "Unfortunately, this approach does not scale well; <b><font color='darkred'>an increase in the size of the hyperparameter search space will result in an exponential rise in run time and computation</font></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad74a5d2",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <font color='darkgreen'>Random search</font>\n",
    "<b><font size='3ptx'>The random search is also an uninformed search method that treats iterations independently.</font></b> <b>However, instead of searching for all hyperparameter sets in the search space, it evaluates a specific number of hyperparameter sets at random. This number is determined by the user.</b>\n",
    "\n",
    "Since it performs fewer trials in hyperparameter tuning, <b>the method requires less computation and run time than the grid search.</b>\n",
    "\n",
    "Unfortunately, <b><font color='darkred'>since the random search tests hyperparameter sets at random, it runs the risk of missing the ideal set of hyperparameters and forgoing peak model performance</font></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a0491",
   "metadata": {},
   "source": [
    "<a id='sect1_3'></a>\n",
    "### <font color='darkgreen'>Bayesian Optimization</font>\n",
    "<b><font size='3ptx'>Unlike the grid search and random search, which treat hyperparameter sets independently, the Bayesian optimization is an informed search method, meaning that it learns from previous iterations</font>. The number of trials in this approach is determined by the user.</b>\n",
    "\n",
    "As the name suggests, the process is based on [**Bayes’ theorem**](https://en.wikipedia.org/wiki/Bayes%27_theorem):\n",
    "> $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "For this use case, the theorem can be modified to the following:\n",
    "> $P(Score|Hyperparameters) = \\frac{P(Hyperparameters|Score) P(Score)}{P(Hyperparameters)}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Simply put, <b>this method creates a probabilistic model, in which it maps hyperparameters to their corresponding score probability</b>.\n",
    "\n",
    "Instead of painstakingly trying every hyperparameter set or testing hyperparameter sets at random, <b><font color='darkgreen'>the Bayesian optimization method can converge to the optimal hyperparameters. Thus, the best hyperparameters can be obtained without exploring the entire sample space</font></b>.\n",
    "\n",
    "With the Bayesian optimization method, users do not have to endure long run times that come from evaluating every hyperparameter set. They also do not have to incorporate randomness and risk missing the optimal solution.\n",
    "\n",
    "That being said, Bayesian optimization does have its own drawback. Since this is an informed learning method, additional time is required to determine the next hyperparameters to evaluate based on the results of the previous iterations. <b><font color='darkred'>At the expense of minimizing the number of trials, Bayesian optimization requires more time for each iteration.</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe585d",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Case Study</font>  ([back](#sect0))\n",
    "* <b><a href='#sect2_1'>Grid Search</a></b>\n",
    "* <b><a href='#sect2_2'>Random Search</a></b>\n",
    "* <b><a href='#sect2_3'>Bayesian Optimization</a></b>\n",
    "* <b><a href='#sect2_4'>Comparison</a></b>\n",
    "<br/>\n",
    "\n",
    "<b><font size='3ptx'>We have explored the ins and outs of the three hyperparameter tuning approaches. To consolidate our understanding of these methods, it is best to use an example.</font></b>\n",
    "\n",
    "Let’s fine-tune a classification model with all three approaches and determine which one yields the best results.\n",
    "\n",
    "For the exercise, we will use the [load digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) from the Sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb0d5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c70e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f5d1d",
   "metadata": {},
   "source": [
    "The goal is to fine-tune a random forest model with the grid search, random search, and Bayesian optimization. Each method will be evaluated based on:\n",
    "* The total number of trials executed\n",
    "* The number of trials needed to yield the optimal hyperparameters\n",
    "* The score of the model (<font color='brown'>F-1 score in this case</font>)\n",
    "* The run time\n",
    "<br/>\n",
    "\n",
    "<b>The random forest classifier object and the search space</b> are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d331f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest classifier object\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# define sample space\n",
    "param_grid = {\n",
    "    'n_estimators': [100,150,200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [5, 6, 7]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecaa1ec",
   "metadata": {},
   "source": [
    "Altogether, there are 810 unique hyperparameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787a210",
   "metadata": {},
   "source": [
    "<a id='sect2_1'></a>\n",
    "### <font color='darkgreen'>Grid Search</font> ([back](#sect2))\n",
    "<b>First, let’s obtain the optimal hyperparameters using the grid search method and time the process. Of course, this means that <font size='3ptx'>we will test all 810 hyperparameter sets and pick out the one that yields the best results</font></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fac7bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.08 s, sys: 2.12 s, total: 10.2 s\n",
      "Wall time: 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create grid search object\n",
    "gs = GridSearchCV(estimator=rfc,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='f1_micro',\n",
    "                  cv=5,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=0)\n",
    "\n",
    "# perform hyperparameter tuning (while timing the process)\n",
    "time_start = time.time()\n",
    "gs.fit(X_train, y_train)\n",
    "time_grid = time.time() - time_start\n",
    "\n",
    "# store result in a data frame \n",
    "values_grid = [810, gs.best_index_+1, gs.best_score_, time_grid]\n",
    "columns = ['Number of iterations', 'Iteration Number of Optimal Hyperparamters', 'Score', 'Time Elapsed (s)']\n",
    "results_grid = pd.DataFrame([values_grid], columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04823c",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <font color='darkgreen'>Random Search</font> ([back](#sect2))\n",
    "Next, we will use the random search to identify the optimal hyperparameters and time the process. The search is limited to 100 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f26fb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 240 ms, total: 1.51 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a random search object\n",
    "rs = RandomizedSearchCV(estimator=rfc,\n",
    "                  param_distributions=param_grid,\n",
    "                  scoring='f1_micro',\n",
    "                  cv=5,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=0,\n",
    "                  n_iter=100)\n",
    "\n",
    "# perform hyperparamter tuning (while timing the process)\n",
    "time_start = time.time()\n",
    "rs.fit(X_train, y_train)\n",
    "time_random = time.time() - time_start\n",
    "\n",
    "# store result in a data frame \n",
    "values_grid = [[100, rs.best_index_+1, rs.best_score_, time_random]]\n",
    "results_random = pd.DataFrame(values_grid, columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb798f1",
   "metadata": {},
   "source": [
    "<a id='sect2_3'></a>\n",
    "### <font color='darkgreen'>Bayesian Optimization</font> ([back](#sect2))\n",
    "<b><font size='3ptx'>Finally, we perform hyperparameter tuning with the Bayesian optimization and time the process. In Python, this can be accomplished with the [Optuna module](https://optuna.org/)</font></b>.\n",
    "\n",
    "Its syntax differs from that of Sklearn, but it performs the same operation. For the sake of consistency, we will use 100 trials in this procedure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3778e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"return the f1-score\"\"\"\n",
    "\n",
    "    # search space\n",
    "    n_estimators =  trial.suggest_int('n_estimators', low=100, high=200, step=50)\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', low=2, high=4, step=1)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', low=1, high=5, step=1)\n",
    "    max_depth = trial.suggest_int('max_depth', low=5, high=7, step=1)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt','log2'])\n",
    "\n",
    "    # random forest classifier object\n",
    "    rfc = RandomForestClassifier(\n",
    "        n_estimators=n_estimators, \n",
    "        criterion=criterion,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_depth=max_depth,\n",
    "        max_features=max_features,\n",
    "        random_state=42)\n",
    "    \n",
    "    score =  cross_val_score(estimator=rfc, \n",
    "                             X=X_train, \n",
    "                             y=y_train, \n",
    "                             scoring='f1_micro',\n",
    "                             cv=5,\n",
    "                             n_jobs=-1).mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa94b4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.48 s, sys: 291 ms, total: 2.77 s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a study (aim to maximize score)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(\n",
    "    sampler=TPESampler(), direction='maximize')\n",
    "\n",
    "# perform hyperparamter tuning (while timing the process)\n",
    "time_start = time.time()\n",
    "study.optimize(objective, n_trials=100)\n",
    "time_bayesian = time.time() - time_start\n",
    "\n",
    "# store result in a data frame \n",
    "values_bayesian = [100, study.best_trial.number, study.best_trial.value, time_bayesian]\n",
    "results_bayesian = pd.DataFrame([values_bayesian], columns = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225903f9",
   "metadata": {},
   "source": [
    "<a id='sect2_4'></a>\n",
    "### <font color='darkgreen'>Comparison</font> ([back](#sect2))\n",
    "<b><font size='3ptx'>Now that we have executed hyperparameter tuning with all three approaches, let’s see how the results of each method compare to each other.</font></b>\n",
    "\n",
    "For convenience, we will store the results of all 3 hyperparameter tuning procedures in a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c4c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of iterations</th>\n",
       "      <th>Iteration Number of Optimal Hyperparamters</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time Elapsed (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Grid Search</th>\n",
       "      <td>810</td>\n",
       "      <td>680</td>\n",
       "      <td>0.935426</td>\n",
       "      <td>523.337782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Search</th>\n",
       "      <td>100</td>\n",
       "      <td>25</td>\n",
       "      <td>0.930965</td>\n",
       "      <td>61.371032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Optimization</th>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>0.935426</td>\n",
       "      <td>95.283444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Number of iterations  \\\n",
       "Grid Search                             810   \n",
       "Random Search                           100   \n",
       "Bayesian Optimization                   100   \n",
       "\n",
       "                       Iteration Number of Optimal Hyperparamters     Score  \\\n",
       "Grid Search                                                   680  0.935426   \n",
       "Random Search                                                  25  0.930965   \n",
       "Bayesian Optimization                                          27  0.935426   \n",
       "\n",
       "                       Time Elapsed (s)  \n",
       "Grid Search                  523.337782  \n",
       "Random Search                 61.371032  \n",
       "Bayesian Optimization         95.283444  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results_grid.append(results_random).append(results_bayesian)\n",
    "df.index = ['Grid Search', 'Random Search', 'Bayesian Optimization']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ca78b",
   "metadata": {},
   "source": [
    "<b>The grid search registered the highest score</b> (<font color='brown'>joint with the Bayesian optimization method</font>). However, the method required carrying out 810 trials and only managed to obtain the optimal hyperparameters at the 680th iteration. Also, <b><font color='darkred'>its run time far exceeded that of the random search and the Bayesian optimization methods</font></b>.\n",
    "\n",
    "<b>The random search method required only 100 trials and needed only 25 iterations to find the best hyperparameter set</b>. It also took the least amount of time to execute. However, <b><font color='darkred'>the random search method registered the lowest score out of the 3 methods</font></b>.\n",
    "\n",
    "<b>The Bayesian optimization also performed 100 trials but was able to achieve the highest score after only 27 iterations, far less than the grid search’s 680 iterations</b>. Although it executed the same number of trials as the random search, it has a longer run time since it is an informed search method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6ff60",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Which method is the best?</font> ([back](#sect0))\n",
    "<b><font size='3ptx'>Given that the grid search, random search, and Bayesian optimization all have their own trade-off between run time, the number of iterations, and performance, is it really possible to come to a consensus on which method is the best?</font></b>\n",
    "\n",
    "Probably not.\n",
    "\n",
    "After all, <b>the ideal hyperparameter tuning method depends on the use case</b>. Ask yourself:\n",
    "* What are the constraints of your machine learning task?\n",
    "* Does your project prioritize maximizing performance or minimizing run time and/or the number of iterations?\n",
    "\n",
    "<br/>\n",
    "\n",
    "Answering these questions will help decide the most suitable hyperparameter optimization approach. \n",
    "\n",
    "* **The grid search** is ideal if the computational demand and run-time are not limiting factors.\n",
    "* **The random search** is suitable if you’re willing to sacrifice performance in exchange for fewer iterations and smaller run time (<font color='brown'>Theoretically, the random search could find the best hyperparameters, but this is left entirely up to chance</font>).\n",
    "* **Bayesian optimization** is the best fit if you wish to obtain the optimal hyperparameters with fewer trials but are willing to have longer run times for each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef6e57",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>My 2 cents</font>\n",
    "<b><font size='3ptx'>If you hate diplomatic answers and just want my personal opinion, I would say that I usually favor the Bayesian optimization.</font></b>\n",
    "\n",
    "<b>Given the run time needed for fine-tuning models with larger training data sets and search spaces, I usually shun the grid search.</b> The random search requires fewer iterations and is the fastest of all 3 methods, but its level of success depends on the hyperparameter sets that are selected at random. In some cases, it will select the optimal hyperparameters; in other cases, it will omit the optimal hyperparameters completely. Due to this inconsistency, I do not like relying on randomness for bigger machine learning tasks.\n",
    "\n",
    "<b>I prefer the Bayesian optimization approach for its ability to consistently attain the optimal hyperparameters with fewer iterations.</b> Its individual iterations may take more time than those of the uninformed search methods, but that is rarely a deal-breaker for me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda75f7",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Conclusion</font>\n",
    "<b><font size='3ptx'>The main takeaway from this analysis is that each hyperparameter tuning method has its own unique trade-off between run time, number of iterations, and performance.</font></b>\n",
    "\n",
    "Ultimately, the best approach for you will depend on your priorities and constraints. Having a strong understanding of the data as well as the objectives will ensure that you make the correct decision.\n",
    "\n",
    "I wish you the best of luck in your data science endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
