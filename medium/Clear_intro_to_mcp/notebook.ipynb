{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2d2f72-fc11-43b1-947a-5dcd5a5ffa2a",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://towardsdatascience.com/clear-intro-to-mcp/)) <font size='3ptx'><b>MCP (Model Context Protocol) is a way to democratize access to tools for AI Agents. In this article we cover the fundamental components of MCP, how they work together, and a code example of how MCP works in practice.</b> As the race to move AI agents from prototype to production heats up, the need for a standardized way for agents to call tools across different providers is pressing. </font>\n",
    "\n",
    "This transition to a standardized approach to agent tool calling is similar to what we saw with REST APIs. <b>Before they existed, developers had to deal with a mess of proprietary protocols just to pull data from different services. REST brought order to chaos, enabling systems to talk to each other in a consistent way</b>.\n",
    "\n",
    "<b><font size='3ptx'>[MCP](https://modelcontextprotocol.io/introduction) (Model Context Protocol)</font> is aiming to, as it sounds, provide context for AI models in a standard way</b>. Without it, we’re headed towards tool-calling mayhem where multiple incompatible versions of “standardized” tool calls crop up simply because there’s no shared way for agents to organize, share, and invoke tools. MCP gives us a shared language and the democratization of tool calling.\n",
    "\n",
    "One thing I’m personally excited about is <b>how tool-calling standards like MCP can actually make [Ai Systems](https://towardsdatascience.com/tag/ai-systems/) safer. With easier access to well-tested tools more companies can avoid reinventing the wheel</b>, which reduces security risks and minimizes the chance of malicious code. As Ai systems start scaling in 2025, these are valid concerns.\n",
    "\n",
    "As I dove into MCP, I <b>realized a huge gap in documentation. There’s plenty of high-level “what does it do” content, but when you actually want to understand how it works</b>, the resources start to fall short—especially for those who aren’t native developers. It’s either high level explainers or deep in the source code.\n",
    "\n",
    "<b>In this piece, I’m going to break MCP down for a broader audience — making the concepts and functionality clear and digestible</b>. If you’re able, follow along in the coding section, if not it will be well explained in natural language above the code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e099c3b-dc96-4141-9cae-fb9191096d6c",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>An Analogy to Understand MCP: The Restaurant</font></b>\n",
    "Let’s imagine the concept of MCP as a restaurant where we have:\n",
    "* **The Host** = The restaurant building (the environment where the agent runs)\n",
    "* **The Server** = The kitchen (where tools live)\n",
    "* **The Client** = The waiter (who sends tool requests)\n",
    "* **The Agent** = The customer (who decides what tool to use)\n",
    "* **The Tools** = The recipes (the code that gets executed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f3faf-00c0-41f6-ad5b-a523a752ce5d",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>The Components of MCP</font></b>\n",
    "\n",
    "\n",
    "#### <b><font size='3ptx'>Host</font></b>\n",
    "This is where the agent operates. In our analogy, it’s the restaurant building; in MCP, it’s wherever your agents or LLMs actually run. If you’re using Ollama locally, you’re the host. If you’re using Claude or GPT, then Anthropic or OpenAI are the hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57c629-7550-4f88-b3e2-aca5a42a1d51",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Client</font></b>\n",
    "This is the environment that sends tool call requests from the agent. Think of it as the waiter who takes your order and delivers it to the kitchen. In practical terms, it’s the application or interface where your agent runs. The client passes tool call requests to the Server using MCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efdfad-8d27-437b-8aef-e9536aa0354b",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Server</font></b>\n",
    "This is the kitchen where recipes, or tools, are housed. It centralizes tools so agents can access them easily. Servers can be local (spun up by users) or remote (hosted by companies offering tools). Tools on a server are typically either grouped by function or integration. For instance, all Slack-related tools can be on a “Slack server,” or all messaging tools can be grouped together on a “messaging server”. That decision is based on architectural and developer preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93e3d7-4025-4f9e-97bb-90c9b924de26",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Agent</font></b>\n",
    "<b>The “brains” of the operation. Powered by an LLM, it decides which tools to call to complete a task</b>. When it determines a tool is needed, it initiates a request to the server. The agent doesn’t need to natively understand MCP because it learns how to use it through the metadata associated with each of the tools. This metadata associated with each tool tells the agent the protocol for calling the tool and the execution method. But it is important to note that the platform or agent needs to support MCP so that it handles tool calls automatically. Otherwise it is up to the developer to write the complex translation logic of how to parse the metadata from the schema, form tool call requests in MCP format, map the requests to the correct function, execute the code, and return the result in MCP complaint format back to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75d57c-84b2-48a1-afa6-09db9129b2f4",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Tools</font></b>\n",
    "These are the functions, such as calling APIs or custom code, that “does the work”. Tools live on servers and can be:>\n",
    "* Custom tools you create and host on a local server.\n",
    "* Premade tools hosted by others on a remote server.\n",
    "* Premade code created by others but hosted by you on a local server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e636d4b-84ee-433c-902f-5b78d6f375aa",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How the components fit together</font></b>\n",
    "\n",
    "1. <font size='3ptx'><b>Server Registers Tools</b></font>\n",
    "> Each tool is defined with a name, description, input/output schemas, a function handler (the code that runs) and registered to the server. This usually involves calling a method or API to tell the server “hey, here’s a new tool and this is how you use it”.\n",
    "\n",
    "2. <font size='3ptx'><b>Server Exposes Metadata</b></font>\n",
    "> When the server starts or an agent connects, it exposes the tool metadata (schemas, descriptions) via MCP.\n",
    "\n",
    "3. <font size='3ptx'><b>Agent Discovers Tools</b></font>\n",
    "> The agent queries the server (using MCP) to see what tools are available. It understands how to use each tool from the tool metadata. This typically happens on startup or when tools are added.\n",
    "\n",
    "4. <font size='3ptx'><b>Agent Plans Tool Use</b></font>\n",
    "> When the agent determines a tool is needed (based on user input or task context), it forms a tool call request in a standardized MCP JSON format which includes tool name, input parameters that match the tool’s input schema, and any other metadata. The client acts as the transport layer and sends the MCP formatted request to the server over HTTP.\n",
    "\n",
    "5. <font size='3ptx'><b>Translation Layer Executes</b></font>\n",
    "> The translation layer takes the agent’s standardized tool call (via MCP), maps the request to the corresponding function on the server, executes the function, formats the result back to MCP, and sends it back to the agent. A framework that abstracts MCP for you deos all of this without the developer needing to write the translation layer logic (which sounds like a headache).\n",
    "\n",
    "![procedure call](https://contributor.insightmediagroup.io/wp-content/uploads/2025/03/AD_4nXeeW11YxoD4PFmiDyq3U05WG_plNlrzy7IZUa1bcWQfG2_ECnwJ3xlGSFGMX94R_f0ZwmrZyqUBu4u-uIfvwiJD74XKtCu94FTE0vkfdxg1Ig_iZH3MPJOdL64qvctItpOxkCcA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43da41b-74b9-44e2-ba96-1d598a3de99d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Code Example of \"MCP Python SDK\"</font></b>\n",
    "<font size='3ptx'><b>The [Model Context Protocol](https://modelcontextprotocol.io/introduction) allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction</b></font>. [**This Python SDK**](https://github.com/modelcontextprotocol/python-sdk) implements the full MCP specification, making it easy to:\n",
    "* Build MCP clients that can connect to any MCP server\n",
    "* Create MCP servers that expose resources, prompts and tools\n",
    "* Use standard transports like stdio and SSE\n",
    "* Handle all MCP protocol messages and lifecycle events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914b9c6-441c-4ac8-97bd-8977dbd3a947",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Core Concepts</font></b>\n",
    "\n",
    "#### <b><font size='3ptx'>Server</font></b>\n",
    "The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fake_database import Database  # Replace with your actual DB type\n",
    "\n",
    "from mcp.server.fastmcp import Context, FastMCP\n",
    "\n",
    "# Create a named server\n",
    "mcp = FastMCP(\"My App\")\n",
    "\n",
    "# Specify dependencies for deployment and development\n",
    "mcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    db: Database\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n",
    "    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n",
    "    # Initialize on startup\n",
    "    db = await Database.connect()\n",
    "    try:\n",
    "        yield AppContext(db=db)\n",
    "    finally:\n",
    "        # Cleanup on shutdown\n",
    "        await db.disconnect()\n",
    "\n",
    "\n",
    "# Pass lifespan to server\n",
    "mcp = FastMCP(\"My App\", lifespan=app_lifespan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad397c-e076-4134-abaf-c9c4bec4d783",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Resources</font></b>\n",
    "Resources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data <b>but shouldn't perform significant computation or have side effects</b>:\n",
    "\n",
    "```python\n",
    "@mcp.resource(\"config://app\")\n",
    "def get_config() -> str:\n",
    "    \"\"\"Static configuration data\"\"\"\n",
    "    return \"App configuration here\"\n",
    "\n",
    "\n",
    "@mcp.resource(\"users://{user_id}/profile\")\n",
    "def get_user_profile(user_id: str) -> str:\n",
    "    \"\"\"Dynamic user data\"\"\"\n",
    "    return f\"Profile data for user {user_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a3031-7322-4297-ae0e-388d975a11d9",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Tools</font></b>\n",
    "Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "def calculate_bmi(weight_kg: float, height_m: float) -> float:\n",
    "    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n",
    "    return weight_kg / (height_m**2)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def fetch_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a city\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"https://api.weather.com/{city}\")\n",
    "        return response.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff6a1a-ccad-4073-9aa8-22b5b78b2922",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Prompts</font></b>\n",
    "Prompts are reusable templates that help LLMs interact with your server effectively:\n",
    "\n",
    "```python\n",
    "@mcp.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def debug_error(error: str) -> list[base.Message]:\n",
    "    return [\n",
    "        base.UserMessage(\"I'm seeing this error:\"),\n",
    "        base.UserMessage(error),\n",
    "        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98176b-e907-425f-8d8d-6de09a26f38c",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Images</font></b>\n",
    "FastMCP provides an <b><font color='blue'>Image</font></b> class that automatically handles image data:\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "def create_thumbnail(image_path: str) -> Image:\n",
    "    \"\"\"Create a thumbnail from an image\"\"\"\n",
    "    img = PILImage.open(image_path)\n",
    "    img.thumbnail((100, 100))\n",
    "    return Image(data=img.tobytes(), format=\"png\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e29d4-59f7-4299-aa69-de4d19c2499c",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Context</font></b>\n",
    "The Context object gives your tools and resources access to MCP capabilities:\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP, Context\n",
    "\n",
    "mcp = FastMCP(\"My App\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def long_task(files: list[str], ctx: Context) -> str:\n",
    "    \"\"\"Process multiple files with progress tracking\"\"\"\n",
    "    for i, file in enumerate(files):\n",
    "        ctx.info(f\"Processing {file}\")\n",
    "        await ctx.report_progress(i, len(files))\n",
    "        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n",
    "    return \"Processing complete\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebcefe-f65e-46ba-9f55-d46d23684798",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Installation</font></b>\n",
    "For projects using pip for dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d7505e-7029-4800-9086-b9ed74a42496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcp==1.6.0\n"
     ]
    }
   ],
   "source": [
    "# pip install mcp mcp[cli]\n",
    "!pip freeze | grep mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37fa52-573f-48b4-b5ca-f0102ab761af",
   "metadata": {},
   "source": [
    "<a id='quick_start_mcp_server_with_client'></a>\n",
    "### <b><font color='darkgreen'>Quickstart (MCP server & client)</font></b>\n",
    "Let's create a simple MCP server that exposes a calculator tool and some data:\n",
    "\n",
    "- `mcp_server.py`\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Create an MCP server\n",
    "mcp = FastMCP(\"Demo\")\n",
    "\n",
    "\n",
    "# Add an addition tool\n",
    "#### Tools ####\n",
    "# Add an addition tool\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "  \"\"\"Add two numbers\"\"\"\n",
    "  print(f\"Adding {a} and {b}\")\n",
    "  return a + b\n",
    "\n",
    "\n",
    "# Add a dynamic greeting resource\n",
    "@mcp.resource(\"greeting://{name}\")\n",
    "def get_greeting(name: str) -> str:\n",
    "  \"\"\"Get a personalized greeting\"\"\"\n",
    "  return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "  mcp.run(transport='sse')\n",
    "```\n",
    "\n",
    "For MCP server's transport (`sse` or `stdio`), you could refer to [here](https://modelcontextprotocol.io/docs/concepts/transports#built-in-transport-types) for details:\n",
    "> Transports in the Model Context Protocol (MCP) provide the foundation for communication between clients and servers. A transport handles the underlying mechanics of how messages are sent and received.\n",
    "\n",
    "The common two options as are:\n",
    "* **Standard Input/Output (stdio)**: The stdio transport enables communication through standard input and output streams. This is particularly useful for local integrations and command-line tools.\n",
    "  - Building command-line tools\n",
    "  - Implementing local integrations\n",
    "  - Needing simple process communication\n",
    "  - Working with shell scripts\n",
    "* **Server-Sent Events (SSE)**: SSE transport enables server-to-client streaming with HTTP POST requests for client-to-server communication.\n",
    "  - Only server-to-client streaming is needed\n",
    "  - Working with restricted networks\n",
    "  - Implementing simple updates\n",
    "\n",
    "Then We can run it by below command:\n",
    "```shell\n",
    "$ python mcp_server.py\n",
    "INFO:     Started server process [4100]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959ad10-c9ff-4086-b772-06d6d4178708",
   "metadata": {},
   "source": [
    "Next, we could prepare another MCP client to interact with our MCP server:\n",
    "\n",
    "* `mcp_client.py`\n",
    "```python\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "\n",
    "async def run():\n",
    "  async with sse_client(url=\"http://localhost:8000/sse\") as streams:\n",
    "    async with ClientSession(*streams) as session:\n",
    "      await session.initialize()\n",
    "\n",
    "      # List available tools\n",
    "      print('===== List available tools =====')\n",
    "      tools = await session.list_tools()\n",
    "      print(str(tools) + '\\n')\n",
    "\n",
    "      # Call a tool\n",
    "      print('===== Call a tool =====')\n",
    "      result = await session.call_tool(\"add\", arguments={\"a\": 4, \"b\": 5})\n",
    "      print(result.content[0].text + '\\n')\n",
    "\n",
    "      # List available resources\n",
    "      print('===== List resources =====')\n",
    "      resources = await session.list_resources()\n",
    "      print(\"resources\", resources)\n",
    "      print(\"\")\n",
    "\n",
    "      # Read a resource\n",
    "      print('===== Read a resource =====')\n",
    "      content = await session.read_resource(\"greeting://john\")\n",
    "      print(\"content\", content.contents[0].text)\n",
    "      print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  import asyncio\n",
    "\n",
    "  asyncio.run(run())\n",
    "```\n",
    "\n",
    "We could execute it as below:\n",
    "```shell\n",
    "$ ./mcp_client.py\n",
    "===== List available tools =====\n",
    "meta=None nextCursor=None tools=[Tool(name='add', description='Add two numbers', inputSchema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'addArguments', 'type': 'object'})]\n",
    "\n",
    "===== Call a tool =====\n",
    "9\n",
    "\n",
    "===== List resources =====\n",
    "resources meta=None nextCursor=None resources=[Resource(uri=AnyUrl('resource://some_static_resource'), name='resource://some_static_resource', description=None, mimeType='text/plain', size=None, annotations=None)]\n",
    "\n",
    "===== Read a resource =====\n",
    "content Hello, john!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bd90c-456f-4cc2-84a0-79c7555173d9",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LangChain + MCP</font></b>\n",
    "([YT:Using MCP with LangGraph agents](https://www.youtube.com/watch?v=OX89LkTvNKQ)) <font size='3ptx'><b>Anthropic's Model Context Protocol (MCP) is an open source protocol to connect LLMs with context, tools, and prompts</b>. It has a growing number of \"servers\" for connecting to various tools or data sources. Here, we show how to connect any MCP server to LangGraph agents, and use MCP tools. </font> ([Repo: langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters))\n",
    "\n",
    "![ui](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffb433-cfde-4128-8f03-b142ef4caceb",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Installation</font></b>\n",
    "The sample code below is coming from Github repo [**langchain-mcp-adapters**](https://github.com/langchain-ai/langchain-mcp-adapters), we have to install it first:\n",
    "> This library provides a lightweight wrapper that makes Anthropic [**Model Context Protocol**](https://modelcontextprotocol.io/introduction) (MCP) tools compatible with [**LangChain**](https://github.com/langchain-ai/langchain) and [**LangGraph**](https://github.com/langchain-ai/langgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d2624d-82d6-40b8-ae84-d5c242191fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-core==0.3.49\n",
      "langchain-mcp-adapters==0.0.5\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-mcp-adapters\n",
    "!pip freeze | grep -P 'langchain-mcp-adapters|langchain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc4edd-abb6-4667-8378-09501cfdc875",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Server</font></b>\n",
    "Here we implement an simple the MCP server **`math_mcp_server.py`** to handle math related problem:\n",
    "- `math_mcp_server.py`:\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db31d9-6625-42a0-ae60-2c98ff1f303a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Client</font></b>\n",
    "To use the MCP client below using OpenAI LLM, you have to apply/generate an OpenAPI key first [here](https://platform.openai.com/api-keys) and use environment variable `OPENAI_API_KEY` to store it for reference. Below MCP client will use OpenAI to answer a simple math question `what's (3 + 5) x 12?` by leveraging tools in our MCP server:\n",
    "\n",
    "- **`mcp_client_with_math_problem.py`**:\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"/path/to/math_mcp_server.py\"],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "  async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "      # Initialize the connection\n",
    "      await session.initialize()\n",
    "\n",
    "      # Get tools\n",
    "      tools = await load_mcp_tools(session)\n",
    "\n",
    "      # Create and run the agent\n",
    "      agent = create_react_agent(model, tools)\n",
    "      agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "      print('Agent response:')\n",
    "      for message in agent_response['messages']:\n",
    "        print(f'{message.__class__.__name__}: {message}')\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "From the code, it will leverage function [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to create an ReAct agent:\n",
    "> The **LLM ReAct (Reasoning + Acting) framework is an approach that combines Large Language Models (LLMs) with reasoning and action-taking capabilities**. This allows AI agents to think, decide, and execute tasks autonomously rather than just generating static responses.\n",
    "\n",
    "\n",
    "Then below is the execution result:\n",
    "```shell\n",
    "$ ./mcp_client_with_math_problem.py\n",
    "Agent response:\n",
    "HumanMessage: content=\"what's (3 + 5) x 12?\" additional_kwargs={} response_metadata={} id='e358bf94-ce8f-4f82-92be-915806b9275d'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_AFOWozah4ZwBchA50hypk5ke', 'function': {'arguments': '{\"a\": 3, \"b\": 5}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_nqxKzHmYvCZugl46rvfXTWx4', 'function': {'arguments': '{\"a\": 8, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 77, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGhpH5pncUiSVfWA73PXyFigjNz2v', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-0bf5927c-9005-4e96-abd0-5e191e457111-0' tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_AFOWozah4ZwBchA50hypk5ke', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_nqxKzHmYvCZugl46rvfXTWx4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 77, 'output_tokens': 51, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='8' name='add' id='4bbdcbcc-6ebd-47fb-9407-cca308a9bf1f' tool_call_id='call_AFOWozah4ZwBchA50hypk5ke'\n",
    "\n",
    "ToolMessage: content='96' name='multiply' id='aca35ceb-717a-48dd-918e-29450ef47308' tool_call_id='call_nqxKzHmYvCZugl46rvfXTWx4'\n",
    "\n",
    "AIMessage: content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 143, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGhpJRbfmopZOraLa2wqbf4nAi4AV', 'finish_reason': 'stop', 'logprobs': None} id='run-ae7bed58-8147-49a6-ae06-b180a5cb137f-0' usage_metadata={'input_tokens': 143, 'output_tokens': 22, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aff32-7e21-4e60-ade6-2158877e75f8",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Multiple MCP Servers</font></b>\n",
    "The [**library**](https://github.com/langchain-ai/langchain-mcp-adapters?tab=readme-ov-file#multiple-mcp-servers) also allows you to connect to multiple MCP servers and load tools from them. Here we prepared a second MCP server to hanlde weather related questions:\n",
    "* `weather_mcp_server.py`\n",
    "```python\n",
    "from typing import List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "\n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_weather(location: str) -> str:\n",
    "  \"\"\"Get weather for location.\"\"\"\n",
    "  return \"It's always sunny in Taiwan\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  mcp.run(transport=\"sse\")\n",
    "```\n",
    "\n",
    "Let's start the Weather MCP server by executing it:\n",
    "```shell\n",
    "$ python weather_mcp_server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663c9e7-2b37-48bb-9772-c27e070ceada",
   "metadata": {},
   "source": [
    "Let's create another MCP client to include both Math and Weather MCP servers:\n",
    "- `mcp_client_with_both_math_and_weather_problem.py`:\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "import asyncio\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "MATH_MCP_SERVER_MODULE_PATH = (\n",
    "    \"/home/john/Gitrepos/ml_articles/medium/Clear_intro_to_mcp/math_mcp_server.py\")\n",
    "\n",
    "async def main():\n",
    "  async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [MATH_MCP_SERVER_MODULE_PATH],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    "  ) as client:\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    for question in [\n",
    "          \"what's (3 + 5) x 12?\",\n",
    "          \"what is the weather in Taiwan?\",\n",
    "      ]:\n",
    "      response = await agent.ainvoke({\"messages\": question})\n",
    "      print(f'===== Q: {question} =====')\n",
    "      for message in response['messages']:\n",
    "        print(f'{message.__class__.__name__}: {message}')\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d076ce3-40d8-46c9-9186-0e10319f67aa",
   "metadata": {},
   "source": [
    "Finally, let's execute it and see how it goes:\n",
    "```shell\n",
    "$ ./mcp_client_with_both_math_and_weather_problem.py\n",
    "===== Q: what's (3 + 5) x 12? =====\n",
    "HumanMessage: content=\"what's (3 + 5) x 12?\" additional_kwargs={} response_metadata={} id='d5e18352-36af-435b-8316-f46adbba7f4b'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_T3qkWNjmZTknweaeHKjYXfoL', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 96, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2MVKfDNX3FSjkor5uGKQHq49eL', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-93181dcd-b5df-4a16-a315-48e2ea9ae304-0' tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_T3qkWNjmZTknweaeHKjYXfoL', 'type': 'tool_call'}] usage_metadata={'input_tokens': 96, 'output_tokens': 18, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='8' name='add' id='64385ecf-b886-4e80-8052-4fbcfc6a27aa' tool_call_id='call_T3qkWNjmZTknweaeHKjYXfoL'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_kb2gEqwQ0ZFGj2fV10GZcSVs', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 121, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2NZETDbejIQ2YllWwIGUnIKFbW', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-79caf813-7905-42f3-85d7-9aefddd21be9-0' tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_kb2gEqwQ0ZFGj2fV10GZcSVs', 'type': 'tool_call'}] usage_metadata={'input_tokens': 121, 'output_tokens': 18, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='96' name='multiply' id='7f189bef-879b-4d8e-ba3c-8944cac24e3c' tool_call_id='call_kb2gEqwQ0ZFGj2fV10GZcSVs'\n",
    "\n",
    "AIMessage: content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 146, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2OSnR9WPntdosKaSwbASZiyMt6', 'finish_reason': 'stop', 'logprobs': None} id='run-2e0b564d-7f8f-4502-9d35-8e31fcdff1fc-0' usage_metadata={'input_tokens': 146, 'output_tokens': 22, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "===== Q: what is the weather in Taiwan? =====\n",
    "HumanMessage: content='what is the weather in Taiwan?' additional_kwargs={} response_metadata={} id='5d8b5330-7fae-414b-bb4b-80c9ce090ec2'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_GW8L6DqAbe5PcmxTFYbIzpMZ', 'function': {'arguments': '{\"location\":\"Taiwan\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 91, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2PwLjt9flSEsoW4aAt4HyGcEE0', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f4016f0f-9bdc-47b5-8f84-f53c15a8eb63-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Taiwan'}, 'id': 'call_GW8L6DqAbe5PcmxTFYbIzpMZ', 'type': 'tool_call'}] usage_metadata={'input_tokens': 91, 'output_tokens': 16, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content=\"It's always sunny in Taiwan\" name='get_weather' id='f0451b5a-149a-48d8-adb6-1aba43662155' tool_call_id='call_GW8L6DqAbe5PcmxTFYbIzpMZ'\n",
    "\n",
    "AIMessage: content='The weather in Taiwan is sunny.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 119, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2R63RTKtSZYXue7PmZR4U7XKOi', 'finish_reason': 'stop', 'logprobs': None} id='run-4c69bfeb-558b-46f3-8cf9-462fb7133601-0' usage_metadata={'input_tokens': 119, 'output_tokens': 9, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6edd3a-e497-4ed0-8dca-877a55c5b8fa",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Google Gemini 2.0 with MCP (Model Context Protocol) Servers</font></b>\n",
    "([source](https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-mcp-example.ipynb)) <font size='3ptx'><b>Gemini models can be used with MCP server using its native tool calling capabilities. MCP, or Model Context Protocol, is an open standard introduced by Anthropic designed to standardize how AI models like Gemini interact with external tools and data sources.</b> Instead of requiring custom integrations for each tool, MCP provides a structured way for models to access context, such as functions (tools), data sources (resources), or pre-defined prompts. <b>This allows AI agents to securely and efficiently connect with real-world systems and workflows</b>.</font>\n",
    "\n",
    "<b>MCP server expose their tools via JSON schema definitions, which can be converted to Gemini compatible OpenAPI schema definitions</b>. This allows you to easily use MCP server with Gemini models, below you will example on how to implement this.\n",
    "\n",
    "You can learn more about Google Search integration with Gemini here:\n",
    "* [**Gemini API doc - Function Calling with the Gemini API**](https://ai.google.dev/gemini-api/docs/function-calling?lang=python&example=weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b3820a-f10d-4ad0-b6a2-a4fcd6d68a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-genai==1.2.0\n",
      "mcp==1.6.0\n"
     ]
    }
   ],
   "source": [
    "# install Google GenAI and MCP\n",
    "#%pip install google-genai mcp \n",
    "!pip freeze | grep -P '(google-genai|mcp)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdfc49-d191-478b-afaf-9b28eb45a5a1",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Simple Example on how to use MCP with Gemini's tool calling</font></b>\n",
    "MCPs can be used with Google DeepMind Gemini by converting the MCP tools into Gemini compatible tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663c528a-100d-4361-b678-2620077c5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "\n",
    "os.environ['ALLOW_RESET'] = 'TRUE'\n",
    "_ = load_dotenv(find_dotenv(os.path.expanduser('~/.env')))\n",
    "\n",
    "client = genai.Client(\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")  # Replace with your actual API key setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e48c37-26d2-495a-b5a8-e4f3014ef450",
   "metadata": {},
   "source": [
    "**<font color='blue'>StdioServerParameters</font>** configures how to start and communicate with the MCP server:\n",
    "* **`command=\"npx\"`**: Specifies that the server will be run using the npx package runner.\n",
    "* **`args`**: A list of command-line arguments passed to npx:\n",
    "  - `\"-y\"`: Automatically confirms any prompts during the execution of the npx command.\n",
    "  - `\"@openbnb/mcp-server-airbnb\"`: This is the npm package name for an Airbnb-specific MCP server. This server likely provides tools related to booking accommodations.\n",
    "  - `\"--ignore-robots-txt\"`: An optional argument that might be specific to the Airbnb MCP server.\n",
    "* **`env=None`**: Specifies that no additional environment variables should be set for the server process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9116d8c-027a-4e42-b9e4-813d2ed70790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"@openbnb/mcp-server-airbnb\",\n",
    "        \"--ignore-robots-txt\",\n",
    "    ],  # Optional command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33822fbc-e8b3-4f3d-9ab9-b016b9631732",
   "metadata": {},
   "source": [
    "* `async with stdio_client(server_params) as (read, write):`: This establishes a connection to the MCP server. stdio_client starts the server process defined by server_params and provides asynchronous read and write streams for communication.\n",
    "* `async with ClientSession(read, write) as session:`: This creates an MCP client session using the established communication streams. The <b><font color='blue'>ClientSession</font></b> object allows you to interact with the MCP server.\n",
    "* `prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"`: This is the user's query that will be sent to the Gemini model.\n",
    "* `await session.initialize()`: Initializes the MCP session, allowing the client to understand the server's capabilities.\n",
    "* `mcp_tools = await session.list_tools()`: This retrieves a list of tools offered by the connected MCP server (<font color='brown'>in this case, the Airbnb server</font>).\n",
    "* `tools = types.Tool(function_declarations=[...])`: This is the crucial step where MCP tools are converted into Gemini-compatible Tool objects.\n",
    "  - It iterates through the `mcp_tools.tools` list.\n",
    "  - For each MCP tool, it creates a function declaration dictionary with the following keys:\n",
    "      - `\"name\"`: The name of the MCP tool.\n",
    "      - `\"description\"`: The description of the MCP tool.\n",
    "      - `\"parameters\"`: The input schema of the MCP tool, defining the expected arguments.\n",
    "  - These function declaration dictionaries are then used to create a `types.Tool` object, which is the format Gemini expects for function calling.\n",
    "* `response = client.models.generate_content(...)`: This sends a request to the Gemini model:\n",
    "  - `model=\"gemini-2.0-flash\"`: Specifies the Gemini model to use (choose one that supports function calling).\n",
    "  - `contents=prompt`: The user's query.\n",
    "  - `config=types.GenerateContentConfig(tools=[tools])`: Configures the generation request to include the converted MCP tools in the tools parameter. This tells Gemini about the available functions it can call.\n",
    "* **Checking for a Function Call:**\n",
    "  - The code then checks if the Gemini response contains a function call. If it does, it extracts the function name and arguments.\n",
    "  - `# In a real app, you would call your function here:`: This is a placeholder indicating where you would implement the logic to actually execute the MCP tool based on the function call received from Gemini. This would likely involve using await session.call_tool(function_call.args, arguments=function_call.args).\n",
    "  - If no function call is found, the code prints the raw text response from Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241d9664-7fdb-4ad8-8483-7c324ff2600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to call: airbnb_search\n",
      "Arguments: {'location': 'Paris', 'checkin': '2024-03-28', 'checkout': '2024-03-30'}\n"
     ]
    }
   ],
   "source": [
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(\n",
    "            read,\n",
    "            write,\n",
    "        ) as session:\n",
    "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get tools from MCP session and convert to Gemini Tool objects\n",
    "            mcp_tools = await session.list_tools()\n",
    "            tools = types.Tool(function_declarations=[\n",
    "                {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": tool.inputSchema,\n",
    "                }\n",
    "                for tool in mcp_tools.tools\n",
    "            ])\n",
    "            \n",
    "            # Send request with function declarations\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",  # Or your preferred model supporting function calling\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.7,\n",
    "                    tools=[tools],\n",
    "                ),  # Example other config\n",
    "            )\n",
    "        # Check for a function call\n",
    "        if response.candidates[0].content.parts[0].function_call:\n",
    "            function_call = response.candidates[0].content.parts[0].function_call\n",
    "            print(f\"Function to call: {function_call.name}\")\n",
    "            print(f\"Arguments: {function_call.args}\")\n",
    "            # In a real app, you would call your function here:\n",
    "            # result = await session.call_tool(function_call.args, arguments=function_call.args)\n",
    "            # sent new request with function call\n",
    "        else:\n",
    "            print(\"No function call found in the response.\")\n",
    "            print(response.text)\n",
    "            \n",
    "await run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dbb9d-e969-48c3-9b6d-51ec65e16046",
   "metadata": {},
   "source": [
    "#### <b>How it Works: Bridging the Gap</b>\n",
    "The core of this process lies in the conversion of MCP tool definitions to Gemini's function declaration format. Here's a breakdown of how this enables the integration:\n",
    "1. **MCP Tool Discovery**: The mcp-server-airbnb (or any other MCP server) exposes a set of tools with defined names, descriptions, and input schemas (parameters).\n",
    "2. **Fetching Tool Definitions**: The Python code uses the MCP client (<b><font color='blue'>ClientSession</font></b>) to connect to the MCP server and retrieve the metadata of these available tools using <font color='blue'>session.list_tools()</font>.\n",
    "3. **Schema Conversion**: The crucial part is the list comprehension that iterates through the MCP tools and extracts their `name`, `description`, and `inputSchema`. These are then structured into a dictionary format that aligns with Gemini's `function_declarations`.\n",
    "4. **Providing Tools to Gemini**: The <b><font color='blue'>types.Tool</font></b> object, containing the converted function declarations, is included in the tools parameter of the generate_content request to Gemini.\n",
    "5. **Gemini's Function Calling**: When Gemini processes the user's prompt along with the provided tools, it can recognize if any of the tools are relevant to fulfill the user's request. If so, it will generate a function call in its response, specifying the name of the function to call and the arguments it believes are necessary based on the tool's input schema.\n",
    "6. **Executing the MCP Tool** (Implementation Required): The commented-out section await session.call_tool(...) highlights the next step in a real application. After receiving a function call from Gemini, your application would use the MCP client to actually execute the corresponding MCP tool with the provided arguments. The result of this execution could then be sent back to Gemini to generate a final response to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f48318-7b58-4196-8a9c-d8f65fe076a3",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Full Agentic example with Gemini and Airbnb MCP</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62507b81-f658-4c5e-912f-d85d615d9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv(os.path.expanduser('~/.env')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd2addf5-4158-4b6e-98fb-070cfc37bb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running agent loop with prompt: I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\n",
      "Attempting to call MCP tool: 'airbnb_search' with args: {'checkin': '2024-03-28', 'location': 'Paris', 'checkout': '2024-03-30'}\n",
      "MCP tool 'airbnb_search' executed successfully.\n",
      "Added 1 tool response parts to history.\n",
      "Making subsequent API call with tool responses...\n",
      "MCP tool calling loop finished. Returning final response.\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "model = \"gemini-2.0-flash\"\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"@openbnb/mcp-server-airbnb\",\n",
    "        \"--ignore-robots-txt\",\n",
    "    ],  # Optional command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):\n",
    "    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    \n",
    "    # --- 1. Get Tools from Session and convert to Gemini Tool objects ---\n",
    "    mcp_tools = await session.list_tools()\n",
    "    tools = types.Tool(function_declarations=[\n",
    "        {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": tool.inputSchema,\n",
    "        }\n",
    "        for tool in mcp_tools.tools\n",
    "    ])\n",
    "    \n",
    "    # --- 2. Initial Request with user prompt and function declarations ---\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=model,  # Or your preferred model supporting function calling\n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            tools=[tools],\n",
    "        ),  # Example other config\n",
    "    )\n",
    "    \n",
    "    # --- 3. Append initial response to contents ---\n",
    "    contents.append(response.candidates[0].content)\n",
    "\n",
    "    # --- 4. Tool Calling Loop ---            \n",
    "    turn_count = 0\n",
    "    max_tool_turns = 5\n",
    "    while response.function_calls and turn_count < max_tool_turns:\n",
    "        turn_count += 1\n",
    "        tool_response_parts: List[types.Part] = []\n",
    "\n",
    "        # --- 4.1 Process all function calls in order and return in this turn ---\n",
    "        for fc_part in response.function_calls:\n",
    "            tool_name = fc_part.name\n",
    "            args = fc_part.args or {}  # Ensure args is a dict\n",
    "            print(f\"Attempting to call MCP tool: '{tool_name}' with args: {args}\")\n",
    "\n",
    "            tool_response: dict\n",
    "            try:\n",
    "                # Call the session's tool executor\n",
    "                tool_result = await session.call_tool(tool_name, args)\n",
    "                print(f\"MCP tool '{tool_name}' executed successfully.\")\n",
    "                if tool_result.isError:\n",
    "                    tool_response = {\"error\": tool_result.content[0].text}\n",
    "                else:\n",
    "                    tool_response = {\"result\": tool_result.content[0].text}\n",
    "            except Exception as e:\n",
    "                tool_response = {\"error\":  f\"Tool execution failed: {type(e).__name__}: {e}\"}\n",
    "            \n",
    "            # Prepare FunctionResponse Part\n",
    "            tool_response_parts.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=tool_name, response=tool_response\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # --- 4.2 Add the tool response(s) to history ---\n",
    "        contents.append(types.Content(role=\"user\", parts=tool_response_parts))\n",
    "        print(f\"Added {len(tool_response_parts)} tool response parts to history.\")\n",
    "\n",
    "        # --- 4.3 Make the next call to the model with updated history ---\n",
    "        print(\"Making subsequent API call with tool responses...\")\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,  # Send updated history\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=1.0,\n",
    "                tools=[tools],\n",
    "            ),  # Keep sending same config\n",
    "        )\n",
    "        contents.append(response.candidates[0].content)\n",
    "\n",
    "    if turn_count >= max_tool_turns and response.function_calls:\n",
    "        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop.\")\n",
    "\n",
    "    print(\"MCP tool calling loop finished. Returning final response.\")\n",
    "    # --- 5. Return Final Response ---\n",
    "    return response\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(\n",
    "            read,\n",
    "            write,\n",
    "        ) as session:\n",
    "            # Test prompt\n",
    "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
    "            print(f\"Running agent loop with prompt: {prompt}\")\n",
    "            # Run agent loop\n",
    "            res = await agent_loop(prompt, client, session)\n",
    "            return res\n",
    "\n",
    "res = await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d2b46ab-090e-4074-bc82-3988d24bc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. I found some apartments in Paris for the dates you provided. Here are some of the top listings:\n",
      "\n",
      "* Guest house in Champs-Élysées: https://www.airbnb.com/rooms/588302148367299476\n",
      "* Flat in Opéra: https://www.airbnb.com/rooms/827523433116382506\n",
      "* Flat in Palais-Royal: https://www.airbnb.com/rooms/1178861381392100805\n",
      "* Flat in Les Halles: https://www.airbnb.com/rooms/17186194\n",
      "* Room in Grenelle: https://www.airbnb.com/rooms/32705269\n",
      "\n",
      "I have a lot more options for you. Do any of these look interesting, or would you like me to continue searching?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263ce91-bc43-41b6-9279-7490abe480b6",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [MCP Introduction: Get started with the Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\n",
    "* [Dev - How to build MCP Servers and Clients from Scratch](https://dev.to/composiodev/how-to-build-mcp-servers-and-clients-from-scratch-4o2f)\n",
    "* [Gitrepo - Learn-With-Yash-Agrawal/Projects\n",
    "/MCP_Server_and_Client](https://github.com/oppasource/Learn-With-Yash-Agrawal/tree/main/Projects/MCP_Server_and_Client) ([YT](https://www.youtube.com/watch?v=-WogqfxWBbM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
