{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2d2f72-fc11-43b1-947a-5dcd5a5ffa2a",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Preface</font></b>\n",
    "([source](https://towardsdatascience.com/clear-intro-to-mcp/)) <font size='3ptx'><b>MCP (Model Context Protocol) is a way to democratize access to tools for AI Agents. In this article we cover the fundamental components of MCP, how they work together, and a code example of how MCP works in practice.</b> As the race to move AI agents from prototype to production heats up, the need for a standardized way for agents to call tools across different providers is pressing. </font>\n",
    "\n",
    "This transition to a standardized approach to agent tool calling is similar to what we saw with REST APIs. <b>Before they existed, developers had to deal with a mess of proprietary protocols just to pull data from different services. REST brought order to chaos, enabling systems to talk to each other in a consistent way</b>.\n",
    "\n",
    "<b><font size='3ptx'>[MCP](https://modelcontextprotocol.io/introduction) (Model Context Protocol)</font> is aiming to, as it sounds, provide context for AI models in a standard way</b>. Without it, we’re headed towards tool-calling mayhem where multiple incompatible versions of “standardized” tool calls crop up simply because there’s no shared way for agents to organize, share, and invoke tools. MCP gives us a shared language and the democratization of tool calling.\n",
    "\n",
    "One thing I’m personally excited about is <b>how tool-calling standards like MCP can actually make [Ai Systems](https://towardsdatascience.com/tag/ai-systems/) safer. With easier access to well-tested tools more companies can avoid reinventing the wheel</b>, which reduces security risks and minimizes the chance of malicious code. As Ai systems start scaling in 2025, these are valid concerns.\n",
    "\n",
    "As I dove into MCP, I <b>realized a huge gap in documentation. There’s plenty of high-level “what does it do” content, but when you actually want to understand how it works</b>, the resources start to fall short—especially for those who aren’t native developers. It’s either high level explainers or deep in the source code.\n",
    "\n",
    "<b>In this piece, I’m going to break MCP down for a broader audience — making the concepts and functionality clear and digestible</b>. If you’re able, follow along in the coding section, if not it will be well explained in natural language above the code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e099c3b-dc96-4141-9cae-fb9191096d6c",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>An Analogy to Understand MCP: The Restaurant</font></b>\n",
    "Let’s imagine the concept of MCP as a restaurant where we have:\n",
    "* **The Host** = The restaurant building (the environment where the agent runs)\n",
    "* **The Server** = The kitchen (where tools live)\n",
    "* **The Client** = The waiter (who sends tool requests)\n",
    "* **The Agent** = The customer (who decides what tool to use)\n",
    "* **The Tools** = The recipes (the code that gets executed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f3faf-00c0-41f6-ad5b-a523a752ce5d",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>The Components of MCP</font></b>\n",
    "\n",
    "\n",
    "#### <b><font size='3ptx'>Host</font></b>\n",
    "This is where the agent operates. In our analogy, it’s the restaurant building; in MCP, it’s wherever your agents or LLMs actually run. If you’re using Ollama locally, you’re the host. If you’re using Claude or GPT, then Anthropic or OpenAI are the hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f57c629-7550-4f88-b3e2-aca5a42a1d51",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Client</font></b>\n",
    "This is the environment that sends tool call requests from the agent. Think of it as the waiter who takes your order and delivers it to the kitchen. In practical terms, it’s the application or interface where your agent runs. The client passes tool call requests to the Server using MCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efdfad-8d27-437b-8aef-e9536aa0354b",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Server</font></b>\n",
    "This is the kitchen where recipes, or tools, are housed. It centralizes tools so agents can access them easily. Servers can be local (spun up by users) or remote (hosted by companies offering tools). Tools on a server are typically either grouped by function or integration. For instance, all Slack-related tools can be on a “Slack server,” or all messaging tools can be grouped together on a “messaging server”. That decision is based on architectural and developer preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93e3d7-4025-4f9e-97bb-90c9b924de26",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Agent</font></b>\n",
    "<b>The “brains” of the operation. Powered by an LLM, it decides which tools to call to complete a task</b>. When it determines a tool is needed, it initiates a request to the server. The agent doesn’t need to natively understand MCP because it learns how to use it through the metadata associated with each of the tools. This metadata associated with each tool tells the agent the protocol for calling the tool and the execution method. But it is important to note that the platform or agent needs to support MCP so that it handles tool calls automatically. Otherwise it is up to the developer to write the complex translation logic of how to parse the metadata from the schema, form tool call requests in MCP format, map the requests to the correct function, execute the code, and return the result in MCP complaint format back to the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75d57c-84b2-48a1-afa6-09db9129b2f4",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Tools</font></b>\n",
    "These are the functions, such as calling APIs or custom code, that “does the work”. Tools live on servers and can be:>\n",
    "* Custom tools you create and host on a local server.\n",
    "* Premade tools hosted by others on a remote server.\n",
    "* Premade code created by others but hosted by you on a local server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e636d4b-84ee-433c-902f-5b78d6f375aa",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>How the components fit together</font></b>\n",
    "\n",
    "1. <font size='3ptx'><b>Server Registers Tools</b></font>\n",
    "> Each tool is defined with a name, description, input/output schemas, a function handler (the code that runs) and registered to the server. This usually involves calling a method or API to tell the server “hey, here’s a new tool and this is how you use it”.\n",
    "\n",
    "2. <font size='3ptx'><b>Server Exposes Metadata</b></font>\n",
    "> When the server starts or an agent connects, it exposes the tool metadata (schemas, descriptions) via MCP.\n",
    "\n",
    "3. <font size='3ptx'><b>Agent Discovers Tools</b></font>\n",
    "> The agent queries the server (using MCP) to see what tools are available. It understands how to use each tool from the tool metadata. This typically happens on startup or when tools are added.\n",
    "\n",
    "4. <font size='3ptx'><b>Agent Plans Tool Use</b></font>\n",
    "> When the agent determines a tool is needed (based on user input or task context), it forms a tool call request in a standardized MCP JSON format which includes tool name, input parameters that match the tool’s input schema, and any other metadata. The client acts as the transport layer and sends the MCP formatted request to the server over HTTP.\n",
    "\n",
    "5. <font size='3ptx'><b>Translation Layer Executes</b></font>\n",
    "> The translation layer takes the agent’s standardized tool call (via MCP), maps the request to the corresponding function on the server, executes the function, formats the result back to MCP, and sends it back to the agent. A framework that abstracts MCP for you deos all of this without the developer needing to write the translation layer logic (which sounds like a headache).\n",
    "\n",
    "![procedure call](https://contributor.insightmediagroup.io/wp-content/uploads/2025/03/AD_4nXeeW11YxoD4PFmiDyq3U05WG_plNlrzy7IZUa1bcWQfG2_ECnwJ3xlGSFGMX94R_f0ZwmrZyqUBu4u-uIfvwiJD74XKtCu94FTE0vkfdxg1Ig_iZH3MPJOdL64qvctItpOxkCcA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43da41b-74b9-44e2-ba96-1d598a3de99d",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Code Example of \"MCP Python SDK\"</font></b>\n",
    "<font size='3ptx'><b>The [Model Context Protocol](https://modelcontextprotocol.io/introduction) allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction</b></font>. [**This Python SDK**](https://github.com/modelcontextprotocol/python-sdk) implements the full MCP specification, making it easy to:\n",
    "* Build MCP clients that can connect to any MCP server\n",
    "* Create MCP servers that expose resources, prompts and tools\n",
    "* Use standard transports like stdio and SSE\n",
    "* Handle all MCP protocol messages and lifecycle events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914b9c6-441c-4ac8-97bd-8977dbd3a947",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Core Concepts</font></b>\n",
    "\n",
    "#### <b><font size='3ptx'>Server</font></b>\n",
    "The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fake_database import Database  # Replace with your actual DB type\n",
    "\n",
    "from mcp.server.fastmcp import Context, FastMCP\n",
    "\n",
    "# Create a named server\n",
    "mcp = FastMCP(\"My App\")\n",
    "\n",
    "# Specify dependencies for deployment and development\n",
    "mcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    db: Database\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n",
    "    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n",
    "    # Initialize on startup\n",
    "    db = await Database.connect()\n",
    "    try:\n",
    "        yield AppContext(db=db)\n",
    "    finally:\n",
    "        # Cleanup on shutdown\n",
    "        await db.disconnect()\n",
    "\n",
    "\n",
    "# Pass lifespan to server\n",
    "mcp = FastMCP(\"My App\", lifespan=app_lifespan)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad397c-e076-4134-abaf-c9c4bec4d783",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Resources</font></b>\n",
    "Resources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data <b>but shouldn't perform significant computation or have side effects</b>:\n",
    "\n",
    "```python\n",
    "@mcp.resource(\"config://app\")\n",
    "def get_config() -> str:\n",
    "    \"\"\"Static configuration data\"\"\"\n",
    "    return \"App configuration here\"\n",
    "\n",
    "\n",
    "@mcp.resource(\"users://{user_id}/profile\")\n",
    "def get_user_profile(user_id: str) -> str:\n",
    "    \"\"\"Dynamic user data\"\"\"\n",
    "    return f\"Profile data for user {user_id}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a3031-7322-4297-ae0e-388d975a11d9",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Tools</font></b>\n",
    "Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "def calculate_bmi(weight_kg: float, height_m: float) -> float:\n",
    "    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n",
    "    return weight_kg / (height_m**2)\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def fetch_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a city\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(f\"https://api.weather.com/{city}\")\n",
    "        return response.text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fff6a1a-ccad-4073-9aa8-22b5b78b2922",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Prompts</font></b>\n",
    "Prompts are reusable templates that help LLMs interact with your server effectively:\n",
    "\n",
    "```python\n",
    "@mcp.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def debug_error(error: str) -> list[base.Message]:\n",
    "    return [\n",
    "        base.UserMessage(\"I'm seeing this error:\"),\n",
    "        base.UserMessage(error),\n",
    "        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98176b-e907-425f-8d8d-6de09a26f38c",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Images</font></b>\n",
    "FastMCP provides an <b><font color='blue'>Image</font></b> class that automatically handles image data:\n",
    "\n",
    "```python\n",
    "@mcp.tool()\n",
    "def create_thumbnail(image_path: str) -> Image:\n",
    "    \"\"\"Create a thumbnail from an image\"\"\"\n",
    "    img = PILImage.open(image_path)\n",
    "    img.thumbnail((100, 100))\n",
    "    return Image(data=img.tobytes(), format=\"png\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e29d4-59f7-4299-aa69-de4d19c2499c",
   "metadata": {},
   "source": [
    "#### <b><font size='3ptx'>Context</font></b>\n",
    "The Context object gives your tools and resources access to MCP capabilities:\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP, Context\n",
    "\n",
    "mcp = FastMCP(\"My App\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def long_task(files: list[str], ctx: Context) -> str:\n",
    "    \"\"\"Process multiple files with progress tracking\"\"\"\n",
    "    for i, file in enumerate(files):\n",
    "        ctx.info(f\"Processing {file}\")\n",
    "        await ctx.report_progress(i, len(files))\n",
    "        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n",
    "    return \"Processing complete\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebcefe-f65e-46ba-9f55-d46d23684798",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Installation</font></b>\n",
    "For projects using pip for dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d7505e-7029-4800-9086-b9ed74a42496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcp==1.6.0\n"
     ]
    }
   ],
   "source": [
    "# pip install mcp mcp[cli]\n",
    "!pip freeze | grep mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37fa52-573f-48b4-b5ca-f0102ab761af",
   "metadata": {},
   "source": [
    "<a id='quick_start_mcp_server_with_client'></a>\n",
    "### <b><font color='darkgreen'>Quickstart (MCP server & client)</font></b>\n",
    "Let's create a simple MCP server that exposes a calculator tool and some data:\n",
    "\n",
    "- `mcp_server.py`\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Create an MCP server\n",
    "mcp = FastMCP(\"Demo\")\n",
    "\n",
    "\n",
    "# Add an addition tool\n",
    "#### Tools ####\n",
    "# Add an addition tool\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "  \"\"\"Add two numbers\"\"\"\n",
    "  print(f\"Adding {a} and {b}\")\n",
    "  return a + b\n",
    "\n",
    "\n",
    "# Add a dynamic greeting resource\n",
    "@mcp.resource(\"greeting://{name}\")\n",
    "def get_greeting(name: str) -> str:\n",
    "  \"\"\"Get a personalized greeting\"\"\"\n",
    "  return f\"Hello, {name}!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize and run the server\n",
    "  mcp.run(transport='sse')\n",
    "```\n",
    "\n",
    "For MCP server's transport (`sse` or `stdio`), you could refer to [here](https://modelcontextprotocol.io/docs/concepts/transports#built-in-transport-types) for details:\n",
    "> Transports in the Model Context Protocol (MCP) provide the foundation for communication between clients and servers. A transport handles the underlying mechanics of how messages are sent and received.\n",
    "\n",
    "The common two options as are:\n",
    "* **Standard Input/Output (stdio)**: The stdio transport enables communication through standard input and output streams. This is particularly useful for local integrations and command-line tools.\n",
    "  - Building command-line tools\n",
    "  - Implementing local integrations\n",
    "  - Needing simple process communication\n",
    "  - Working with shell scripts\n",
    "* **Server-Sent Events (SSE)**: SSE transport enables server-to-client streaming with HTTP POST requests for client-to-server communication.\n",
    "  - Only server-to-client streaming is needed\n",
    "  - Working with restricted networks\n",
    "  - Implementing simple updates\n",
    "\n",
    "Then We can run it by below command:\n",
    "```shell\n",
    "$ python mcp_server.py\n",
    "INFO:     Started server process [4100]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0959ad10-c9ff-4086-b772-06d6d4178708",
   "metadata": {},
   "source": [
    "Next, we could prepare another MCP client to interact with our MCP server:\n",
    "\n",
    "* `mcp_client.py`\n",
    "```python\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "\n",
    "async def run():\n",
    "  async with sse_client(url=\"http://localhost:8000/sse\") as streams:\n",
    "    async with ClientSession(*streams) as session:\n",
    "      await session.initialize()\n",
    "\n",
    "      # List available tools\n",
    "      print('===== List available tools =====')\n",
    "      tools = await session.list_tools()\n",
    "      print(str(tools) + '\\n')\n",
    "\n",
    "      # Call a tool\n",
    "      print('===== Call a tool =====')\n",
    "      result = await session.call_tool(\"add\", arguments={\"a\": 4, \"b\": 5})\n",
    "      print(result.content[0].text + '\\n')\n",
    "\n",
    "      # List available resources\n",
    "      print('===== List resources =====')\n",
    "      resources = await session.list_resources()\n",
    "      print(\"resources\", resources)\n",
    "      print(\"\")\n",
    "\n",
    "      # Read a resource\n",
    "      print('===== Read a resource =====')\n",
    "      content = await session.read_resource(\"greeting://john\")\n",
    "      print(\"content\", content.contents[0].text)\n",
    "      print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  import asyncio\n",
    "\n",
    "  asyncio.run(run())\n",
    "```\n",
    "\n",
    "We could execute it as below:\n",
    "```shell\n",
    "$ ./mcp_client.py\n",
    "===== List available tools =====\n",
    "meta=None nextCursor=None tools=[Tool(name='add', description='Add two numbers', inputSchema={'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'addArguments', 'type': 'object'})]\n",
    "\n",
    "===== Call a tool =====\n",
    "9\n",
    "\n",
    "===== List resources =====\n",
    "resources meta=None nextCursor=None resources=[Resource(uri=AnyUrl('resource://some_static_resource'), name='resource://some_static_resource', description=None, mimeType='text/plain', size=None, annotations=None)]\n",
    "\n",
    "===== Read a resource =====\n",
    "content Hello, john!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bd90c-456f-4cc2-84a0-79c7555173d9",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>LangChain + MCP</font></b>\n",
    "([YT:Using MCP with LangGraph agents](https://www.youtube.com/watch?v=OX89LkTvNKQ)) <font size='3ptx'><b>Anthropic's Model Context Protocol (MCP) is an open source protocol to connect LLMs with context, tools, and prompts</b>. It has a growing number of \"servers\" for connecting to various tools or data sources. Here, we show how to connect any MCP server to LangGraph agents, and use MCP tools. </font> ([Repo: langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters))\n",
    "\n",
    "![ui](images/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffb433-cfde-4128-8f03-b142ef4caceb",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Installation</font></b>\n",
    "The sample code below is coming from Github repo [**langchain-mcp-adapters**](https://github.com/langchain-ai/langchain-mcp-adapters), we have to install it first:\n",
    "> This library provides a lightweight wrapper that makes Anthropic [**Model Context Protocol**](https://modelcontextprotocol.io/introduction) (MCP) tools compatible with [**LangChain**](https://github.com/langchain-ai/langchain) and [**LangGraph**](https://github.com/langchain-ai/langgraph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d2624d-82d6-40b8-ae84-d5c242191fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain-core==0.3.49\n",
      "langchain-mcp-adapters==0.0.5\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-mcp-adapters\n",
    "!pip freeze | grep -P 'langchain-mcp-adapters|langchain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fc4edd-abb6-4667-8378-09501cfdc875",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Server</font></b>\n",
    "Here we implement an simple the MCP server **`math_mcp_server.py`** to handle math related problem:\n",
    "- `math_mcp_server.py`:\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"Math\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    return a * b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db31d9-6625-42a0-ae60-2c98ff1f303a",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>MCP Client</font></b>\n",
    "To use the MCP client below using OpenAI LLM, you have to apply/generate an OpenAPI key first [here](https://platform.openai.com/api-keys) and use environment variable `OPENAI_API_KEY` to store it for reference. Below MCP client will use OpenAI to answer a simple math question `what's (3 + 5) x 12?` by leveraging tools in our MCP server:\n",
    "\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"/path/to/math_mcp_server.py\"],\n",
    ")\n",
    "\n",
    "async def main():\n",
    "  async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "      # Initialize the connection\n",
    "      await session.initialize()\n",
    "\n",
    "      # Get tools\n",
    "      tools = await load_mcp_tools(session)\n",
    "\n",
    "      # Create and run the agent\n",
    "      agent = create_react_agent(model, tools)\n",
    "      agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "      print('Agent response:')\n",
    "      for message in agent_response['messages']:\n",
    "        print(f'{message.__class__.__name__}: {message}')\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "From the code, it will leverage function [`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to create an ReAct agent:\n",
    "> The **LLM ReAct (Reasoning + Acting) framework is an approach that combines Large Language Models (LLMs) with reasoning and action-taking capabilities**. This allows AI agents to think, decide, and execute tasks autonomously rather than just generating static responses.\n",
    "\n",
    "\n",
    "Then below is the execution result:\n",
    "```shell\n",
    "$ ./mcp_client_with_math_problem.py\n",
    "Agent response:\n",
    "HumanMessage: content=\"what's (3 + 5) x 12?\" additional_kwargs={} response_metadata={} id='e358bf94-ce8f-4f82-92be-915806b9275d'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_AFOWozah4ZwBchA50hypk5ke', 'function': {'arguments': '{\"a\": 3, \"b\": 5}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_nqxKzHmYvCZugl46rvfXTWx4', 'function': {'arguments': '{\"a\": 8, \"b\": 12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 77, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGhpH5pncUiSVfWA73PXyFigjNz2v', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-0bf5927c-9005-4e96-abd0-5e191e457111-0' tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_AFOWozah4ZwBchA50hypk5ke', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_nqxKzHmYvCZugl46rvfXTWx4', 'type': 'tool_call'}] usage_metadata={'input_tokens': 77, 'output_tokens': 51, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='8' name='add' id='4bbdcbcc-6ebd-47fb-9407-cca308a9bf1f' tool_call_id='call_AFOWozah4ZwBchA50hypk5ke'\n",
    "\n",
    "ToolMessage: content='96' name='multiply' id='aca35ceb-717a-48dd-918e-29450ef47308' tool_call_id='call_nqxKzHmYvCZugl46rvfXTWx4'\n",
    "\n",
    "AIMessage: content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 143, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGhpJRbfmopZOraLa2wqbf4nAi4AV', 'finish_reason': 'stop', 'logprobs': None} id='run-ae7bed58-8147-49a6-ae06-b180a5cb137f-0' usage_metadata={'input_tokens': 143, 'output_tokens': 22, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aff32-7e21-4e60-ade6-2158877e75f8",
   "metadata": {},
   "source": [
    "### <b><font color='darkgreen'>Multiple MCP Servers</font></b>\n",
    "The [**library**](https://github.com/langchain-ai/langchain-mcp-adapters?tab=readme-ov-file#multiple-mcp-servers) also allows you to connect to multiple MCP servers and load tools from them. Here we prepared a second MCP server to hanlde weather related questions:\n",
    "* `weather_mcp_server.py`\n",
    "```python\n",
    "from typing import List\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "\n",
    "mcp = FastMCP(\"Weather\")\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_weather(location: str) -> str:\n",
    "  \"\"\"Get weather for location.\"\"\"\n",
    "  return \"It's always sunny in Taiwan\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  mcp.run(transport=\"sse\")\n",
    "```\n",
    "\n",
    "Let's start the Weather MCP server by executing it:\n",
    "```shell\n",
    "$ python weather_mcp_server.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663c9e7-2b37-48bb-9772-c27e070ceada",
   "metadata": {},
   "source": [
    "Let's create another MCP client to include both Math and Weather MCP servers:\n",
    "- `mcp_client_with_both_math_and_weather_problem.py`:\n",
    "```python\n",
    "# Create server parameters for stdio connection\n",
    "import asyncio\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "MATH_MCP_SERVER_MODULE_PATH = (\n",
    "    \"/home/john/Gitrepos/ml_articles/medium/Clear_intro_to_mcp/math_mcp_server.py\")\n",
    "\n",
    "async def main():\n",
    "  async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [MATH_MCP_SERVER_MODULE_PATH],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    "  ) as client:\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    for question in [\n",
    "          \"what's (3 + 5) x 12?\",\n",
    "          \"what is the weather in Taiwan?\",\n",
    "      ]:\n",
    "      response = await agent.ainvoke({\"messages\": question})\n",
    "      print(f'===== Q: {question} =====')\n",
    "      for message in response['messages']:\n",
    "        print(f'{message.__class__.__name__}: {message}')\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(main())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d076ce3-40d8-46c9-9186-0e10319f67aa",
   "metadata": {},
   "source": [
    "Finally, let's execute it and see how it goes:\n",
    "```shell\n",
    "$ ./mcp_client_with_both_math_and_weather_problem.py\n",
    "===== Q: what's (3 + 5) x 12? =====\n",
    "HumanMessage: content=\"what's (3 + 5) x 12?\" additional_kwargs={} response_metadata={} id='d5e18352-36af-435b-8316-f46adbba7f4b'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_T3qkWNjmZTknweaeHKjYXfoL', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 96, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2MVKfDNX3FSjkor5uGKQHq49eL', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-93181dcd-b5df-4a16-a315-48e2ea9ae304-0' tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_T3qkWNjmZTknweaeHKjYXfoL', 'type': 'tool_call'}] usage_metadata={'input_tokens': 96, 'output_tokens': 18, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='8' name='add' id='64385ecf-b886-4e80-8052-4fbcfc6a27aa' tool_call_id='call_T3qkWNjmZTknweaeHKjYXfoL'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_kb2gEqwQ0ZFGj2fV10GZcSVs', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 121, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2NZETDbejIQ2YllWwIGUnIKFbW', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-79caf813-7905-42f3-85d7-9aefddd21be9-0' tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_kb2gEqwQ0ZFGj2fV10GZcSVs', 'type': 'tool_call'}] usage_metadata={'input_tokens': 121, 'output_tokens': 18, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content='96' name='multiply' id='7f189bef-879b-4d8e-ba3c-8944cac24e3c' tool_call_id='call_kb2gEqwQ0ZFGj2fV10GZcSVs'\n",
    "\n",
    "AIMessage: content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 146, 'total_tokens': 168, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2OSnR9WPntdosKaSwbASZiyMt6', 'finish_reason': 'stop', 'logprobs': None} id='run-2e0b564d-7f8f-4502-9d35-8e31fcdff1fc-0' usage_metadata={'input_tokens': 146, 'output_tokens': 22, 'total_tokens': 168, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "===== Q: what is the weather in Taiwan? =====\n",
    "HumanMessage: content='what is the weather in Taiwan?' additional_kwargs={} response_metadata={} id='5d8b5330-7fae-414b-bb4b-80c9ce090ec2'\n",
    "\n",
    "AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_GW8L6DqAbe5PcmxTFYbIzpMZ', 'function': {'arguments': '{\"location\":\"Taiwan\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 91, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2PwLjt9flSEsoW4aAt4HyGcEE0', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-f4016f0f-9bdc-47b5-8f84-f53c15a8eb63-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'Taiwan'}, 'id': 'call_GW8L6DqAbe5PcmxTFYbIzpMZ', 'type': 'tool_call'}] usage_metadata={'input_tokens': 91, 'output_tokens': 16, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "\n",
    "ToolMessage: content=\"It's always sunny in Taiwan\" name='get_weather' id='f0451b5a-149a-48d8-adb6-1aba43662155' tool_call_id='call_GW8L6DqAbe5PcmxTFYbIzpMZ'\n",
    "\n",
    "AIMessage: content='The weather in Taiwan is sunny.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 119, 'total_tokens': 128, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6dd05565ef', 'id': 'chatcmpl-BGi2R63RTKtSZYXue7PmZR4U7XKOi', 'finish_reason': 'stop', 'logprobs': None} id='run-4c69bfeb-558b-46f3-8cf9-462fb7133601-0' usage_metadata={'input_tokens': 119, 'output_tokens': 9, 'total_tokens': 128, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263ce91-bc43-41b6-9279-7490abe480b6",
   "metadata": {},
   "source": [
    "## <b><font color='darkblue'>Supplement</font></b>\n",
    "* [MCP Introduction: Get started with the Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\n",
    "* [Dev - How to build MCP Servers and Clients from Scratch](https://dev.to/composiodev/how-to-build-mcp-servers-and-clients-from-scratch-4o2f)\n",
    "* [Gitrepo - Learn-With-Yash-Agrawal/Projects\n",
    "/MCP_Server_and_Client](https://github.com/oppasource/Learn-With-Yash-Agrawal/tree/main/Projects/MCP_Server_and_Client) ([YT](https://www.youtube.com/watch?v=-WogqfxWBbM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
