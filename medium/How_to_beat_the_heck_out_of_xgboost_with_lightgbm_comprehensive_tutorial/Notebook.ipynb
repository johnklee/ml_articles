{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>You Are Missing Out on LightGBM. It Crushes XGBoost in Every Aspect</font>\n",
    "([article source](https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997)) <font size='3ptx'><b>Not anymore, XGBoost, not anymore</b></font>\n",
    "\n",
    "Learn how to crush <b><a href='https://xgboost.readthedocs.io/en/latest/python/python_intro.html'>XGBoost</a></b> in this comprehensive <b><a href='https://lightgbm.readthedocs.io/en/latest/Python-Intro.html'>LightGBM</a></b> tutorial.\n",
    "\n",
    "So many people are drawn to XGBoost like a moth to a flame. Yes, it has seen some glorious days in prestigious competitions, and it’s still the most widely-used ML library.\n",
    "\n",
    "But, it has been 4 years since XGBoost lost its top spot in terms of performance. In 2017, Microsoft open-sourced <b><a href='https://lightgbm.readthedocs.io/en/latest/Python-Intro.html'>LightGBM</a></b> (<font color='brown'>Light Gradient Boosting Machine</font>) that gives equally high accuracy with 2–10 times less training speed.\n",
    "\n",
    "This is a game-changing advantage considering the ubiquity of massive, million-row datasets. There are other distinctions that tip the scales towards LightGBM and give it an edge over XGBoost.\n",
    "\n",
    "By the end of this post, you will learn about these advantages, including:\n",
    "* How to develop LightGBM models for classification and regression tasks\n",
    "* Structural differences between XGBoost and LGBM\n",
    "* How to use early stopping and evaluation sets\n",
    "* Enabling powerful categorical feature support for up 8x times speed increase\n",
    "* Implementing successful cross-validation with LGBM\n",
    "* Hyperparameter tuning with <b><a href='https://optuna.org/'>Optuna</a></b> (Part II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Agenda</a>\n",
    "* <font size='3ptx'><b><a href='#sect1'>XGBoost vs. LightGBM</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect2'>Model initialization and objectives</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect3'>Controlling the number of decision trees</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect4'>Early stopping</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect5'>Eval sets and metrics</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect6'>Establish a baseline</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect7'>Categorical and missing values support</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect8'>Cross-validation with LightGBM</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>XGBoost vs. LightGBM</font>\n",
    "<font size='3ptx'><b>When LGBM got released, it came with ground-breaking changes to the way it grows decision trees.</b></font>\n",
    "> Both XGBoost and LightGBM are ensebmle algorithms. They use a special type of decision trees, also called weak learners, to capture complex, non-linear patterns.\n",
    "\n",
    "<br/>\n",
    "\n",
    "In XGBoost (<font color='brown'>and many other libraries</font>), decision trees were built one level at a time:\n",
    "![How XGBoost built trees](images/1.png)\n",
    "<br/>\n",
    "\n",
    "<b>This type of structure tends to result in unnecessary nodes and leaves because the trees continued to build until the `max_depth` reached</b>. This led to higher model complexity and training cost runtime.\n",
    "\n",
    "In contrast, <b><a href='https://lightgbm.readthedocs.io/en/latest/Python-Intro.html'>LightGBM</a></b> takes a leaf-wise approach:\n",
    "![How LightGBM built trees](images/2.png)\n",
    "<br/>\n",
    "\n",
    "The structure continues to grow with the most promising branches and leaves (<font color='brown'>nodes with the most delta loss</font>), holding the number of the decision leaves constant. (<font color='brown'>If this doesn’t make sense to you, don’t sweat. This won’t prevent you from effectively using LGBM</font>).\n",
    "\n",
    "This is one of the main reasons LGBM crushed XGBoost in terms of speed when it first came out.\n",
    "![Training time comparison between LGBM and XGBoost](images/3.png)\n",
    "<br/>\n",
    "\n",
    "Above is a benchmark comparison of XGBoost with traditional decision trees and LGBM with leaf-wise structure (<font color='brown'>first and last columns</font>) on datasets with ~500k-13M samples. It shows that <b>LGBM is orders of magnitude faster than XGB</b>.\n",
    "\n",
    "<b>LGBM also uses <a href='https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-speed-and-memory-usage'>histogram binning</a> of continuous features, which provides even more speed-up than traditional gradient boosting</b>. Binning numeric values significantly decrease the number of split points to consider in decision trees, and they remove the need to use sorting algorithms, which are always computation-heavy (<font color='brown'>check last two columns</font>).\n",
    "![Training time comparison between LGBM and XGBoost using histogram-binning](images/4.png)\n",
    "<br/>\n",
    "\n",
    "We will continue exploring the differences in the coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Model initialization and objectives</font>\n",
    "<font size='3ptx'><b>Like XGBoost, LGBM has two APIs — core learning API and Sklearn-compatible one. You know I am a big fan of <a href='https://scikit-learn.org/stable/'>Sklearn</a>, so this tutorial will focus on that version.</b></font>\n",
    "> Sklearn-compatible API of XGBoost and LGBM allows you to integrate their models in the Sklearn ecosystem so that you can use them inside pipelines in combination with other transformers.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Sklearn API exposes <b><a href='https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html'>LGBMRegressor</a></b> and <b><a href='https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier'>LGBMClassifier</a></b>, with the familiar `fit/predict/predict_proba` pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!pip install lightgbm\n",
    "#!!pip install --upgrade sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm  # standard alias\n",
    "\n",
    "clf = lgbm.LGBMClassifier(objective=\"binary\")  # or 'mutliclass'\n",
    "reg = lgbm.LGBMRegressor()  # default - 'regression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>objective specifies the type of learning task</b>. Besides the common ones like `binary`, `multiclass` and `regression` tasks, there are others like `poisson`, `tweedie` regressions. See <a href='https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters'>this section</a> of the documentation for the full list of objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Controlling the number of decision trees</font>\n",
    "* <b><a href='#sect3_1'>Loading dataset</a></b>\n",
    "* <b><a href='#sect3_2'>Train Classifier</a></b>\n",
    "\n",
    "<font size='3ptx'><b>The number of decision trees inside the ensemble significantly affects the results</b></font>. You can control it using the `n_estimators` parameter in both the classifier and regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "RANDOM_STATE = 1121218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_1'></a>\n",
    "### <font color='darkgreen'>Loading dataset</font>\n",
    "Below, we will fit an <b><a href='https://lightgbm.readthedocs.io/en/latest/index.html'>LGBM</a></b> binary classifier on the <a href='https://www.kaggle.com/c/tabular-playground-series-mar-2021/data'>Kaggle TPS March dataset</a> with 1000 decision trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>...</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>0.965006</td>\n",
       "      <td>0.665915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>K</td>\n",
       "      <td>W</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>0.927406</td>\n",
       "      <td>0.493729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>BM</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.549452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>K</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>A</td>\n",
       "      <td>Y</td>\n",
       "      <td>AD</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>0.760318</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>I</td>\n",
       "      <td>G</td>\n",
       "      <td>B</td>\n",
       "      <td>E</td>\n",
       "      <td>BI</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>Q</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>0.357084</td>\n",
       "      <td>0.328960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8  ...     cont2     cont3  \\\n",
       "0   0    A    I    A    B    B   BI    A    S    Q  ...  0.759439  0.795549   \n",
       "1   1    A    I    A    A    E   BI    K    W   AD  ...  0.386385  0.541366   \n",
       "2   2    A    K    A    A    E   BI    A    E   BM  ...  0.343255  0.616352   \n",
       "3   3    A    K    A    C    E   BI    A    Y   AD  ...  0.831147  0.807807   \n",
       "4   4    A    I    G    B    E   BI    C    G    Q  ...  0.338818  0.277308   \n",
       "\n",
       "      cont4     cont5     cont6     cont7     cont8     cont9    cont10 target  \n",
       "0  0.681917  0.621672  0.592184  0.791921  0.815254  0.965006  0.665915      0  \n",
       "1  0.388982  0.357778  0.600044  0.408701  0.399353  0.927406  0.493729      0  \n",
       "2  0.793687  0.552877  0.352113  0.388835  0.412303  0.292696  0.549452      0  \n",
       "3  0.800032  0.619147  0.221789  0.897617  0.633669  0.760318  0.934242      0  \n",
       "4  0.610578  0.128291  0.578764  0.279167  0.351103  0.357084  0.328960      1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_march = pd.read_csv(\"../../datas/kaggle_tabular_playground_series_mar_2021/train.csv\")\n",
    "tps_march.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_march.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     19\n",
       "float64    11\n",
       "int64       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tps_march.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cont0</th>\n",
       "      <th>cont1</th>\n",
       "      <th>cont2</th>\n",
       "      <th>cont3</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>...</th>\n",
       "      <th>cat16_C</th>\n",
       "      <th>cat16_D</th>\n",
       "      <th>cat17_A</th>\n",
       "      <th>cat17_B</th>\n",
       "      <th>cat17_C</th>\n",
       "      <th>cat17_D</th>\n",
       "      <th>cat18_A</th>\n",
       "      <th>cat18_B</th>\n",
       "      <th>cat18_C</th>\n",
       "      <th>cat18_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.629858</td>\n",
       "      <td>0.855349</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>0.795549</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.621672</td>\n",
       "      <td>0.592184</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>0.815254</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370727</td>\n",
       "      <td>0.328929</td>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.541366</td>\n",
       "      <td>0.388982</td>\n",
       "      <td>0.357778</td>\n",
       "      <td>0.600044</td>\n",
       "      <td>0.408701</td>\n",
       "      <td>0.399353</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.502272</td>\n",
       "      <td>0.322749</td>\n",
       "      <td>0.343255</td>\n",
       "      <td>0.616352</td>\n",
       "      <td>0.793687</td>\n",
       "      <td>0.552877</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.388835</td>\n",
       "      <td>0.412303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.934242</td>\n",
       "      <td>0.707663</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.807807</td>\n",
       "      <td>0.800032</td>\n",
       "      <td>0.619147</td>\n",
       "      <td>0.221789</td>\n",
       "      <td>0.897617</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.254427</td>\n",
       "      <td>0.274514</td>\n",
       "      <td>0.338818</td>\n",
       "      <td>0.277308</td>\n",
       "      <td>0.610578</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.578764</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>0.351103</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 635 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     cont0     cont1     cont2     cont3     cont4     cont5     cont6  \\\n",
       "0   0  0.629858  0.855349  0.759439  0.795549  0.681917  0.621672  0.592184   \n",
       "1   1  0.370727  0.328929  0.386385  0.541366  0.388982  0.357778  0.600044   \n",
       "2   2  0.502272  0.322749  0.343255  0.616352  0.793687  0.552877  0.352113   \n",
       "3   3  0.934242  0.707663  0.831147  0.807807  0.800032  0.619147  0.221789   \n",
       "4   4  0.254427  0.274514  0.338818  0.277308  0.610578  0.128291  0.578764   \n",
       "\n",
       "      cont7     cont8  ...  cat16_C  cat16_D  cat17_A  cat17_B  cat17_C  \\\n",
       "0  0.791921  0.815254  ...        0        1        0        0        0   \n",
       "1  0.408701  0.399353  ...        0        0        0        0        0   \n",
       "2  0.388835  0.412303  ...        0        1        0        0        0   \n",
       "3  0.897617  0.633669  ...        0        1        0        0        0   \n",
       "4  0.279167  0.351103  ...        0        0        0        0        0   \n",
       "\n",
       "   cat17_D  cat18_A  cat18_B  cat18_C  cat18_D  \n",
       "0        1        0        1        0        0  \n",
       "1        1        0        1        0        0  \n",
       "2        1        0        1        0        0  \n",
       "3        1        0        1        0        0  \n",
       "4        1        0        1        0        0  \n",
       "\n",
       "[5 rows x 635 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode categoricals\n",
    "X_enc = pd.get_dummies(X)\n",
    "X_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3_2'></a>\n",
    "### <font color='darkgreen'>Train Classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(n_estimators=1000, objective='binary', random_state=1121218)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=1000, random_state=RANDOM_STATE)\n",
    "clf.fit(X_enc, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding more trees leads to more accuracy but increases the risk of overfitting</b>. To combat this, you can create many trees (<font color='brown'>+2000</font>) and choose a smaller `learning_rate` (<font color='brown'>more on this later</font>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(n_estimators=3000, objective='binary', random_state=1121218)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=3000, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "clf.fit(X_enc, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in XGBoost, fitting a single decision tree to the data is called a <b><font color='darkblue'>boosting round</font></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect4'></a>\n",
    "## <font color='darkblue'>Early stopping</font>\n",
    "<font size='3ptx'><b>Each tree in the ensemble builds on the predictions of the last tree — i.e., each boosting round is an improvement of the last</b></font>.\n",
    "\n",
    "If the predictions don’t improve after a sequence of rounds, it is sensible to stop the training of the ensemble even if we are not at a hard stop for <font color='violet'>n_estimators</font>. To achieve this, LGBM provides <font color='violet'>early_stopping_rounds</font> parameter inside the fit function. For example, setting it to 100 means we stop the training if the predictions have not improved for the last 100 rounds.\n",
    "\n",
    "Before looking at a code example, we should learn a couple of concepts connected to early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect6'></a>\n",
    "## <font color='darkblue'>Eval sets and metrics</font>\n",
    "<b>Early stopping is only enabled when you pass a set of evaluation sets to <font color='violet'>eval_set</font> parameter of the `fit` method</b>. These evaluation sets are used to keep track of the quality of the predictions from one boosting round to the next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.542166\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's binary_logloss: 0.51505\n",
      "[3]\tvalid_0's binary_logloss: 0.493516\n",
      "[4]\tvalid_0's binary_logloss: 0.475673\n",
      "[5]\tvalid_0's binary_logloss: 0.461006\n",
      "[6]\tvalid_0's binary_logloss: 0.448741\n",
      "[7]\tvalid_0's binary_logloss: 0.4381\n",
      "[8]\tvalid_0's binary_logloss: 0.42905\n",
      "[9]\tvalid_0's binary_logloss: 0.421275\n",
      "[10]\tvalid_0's binary_logloss: 0.414703\n",
      "[11]\tvalid_0's binary_logloss: 0.408893\n",
      "[12]\tvalid_0's binary_logloss: 0.403806\n",
      "[13]\tvalid_0's binary_logloss: 0.399383\n",
      "[14]\tvalid_0's binary_logloss: 0.395376\n",
      "[15]\tvalid_0's binary_logloss: 0.39188\n",
      "[16]\tvalid_0's binary_logloss: 0.388631\n",
      "[17]\tvalid_0's binary_logloss: 0.385757\n",
      "[18]\tvalid_0's binary_logloss: 0.383222\n",
      "[19]\tvalid_0's binary_logloss: 0.380936\n",
      "[20]\tvalid_0's binary_logloss: 0.378975\n",
      "[21]\tvalid_0's binary_logloss: 0.377094\n",
      "[22]\tvalid_0's binary_logloss: 0.375491\n",
      "[23]\tvalid_0's binary_logloss: 0.373987\n",
      "[24]\tvalid_0's binary_logloss: 0.372595\n",
      "[25]\tvalid_0's binary_logloss: 0.37127\n",
      "[26]\tvalid_0's binary_logloss: 0.369972\n",
      "[27]\tvalid_0's binary_logloss: 0.368846\n",
      "[28]\tvalid_0's binary_logloss: 0.367783\n",
      "[29]\tvalid_0's binary_logloss: 0.366919\n",
      "[30]\tvalid_0's binary_logloss: 0.366042\n",
      "[31]\tvalid_0's binary_logloss: 0.365148\n",
      "[32]\tvalid_0's binary_logloss: 0.364324\n",
      "[33]\tvalid_0's binary_logloss: 0.363577\n",
      "[34]\tvalid_0's binary_logloss: 0.36297\n",
      "[35]\tvalid_0's binary_logloss: 0.36233\n",
      "[36]\tvalid_0's binary_logloss: 0.361658\n",
      "[37]\tvalid_0's binary_logloss: 0.361056\n",
      "[38]\tvalid_0's binary_logloss: 0.36058\n",
      "[39]\tvalid_0's binary_logloss: 0.359999\n",
      "[40]\tvalid_0's binary_logloss: 0.359565\n",
      "[41]\tvalid_0's binary_logloss: 0.359095\n",
      "[42]\tvalid_0's binary_logloss: 0.358665\n",
      "[43]\tvalid_0's binary_logloss: 0.358317\n",
      "[44]\tvalid_0's binary_logloss: 0.357879\n",
      "[45]\tvalid_0's binary_logloss: 0.35748\n",
      "[46]\tvalid_0's binary_logloss: 0.357135\n",
      "[47]\tvalid_0's binary_logloss: 0.356767\n",
      "[48]\tvalid_0's binary_logloss: 0.356461\n",
      "[49]\tvalid_0's binary_logloss: 0.356084\n",
      "[50]\tvalid_0's binary_logloss: 0.355804\n",
      "[51]\tvalid_0's binary_logloss: 0.355549\n",
      "[52]\tvalid_0's binary_logloss: 0.355283\n",
      "[53]\tvalid_0's binary_logloss: 0.355086\n",
      "[54]\tvalid_0's binary_logloss: 0.354773\n",
      "[55]\tvalid_0's binary_logloss: 0.354442\n",
      "[56]\tvalid_0's binary_logloss: 0.354187\n",
      "[57]\tvalid_0's binary_logloss: 0.35393\n",
      "[58]\tvalid_0's binary_logloss: 0.353705\n",
      "[59]\tvalid_0's binary_logloss: 0.353483\n",
      "[60]\tvalid_0's binary_logloss: 0.353256\n",
      "[61]\tvalid_0's binary_logloss: 0.353035\n",
      "[62]\tvalid_0's binary_logloss: 0.3529\n",
      "[63]\tvalid_0's binary_logloss: 0.35269\n",
      "[64]\tvalid_0's binary_logloss: 0.352466\n",
      "[65]\tvalid_0's binary_logloss: 0.352228\n",
      "[66]\tvalid_0's binary_logloss: 0.352116\n",
      "[67]\tvalid_0's binary_logloss: 0.351908\n",
      "[68]\tvalid_0's binary_logloss: 0.351758\n",
      "[69]\tvalid_0's binary_logloss: 0.351576\n",
      "[70]\tvalid_0's binary_logloss: 0.351345\n",
      "[71]\tvalid_0's binary_logloss: 0.351165\n",
      "[72]\tvalid_0's binary_logloss: 0.350955\n",
      "[73]\tvalid_0's binary_logloss: 0.350752\n",
      "[74]\tvalid_0's binary_logloss: 0.350626\n",
      "[75]\tvalid_0's binary_logloss: 0.35047\n",
      "[76]\tvalid_0's binary_logloss: 0.35038\n",
      "[77]\tvalid_0's binary_logloss: 0.350225\n",
      "[78]\tvalid_0's binary_logloss: 0.350024\n",
      "[79]\tvalid_0's binary_logloss: 0.349886\n",
      "[80]\tvalid_0's binary_logloss: 0.349788\n",
      "[81]\tvalid_0's binary_logloss: 0.349639\n",
      "[82]\tvalid_0's binary_logloss: 0.349539\n",
      "[83]\tvalid_0's binary_logloss: 0.349485\n",
      "[84]\tvalid_0's binary_logloss: 0.34943\n",
      "[85]\tvalid_0's binary_logloss: 0.349361\n",
      "[86]\tvalid_0's binary_logloss: 0.349317\n",
      "[87]\tvalid_0's binary_logloss: 0.349282\n",
      "[88]\tvalid_0's binary_logloss: 0.349192\n",
      "[89]\tvalid_0's binary_logloss: 0.349095\n",
      "[90]\tvalid_0's binary_logloss: 0.349006\n",
      "[91]\tvalid_0's binary_logloss: 0.34889\n",
      "[92]\tvalid_0's binary_logloss: 0.348825\n",
      "[93]\tvalid_0's binary_logloss: 0.348789\n",
      "[94]\tvalid_0's binary_logloss: 0.348697\n",
      "[95]\tvalid_0's binary_logloss: 0.348618\n",
      "[96]\tvalid_0's binary_logloss: 0.348557\n",
      "[97]\tvalid_0's binary_logloss: 0.348534\n",
      "[98]\tvalid_0's binary_logloss: 0.348502\n",
      "[99]\tvalid_0's binary_logloss: 0.348481\n",
      "[100]\tvalid_0's binary_logloss: 0.348359\n",
      "[101]\tvalid_0's binary_logloss: 0.348325\n",
      "[102]\tvalid_0's binary_logloss: 0.348288\n",
      "[103]\tvalid_0's binary_logloss: 0.348212\n",
      "[104]\tvalid_0's binary_logloss: 0.348174\n",
      "[105]\tvalid_0's binary_logloss: 0.348174\n",
      "[106]\tvalid_0's binary_logloss: 0.348132\n",
      "[107]\tvalid_0's binary_logloss: 0.348098\n",
      "[108]\tvalid_0's binary_logloss: 0.348033\n",
      "[109]\tvalid_0's binary_logloss: 0.348052\n",
      "[110]\tvalid_0's binary_logloss: 0.348001\n",
      "[111]\tvalid_0's binary_logloss: 0.347969\n",
      "[112]\tvalid_0's binary_logloss: 0.347958\n",
      "[113]\tvalid_0's binary_logloss: 0.347954\n",
      "[114]\tvalid_0's binary_logloss: 0.34792\n",
      "[115]\tvalid_0's binary_logloss: 0.34788\n",
      "[116]\tvalid_0's binary_logloss: 0.347881\n",
      "[117]\tvalid_0's binary_logloss: 0.347839\n",
      "[118]\tvalid_0's binary_logloss: 0.347849\n",
      "[119]\tvalid_0's binary_logloss: 0.347837\n",
      "[120]\tvalid_0's binary_logloss: 0.347821\n",
      "[121]\tvalid_0's binary_logloss: 0.347814\n",
      "[122]\tvalid_0's binary_logloss: 0.347716\n",
      "[123]\tvalid_0's binary_logloss: 0.347702\n",
      "[124]\tvalid_0's binary_logloss: 0.347687\n",
      "[125]\tvalid_0's binary_logloss: 0.347662\n",
      "[126]\tvalid_0's binary_logloss: 0.347663\n",
      "[127]\tvalid_0's binary_logloss: 0.347623\n",
      "[128]\tvalid_0's binary_logloss: 0.347592\n",
      "[129]\tvalid_0's binary_logloss: 0.347548\n",
      "[130]\tvalid_0's binary_logloss: 0.347516\n",
      "[131]\tvalid_0's binary_logloss: 0.347485\n",
      "[132]\tvalid_0's binary_logloss: 0.347435\n",
      "[133]\tvalid_0's binary_logloss: 0.347412\n",
      "[134]\tvalid_0's binary_logloss: 0.347379\n",
      "[135]\tvalid_0's binary_logloss: 0.347378\n",
      "[136]\tvalid_0's binary_logloss: 0.347374\n",
      "[137]\tvalid_0's binary_logloss: 0.347364\n",
      "[138]\tvalid_0's binary_logloss: 0.347285\n",
      "[139]\tvalid_0's binary_logloss: 0.347321\n",
      "[140]\tvalid_0's binary_logloss: 0.347318\n",
      "[141]\tvalid_0's binary_logloss: 0.347248\n",
      "[142]\tvalid_0's binary_logloss: 0.347238\n",
      "[143]\tvalid_0's binary_logloss: 0.347203\n",
      "[144]\tvalid_0's binary_logloss: 0.347185\n",
      "[145]\tvalid_0's binary_logloss: 0.347174\n",
      "[146]\tvalid_0's binary_logloss: 0.347156\n",
      "[147]\tvalid_0's binary_logloss: 0.347126\n",
      "[148]\tvalid_0's binary_logloss: 0.347153\n",
      "[149]\tvalid_0's binary_logloss: 0.34716\n",
      "[150]\tvalid_0's binary_logloss: 0.347131\n",
      "[151]\tvalid_0's binary_logloss: 0.347115\n",
      "[152]\tvalid_0's binary_logloss: 0.347108\n",
      "[153]\tvalid_0's binary_logloss: 0.347066\n",
      "[154]\tvalid_0's binary_logloss: 0.347036\n",
      "[155]\tvalid_0's binary_logloss: 0.347026\n",
      "[156]\tvalid_0's binary_logloss: 0.347005\n",
      "[157]\tvalid_0's binary_logloss: 0.347015\n",
      "[158]\tvalid_0's binary_logloss: 0.347007\n",
      "[159]\tvalid_0's binary_logloss: 0.347002\n",
      "[160]\tvalid_0's binary_logloss: 0.346987\n",
      "[161]\tvalid_0's binary_logloss: 0.346942\n",
      "[162]\tvalid_0's binary_logloss: 0.346931\n",
      "[163]\tvalid_0's binary_logloss: 0.346956\n",
      "[164]\tvalid_0's binary_logloss: 0.346967\n",
      "[165]\tvalid_0's binary_logloss: 0.34696\n",
      "[166]\tvalid_0's binary_logloss: 0.34695\n",
      "[167]\tvalid_0's binary_logloss: 0.346957\n",
      "[168]\tvalid_0's binary_logloss: 0.346923\n",
      "[169]\tvalid_0's binary_logloss: 0.346916\n",
      "[170]\tvalid_0's binary_logloss: 0.346848\n",
      "[171]\tvalid_0's binary_logloss: 0.346875\n",
      "[172]\tvalid_0's binary_logloss: 0.346873\n",
      "[173]\tvalid_0's binary_logloss: 0.346788\n",
      "[174]\tvalid_0's binary_logloss: 0.346733\n",
      "[175]\tvalid_0's binary_logloss: 0.346732\n",
      "[176]\tvalid_0's binary_logloss: 0.346745\n",
      "[177]\tvalid_0's binary_logloss: 0.346739\n",
      "[178]\tvalid_0's binary_logloss: 0.346728\n",
      "[179]\tvalid_0's binary_logloss: 0.346718\n",
      "[180]\tvalid_0's binary_logloss: 0.346706\n",
      "[181]\tvalid_0's binary_logloss: 0.346711\n",
      "[182]\tvalid_0's binary_logloss: 0.346714\n",
      "[183]\tvalid_0's binary_logloss: 0.346709\n",
      "[184]\tvalid_0's binary_logloss: 0.346668\n",
      "[185]\tvalid_0's binary_logloss: 0.346696\n",
      "[186]\tvalid_0's binary_logloss: 0.346664\n",
      "[187]\tvalid_0's binary_logloss: 0.346648\n",
      "[188]\tvalid_0's binary_logloss: 0.34664\n",
      "[189]\tvalid_0's binary_logloss: 0.346626\n",
      "[190]\tvalid_0's binary_logloss: 0.346599\n",
      "[191]\tvalid_0's binary_logloss: 0.346588\n",
      "[192]\tvalid_0's binary_logloss: 0.346568\n",
      "[193]\tvalid_0's binary_logloss: 0.346562\n",
      "[194]\tvalid_0's binary_logloss: 0.346568\n",
      "[195]\tvalid_0's binary_logloss: 0.346518\n",
      "[196]\tvalid_0's binary_logloss: 0.346512\n",
      "[197]\tvalid_0's binary_logloss: 0.346508\n",
      "[198]\tvalid_0's binary_logloss: 0.346505\n",
      "[199]\tvalid_0's binary_logloss: 0.346508\n",
      "[200]\tvalid_0's binary_logloss: 0.346502\n",
      "[201]\tvalid_0's binary_logloss: 0.346456\n",
      "[202]\tvalid_0's binary_logloss: 0.346433\n",
      "[203]\tvalid_0's binary_logloss: 0.346401\n",
      "[204]\tvalid_0's binary_logloss: 0.346415\n",
      "[205]\tvalid_0's binary_logloss: 0.346422\n",
      "[206]\tvalid_0's binary_logloss: 0.346398\n",
      "[207]\tvalid_0's binary_logloss: 0.346375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[208]\tvalid_0's binary_logloss: 0.346423\n",
      "[209]\tvalid_0's binary_logloss: 0.346443\n",
      "[210]\tvalid_0's binary_logloss: 0.346438\n",
      "[211]\tvalid_0's binary_logloss: 0.346452\n",
      "[212]\tvalid_0's binary_logloss: 0.346457\n",
      "[213]\tvalid_0's binary_logloss: 0.34646\n",
      "[214]\tvalid_0's binary_logloss: 0.346465\n",
      "[215]\tvalid_0's binary_logloss: 0.346433\n",
      "[216]\tvalid_0's binary_logloss: 0.346392\n",
      "[217]\tvalid_0's binary_logloss: 0.346358\n",
      "[218]\tvalid_0's binary_logloss: 0.346362\n",
      "[219]\tvalid_0's binary_logloss: 0.346328\n",
      "[220]\tvalid_0's binary_logloss: 0.346308\n",
      "[221]\tvalid_0's binary_logloss: 0.346298\n",
      "[222]\tvalid_0's binary_logloss: 0.346297\n",
      "[223]\tvalid_0's binary_logloss: 0.346284\n",
      "[224]\tvalid_0's binary_logloss: 0.346296\n",
      "[225]\tvalid_0's binary_logloss: 0.346292\n",
      "[226]\tvalid_0's binary_logloss: 0.346282\n",
      "[227]\tvalid_0's binary_logloss: 0.346303\n",
      "[228]\tvalid_0's binary_logloss: 0.34628\n",
      "[229]\tvalid_0's binary_logloss: 0.346267\n",
      "[230]\tvalid_0's binary_logloss: 0.346281\n",
      "[231]\tvalid_0's binary_logloss: 0.346279\n",
      "[232]\tvalid_0's binary_logloss: 0.346303\n",
      "[233]\tvalid_0's binary_logloss: 0.346285\n",
      "[234]\tvalid_0's binary_logloss: 0.346282\n",
      "[235]\tvalid_0's binary_logloss: 0.346305\n",
      "[236]\tvalid_0's binary_logloss: 0.346287\n",
      "[237]\tvalid_0's binary_logloss: 0.346294\n",
      "[238]\tvalid_0's binary_logloss: 0.346278\n",
      "[239]\tvalid_0's binary_logloss: 0.346287\n",
      "[240]\tvalid_0's binary_logloss: 0.346265\n",
      "[241]\tvalid_0's binary_logloss: 0.346265\n",
      "[242]\tvalid_0's binary_logloss: 0.346252\n",
      "[243]\tvalid_0's binary_logloss: 0.346253\n",
      "[244]\tvalid_0's binary_logloss: 0.346258\n",
      "[245]\tvalid_0's binary_logloss: 0.346222\n",
      "[246]\tvalid_0's binary_logloss: 0.34626\n",
      "[247]\tvalid_0's binary_logloss: 0.346275\n",
      "[248]\tvalid_0's binary_logloss: 0.346246\n",
      "[249]\tvalid_0's binary_logloss: 0.346267\n",
      "[250]\tvalid_0's binary_logloss: 0.346261\n",
      "[251]\tvalid_0's binary_logloss: 0.346258\n",
      "[252]\tvalid_0's binary_logloss: 0.346289\n",
      "[253]\tvalid_0's binary_logloss: 0.346272\n",
      "[254]\tvalid_0's binary_logloss: 0.346221\n",
      "[255]\tvalid_0's binary_logloss: 0.346235\n",
      "[256]\tvalid_0's binary_logloss: 0.346223\n",
      "[257]\tvalid_0's binary_logloss: 0.346215\n",
      "[258]\tvalid_0's binary_logloss: 0.346246\n",
      "[259]\tvalid_0's binary_logloss: 0.346206\n",
      "[260]\tvalid_0's binary_logloss: 0.346208\n",
      "[261]\tvalid_0's binary_logloss: 0.34624\n",
      "[262]\tvalid_0's binary_logloss: 0.346208\n",
      "[263]\tvalid_0's binary_logloss: 0.346183\n",
      "[264]\tvalid_0's binary_logloss: 0.346208\n",
      "[265]\tvalid_0's binary_logloss: 0.346191\n",
      "[266]\tvalid_0's binary_logloss: 0.346176\n",
      "[267]\tvalid_0's binary_logloss: 0.346215\n",
      "[268]\tvalid_0's binary_logloss: 0.346216\n",
      "[269]\tvalid_0's binary_logloss: 0.346226\n",
      "[270]\tvalid_0's binary_logloss: 0.346238\n",
      "[271]\tvalid_0's binary_logloss: 0.346203\n",
      "[272]\tvalid_0's binary_logloss: 0.346191\n",
      "[273]\tvalid_0's binary_logloss: 0.346186\n",
      "[274]\tvalid_0's binary_logloss: 0.346176\n",
      "[275]\tvalid_0's binary_logloss: 0.346163\n",
      "[276]\tvalid_0's binary_logloss: 0.346164\n",
      "[277]\tvalid_0's binary_logloss: 0.346159\n",
      "[278]\tvalid_0's binary_logloss: 0.346162\n",
      "[279]\tvalid_0's binary_logloss: 0.346175\n",
      "[280]\tvalid_0's binary_logloss: 0.346162\n",
      "[281]\tvalid_0's binary_logloss: 0.346155\n",
      "[282]\tvalid_0's binary_logloss: 0.346153\n",
      "[283]\tvalid_0's binary_logloss: 0.346159\n",
      "[284]\tvalid_0's binary_logloss: 0.346158\n",
      "[285]\tvalid_0's binary_logloss: 0.346148\n",
      "[286]\tvalid_0's binary_logloss: 0.346142\n",
      "[287]\tvalid_0's binary_logloss: 0.346173\n",
      "[288]\tvalid_0's binary_logloss: 0.346183\n",
      "[289]\tvalid_0's binary_logloss: 0.346189\n",
      "[290]\tvalid_0's binary_logloss: 0.346198\n",
      "[291]\tvalid_0's binary_logloss: 0.34621\n",
      "[292]\tvalid_0's binary_logloss: 0.346209\n",
      "[293]\tvalid_0's binary_logloss: 0.346215\n",
      "[294]\tvalid_0's binary_logloss: 0.346219\n",
      "[295]\tvalid_0's binary_logloss: 0.346229\n",
      "[296]\tvalid_0's binary_logloss: 0.346187\n",
      "[297]\tvalid_0's binary_logloss: 0.346167\n",
      "[298]\tvalid_0's binary_logloss: 0.346176\n",
      "[299]\tvalid_0's binary_logloss: 0.346177\n",
      "[300]\tvalid_0's binary_logloss: 0.346171\n",
      "[301]\tvalid_0's binary_logloss: 0.346186\n",
      "[302]\tvalid_0's binary_logloss: 0.346201\n",
      "[303]\tvalid_0's binary_logloss: 0.346213\n",
      "[304]\tvalid_0's binary_logloss: 0.346209\n",
      "[305]\tvalid_0's binary_logloss: 0.346235\n",
      "[306]\tvalid_0's binary_logloss: 0.346266\n",
      "[307]\tvalid_0's binary_logloss: 0.346245\n",
      "[308]\tvalid_0's binary_logloss: 0.346279\n",
      "[309]\tvalid_0's binary_logloss: 0.346267\n",
      "[310]\tvalid_0's binary_logloss: 0.346296\n",
      "[311]\tvalid_0's binary_logloss: 0.346288\n",
      "[312]\tvalid_0's binary_logloss: 0.346269\n",
      "[313]\tvalid_0's binary_logloss: 0.34627\n",
      "[314]\tvalid_0's binary_logloss: 0.346299\n",
      "[315]\tvalid_0's binary_logloss: 0.346284\n",
      "[316]\tvalid_0's binary_logloss: 0.3463\n",
      "[317]\tvalid_0's binary_logloss: 0.346298\n",
      "[318]\tvalid_0's binary_logloss: 0.346287\n",
      "[319]\tvalid_0's binary_logloss: 0.346313\n",
      "[320]\tvalid_0's binary_logloss: 0.346296\n",
      "[321]\tvalid_0's binary_logloss: 0.346279\n",
      "[322]\tvalid_0's binary_logloss: 0.346276\n",
      "[323]\tvalid_0's binary_logloss: 0.346269\n",
      "[324]\tvalid_0's binary_logloss: 0.346281\n",
      "[325]\tvalid_0's binary_logloss: 0.346261\n",
      "[326]\tvalid_0's binary_logloss: 0.34627\n",
      "[327]\tvalid_0's binary_logloss: 0.346263\n",
      "[328]\tvalid_0's binary_logloss: 0.346245\n",
      "[329]\tvalid_0's binary_logloss: 0.346247\n",
      "[330]\tvalid_0's binary_logloss: 0.346256\n",
      "[331]\tvalid_0's binary_logloss: 0.346239\n",
      "[332]\tvalid_0's binary_logloss: 0.346243\n",
      "[333]\tvalid_0's binary_logloss: 0.346234\n",
      "[334]\tvalid_0's binary_logloss: 0.346243\n",
      "[335]\tvalid_0's binary_logloss: 0.346216\n",
      "[336]\tvalid_0's binary_logloss: 0.346222\n",
      "[337]\tvalid_0's binary_logloss: 0.346232\n",
      "[338]\tvalid_0's binary_logloss: 0.346225\n",
      "[339]\tvalid_0's binary_logloss: 0.346248\n",
      "[340]\tvalid_0's binary_logloss: 0.346231\n",
      "[341]\tvalid_0's binary_logloss: 0.346206\n",
      "[342]\tvalid_0's binary_logloss: 0.346221\n",
      "[343]\tvalid_0's binary_logloss: 0.346242\n",
      "[344]\tvalid_0's binary_logloss: 0.346253\n",
      "[345]\tvalid_0's binary_logloss: 0.346276\n",
      "[346]\tvalid_0's binary_logloss: 0.346264\n",
      "[347]\tvalid_0's binary_logloss: 0.34628\n",
      "[348]\tvalid_0's binary_logloss: 0.346277\n",
      "[349]\tvalid_0's binary_logloss: 0.346295\n",
      "[350]\tvalid_0's binary_logloss: 0.346307\n",
      "[351]\tvalid_0's binary_logloss: 0.346312\n",
      "[352]\tvalid_0's binary_logloss: 0.346322\n",
      "[353]\tvalid_0's binary_logloss: 0.34635\n",
      "[354]\tvalid_0's binary_logloss: 0.346352\n",
      "[355]\tvalid_0's binary_logloss: 0.346353\n",
      "[356]\tvalid_0's binary_logloss: 0.346335\n",
      "[357]\tvalid_0's binary_logloss: 0.346333\n",
      "[358]\tvalid_0's binary_logloss: 0.346339\n",
      "[359]\tvalid_0's binary_logloss: 0.346339\n",
      "[360]\tvalid_0's binary_logloss: 0.346318\n",
      "[361]\tvalid_0's binary_logloss: 0.346327\n",
      "[362]\tvalid_0's binary_logloss: 0.346334\n",
      "[363]\tvalid_0's binary_logloss: 0.346351\n",
      "[364]\tvalid_0's binary_logloss: 0.34636\n",
      "[365]\tvalid_0's binary_logloss: 0.346343\n",
      "[366]\tvalid_0's binary_logloss: 0.346318\n",
      "[367]\tvalid_0's binary_logloss: 0.346279\n",
      "[368]\tvalid_0's binary_logloss: 0.346292\n",
      "[369]\tvalid_0's binary_logloss: 0.346301\n",
      "[370]\tvalid_0's binary_logloss: 0.346336\n",
      "[371]\tvalid_0's binary_logloss: 0.346335\n",
      "[372]\tvalid_0's binary_logloss: 0.346337\n",
      "[373]\tvalid_0's binary_logloss: 0.346354\n",
      "[374]\tvalid_0's binary_logloss: 0.346371\n",
      "[375]\tvalid_0's binary_logloss: 0.34635\n",
      "[376]\tvalid_0's binary_logloss: 0.346356\n",
      "[377]\tvalid_0's binary_logloss: 0.346353\n",
      "[378]\tvalid_0's binary_logloss: 0.346363\n",
      "[379]\tvalid_0's binary_logloss: 0.346385\n",
      "[380]\tvalid_0's binary_logloss: 0.346367\n",
      "[381]\tvalid_0's binary_logloss: 0.346371\n",
      "[382]\tvalid_0's binary_logloss: 0.346357\n",
      "[383]\tvalid_0's binary_logloss: 0.346352\n",
      "[384]\tvalid_0's binary_logloss: 0.346358\n",
      "[385]\tvalid_0's binary_logloss: 0.346373\n",
      "[386]\tvalid_0's binary_logloss: 0.346385\n",
      "Early stopping, best iteration is:\n",
      "[286]\tvalid_0's binary_logloss: 0.346142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(n_estimators=10000, objective='binary')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X_enc, y, test_size=0.1)\n",
    "\n",
    "clf = lgbm.LGBMClassifier(objective=\"binary\", n_estimators=10000)\n",
    "eval_set = [(X_eval, y_eval)]\n",
    "\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=100,\n",
    "    eval_metric=\"binary_logloss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each round of <font color='violet'>n_estimators</font>, a single decision tree is fit to `(X_train, y_train)` and predictions are made on the passed evaluation set `(X_eval, y_eval)`. The quality of predictions is measured with a passed metric in <font color='violet'>eval_metric</font>.\n",
    "\n",
    "The training stops at the 386th iteration because the validation score has not improved since the 286th one — early stopping of 100 rounds. Now, we have the luxury of creating as many trees as we want and <font color='violet'>early_stopping_rounds</font> can discard the unnecessary ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect6'></a>\n",
    "## <font color='darkblue'>Establish a baseline</font>\n",
    "<font size='3ptx'><b>Let’s establish a baseline score with what we know so far</b></font>. We will do the same for XGBoost so that we can compare the results:\n",
    "* <a href='#sect6_1'><b>Loading dataset</b></a>\n",
    "* <a href='#sect6_2'><b>Train XGBoost</b></a>\n",
    "* <a href='#sect6_3'><b>Train LGBM</b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect6_1'></a>\n",
    "### <font color='darkgreen'>Loading dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n",
    "\n",
    "# Encode categoricals\n",
    "X_enc = pd.get_dummies(X)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_enc, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect6_2'></a>\n",
    "### <font color='darkgreen'>Train XGBoost</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost logloss on the evaluation set: 0.34906\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_estimators=10000,\n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    ")\n",
    "\n",
    "xgb_clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_eval, y_eval)],\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=150,\n",
    "    verbose=False,  # Disable logs\n",
    ")\n",
    "\n",
    "preds = xgb_clf.predict_proba(X_eval)\n",
    "print(f\"XGBoost logloss on the evaluation set: {log_loss(y_eval, preds):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect6_3'></a>\n",
    "### <font color='darkgreen'>Train LGBM</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM logloss on the evaluation set: 0.34671\n",
      "Wall time: 7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lgbm_clf = lgbm.LGBMClassifier(\n",
    "    objective=\"binary\",\n",
    "    random_state=1121218,\n",
    "    n_estimators=10000,\n",
    "    boosting=\"gbdt\",  # default histogram binning of LGBM\n",
    "    #     device='gpu'  # uncomment to use GPU training\n",
    ")\n",
    "\n",
    "lgbm_clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_eval, y_eval)],\n",
    "    eval_metric=\"binary_logloss\",\n",
    "    early_stopping_rounds=150,\n",
    "    verbose=False,  # Disable logs\n",
    ")\n",
    "\n",
    "preds = lgbm_clf.predict_proba(X_eval)\n",
    "print(f\"LightGBM logloss on the evaluation set: {log_loss(y_eval, preds):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM achieved a smaller loss in ~4 times less runtime. Let’s see a final LGBM trick before we move on to cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect7'></a>\n",
    "## <font color='darkblue'>Categorical and missing values support</font>\n",
    "<b><font size='3ptx'>Histogram binning in LGBM comes with built-in support for handling missing values and categorical features</font></b>. TPS March dataset contains 19 categoricals, and we have been using <b><a href='https://en.wikipedia.org/wiki/One-hot'>one-hot encoding</a></b> up to this point.\n",
    "\n",
    "This time, we will let LGBM deal with categoricals and compare the results with XGBoost once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n",
    "\n",
    "# Extract categoricals and their indices\n",
    "cat_features = X.select_dtypes(exclude=np.number).columns.to_list()\n",
    "cat_idx = [X.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "# Convert cat_features to pd.Categorical dtype\n",
    "for col in cat_features:\n",
    "    X[col] = pd.Categorical(X[col])\n",
    "\n",
    "# Unencoded train/test sets\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=4, stratify=y\n",
    ")\n",
    "\n",
    "# Model initialization is the same\n",
    "eval_set = [(X_eval, y_eval)]\n",
    "\n",
    "# Used for comparing with [score, time]\n",
    "lgbm_cat_handling = []\n",
    "lgbm_one_hot = []\n",
    "xgb_one_hot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify the categorical features, pass a list of their indices to <font color='violet'>categorical_feature</font> parameter in the `fit` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>LGBM (cat-handling)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\basic.py:1705: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM logloss with default cateogircal feature handling: 0.35861\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = time.monotonic()\n",
    "lgbm_clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    categorical_feature=cat_idx,  # Specify the categoricals\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=150,\n",
    "    eval_metric=\"logloss\",\n",
    "    verbose=False,\n",
    ")\n",
    "rt = time.monotonic() - st\n",
    "\n",
    "preds = lgbm_clf.predict_proba(X_eval)\n",
    "loss = log_loss(y_eval, preds)\n",
    "print(f\"LGBM logloss with default cateogircal feature handling: {loss:.5f}\")\n",
    "lgbm_cat_handling.extend([loss, rt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>LGBM (one-hot)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n",
    "\n",
    "# Encode categoricals\n",
    "X_enc = pd.get_dummies(X)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X_enc, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Model initialization is the same\n",
    "eval_set = [(X_eval, y_eval)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "LGBM logloss with default cateogircal feature handling: 0.34994\n",
      "Wall time: 7.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = time.monotonic()\n",
    "lgbm_clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=150,\n",
    "    eval_metric=\"logloss\",\n",
    "    verbose=False,\n",
    ")\n",
    "rt = time.monotonic() - st\n",
    "\n",
    "preds = lgbm_clf.predict_proba(X_eval)\n",
    "loss = log_loss(y_eval, preds)\n",
    "print(f\"LGBM logloss with default cateogircal feature handling: {loss:.5f}\")\n",
    "lgbm_one_hot.extend([loss, rt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>XGB (one-hot)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM logloss with default cateogircal feature handling: 0.35241\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "st = time.monotonic()\n",
    "xgb_clf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_eval, y_eval)],\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=150,\n",
    "    verbose=False,  # Disable logs\n",
    ")\n",
    "rt = time.monotonic() - st\n",
    "\n",
    "preds = xgb_clf.predict_proba(X_eval)\n",
    "loss = log_loss(y_eval, preds)\n",
    "print(f\"LGBM logloss with default cateogircal feature handling: {loss:.5f}\")\n",
    "xgb_one_hot.extend([loss, rt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can achieve up to 8x speed up if you use <b><a href='https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html'>pandas.Categorical</a></b> data type when using LGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3586085698088037, 4.922000000005937]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_cat_handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBM (cat-handling)    0.358609\n",
       "LGBM (one-hot)         0.349942\n",
       "XGB (one-hot)          0.352410\n",
       "Name: score(log loss), dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign data of lists.  \n",
    "comparing_data = {\n",
    "    'LGBM (cat-handling)': lgbm_cat_handling,\n",
    "    'LGBM (one-hot)': lgbm_one_hot,\n",
    "    'XGB (one-hot)': xgb_one_hot,\n",
    "}  \n",
    "\n",
    "comparing_df = pd.DataFrame(\n",
    "    comparing_data, \n",
    "    index =['score(log loss)', 'time(s)'])\n",
    "\n",
    "comparing_df.iloc[0].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LGBM (cat-handling)</th>\n",
       "      <th>LGBM (one-hot)</th>\n",
       "      <th>XGB (one-hot)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score(log loss)</th>\n",
       "      <td>0.358609</td>\n",
       "      <td>0.349942</td>\n",
       "      <td>0.35241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time(s)</th>\n",
       "      <td>4.922000</td>\n",
       "      <td>6.687000</td>\n",
       "      <td>35.17200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LGBM (cat-handling)  LGBM (one-hot)  XGB (one-hot)\n",
       "score(log loss)             0.358609        0.349942        0.35241\n",
       "time(s)                     4.922000        6.687000       35.17200"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table shows the final scores and runtimes of all three models. As you can see, the version with default categorical handling requires less time compared to the rest two modes while keeping a fair similar score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect8'></a>\n",
    "## <font color='darkblue'>Cross-validation with LightGBM</font>\n",
    "<font size='3ptx'><b>The most common way of doing CV with LGBM is to use Sklearn CV splitters.</b></font>\n",
    "\n",
    "I am not talking about utility functions like <a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html'>cross_validate</a> or <a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html'>cross_val_score</a> but splitters like <b><a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html'>KFold</a></b> or <b><a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html'>StratifiedKFold</a></b> with their split method. Doing CV in this way gives you more control over the whole process.\n",
    "> I have talked many times about the importance of cross-validation. You can read this <a href='https://towardsdatascience.com/6-sklearn-mistakes-that-silently-tell-you-are-a-rookie-84fa55f2b9dd?source=your_stories_page-------------------------------------'>post</a> for more details.\n",
    "\n",
    "<br/>\n",
    "\n",
    "Also, it enables you to use early stopping during cross-validation in a hassle-free manner. Here is what this looks like for the TPS March data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tps_march.drop(\"target\", axis=1), tps_march[[\"target\"]].values.flatten()\n",
    "\n",
    "# Extract categoricals and their indices\n",
    "cat_features = X.select_dtypes(exclude=np.number).columns.to_list()\n",
    "cat_idx = [X.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "# Convert cat_features to pd.Categorical dtype\n",
    "for col in cat_features:\n",
    "    X[col] = pd.Categorical(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Training fold 0============\n",
      "Fold 0 finished with score: 0.34557 in 4.14 seconds.\n",
      "\n",
      "============Training fold 1============\n",
      "Fold 1 finished with score: 0.34932 in 4.63 seconds.\n",
      "\n",
      "============Training fold 2============\n",
      "Fold 2 finished with score: 0.34614 in 5.09 seconds.\n",
      "\n",
      "============Training fold 3============\n",
      "Fold 3 finished with score: 0.35044 in 4.52 seconds.\n",
      "\n",
      "============Training fold 4============\n",
      "Fold 4 finished with score: 0.34595 in 4.26 seconds.\n",
      "\n",
      "============Training fold 5============\n",
      "Fold 5 finished with score: 0.34820 in 5.73 seconds.\n",
      "\n",
      "============Training fold 6============\n",
      "Fold 6 finished with score: 0.35118 in 4.37 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "N_SPLITS = 7\n",
    "N_ESTIMATORS = 10000\n",
    "strat_kf = StratifiedKFold(\n",
    "    n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "scores = np.empty(N_SPLITS)\n",
    "for idx, (train_idx, test_idx) in enumerate(strat_kf.split(X, y)):\n",
    "    print(\"=\" * 12 + f\"Training fold {idx}\" + 12 * \"=\")\n",
    "    start = time.time()\n",
    "\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_val = y[train_idx], y[test_idx]\n",
    "    eval_set = [(X_val, y_val)]\n",
    "\n",
    "    lgbm_clf = lgbm.LGBMClassifier(n_estimators=N_ESTIMATORS)\n",
    "    lgbm_clf.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=eval_set,\n",
    "        categorical_feature=cat_idx,\n",
    "        early_stopping_rounds=200,\n",
    "        eval_metric=\"binary_logloss\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    preds = lgbm_clf.predict_proba(X_val)\n",
    "    loss = log_loss(y_val, preds)\n",
    "    scores[idx] = loss\n",
    "    runtime = time.time() - start\n",
    "    print(f\"Fold {idx} finished with score: {loss:.5f} in {runtime:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a CV splitter — we are choosing <b><a href='https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html'>StratifiedKFold</a></b> because it is a classification problem. Then, loop through each train/test sets using split. In each fold, initialize and train a new LGBM model and optionally report the score and runtime. That's it! That's how most people do CV, including on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Conclusion</font>\n",
    "In this post, we learned pure modeling techniques with <b><a href='https://lightgbm.readthedocs.io/en/latest/index.html'>LightGBM</a></b>. <b>Next up, we will explore how to squeeze every bit of performance out of LGBM models using <a href='https://optuna.org/'>Optuna</a>.</b>\n",
    "\n",
    "Specifically, Part II of this article will include a detailed overview of the most important LGBM hyperparameters and introduce a well-tested hyperparameter tuning workflow. It is already out — read it <a href='https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5?source=your_stories_page-------------------------------------'>here</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
