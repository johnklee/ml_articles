{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Preface</font>\n",
    "([article source](https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de)) <font size='3ptx'><b>An introduction to embedding-based classification of unlabeled text documents</b></font>\n",
    "\n",
    "<b>Text classification is the task of assigning a sentence or document an appropriate category. The categories depend on the selected dataset and can cover arbitrary subjects. Therefore, text classifiers can be used to organize, structure, and categorize any kind of text.</b>\n",
    "\n",
    "Common approaches use supervised learning to classify texts. Especially BERT-based language models achieved very good text classification results in recent years. These conventional text classification approaches usually require a large amount of labeled training data. In practice, however, <font color='darkred'><b>an annotated text dataset for training state-of-the-art classification algorithms is often unavailable. The annotation of data usually involves a lot of manual effort and high expenses</b></font>. Therefore, unsupervised approaches offer the opportunity to run low-cost text classification for unlabeled data sets. <b>In this article, you will learn how to use <a href='https://pypi.org/project/lbl2vec/'>Lbl2Vec</a> to perform unsupervised text classification.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Agenda</font>\n",
    "* <font size='3ptx'><b><a href='#sect1'>How does Lbl2Vec work?</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect2'>Lbl2Vec Tutorial</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1'></a>\n",
    "## <font color='darkblue'>How does Lbl2Vec work?</font>\n",
    "* <font size='3ptx'><b><a href='#sect1_1'>1. Use Manually Defined Keywords for Each Category of Interest</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_2'>2. Create Jointly Embedded Document and Word Vectors</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_3'>3. Find Document Vectors that are Similar to the Keyword Vectors of Each Classification Category</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_4'>4. Clean Outlier Documents for Each Classification Category</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_5'>5. Compute the Centroid of the Outlier Cleaned Document Vectors as Label Vector for Each Classification Category</a></b></font>\n",
    "* <font size='3ptx'><b><a href='#sect1_6'>6. Text Document Classification</a></b></font>\n",
    "<br/>\n",
    "\n",
    "<b><font size='3ptx'><a href='https://pypi.org/project/lbl2vec/'>Lbl2Vec</a> is an algorithm for unsupervised document classification and unsupervised document retrieval.</font> It automatically generates jointly embedded label, document and word vectors and returns documents of categories modeled by manually predefined keywords. The key idea of the algorithm is that many semantically similar keywords can represent a category</b>. \n",
    "\n",
    "In the first step, the algorithm creates a joint embedding of document and word vectors. Once documents and words are embedded in a shared vector space, the goal of the algorithm is to learn label vectors from previously manually defined keywords representing a category. Finally, <b>the algorithm can predict the affiliation of documents to categories based on the similarities of the document vectors with the label vectors</b>. At a high level, the algorithm performs the following steps to classify unlabeled texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_1'></a>\n",
    "### <font color='darkgreen'>1. Use Manually Defined Keywords for Each Category of Interest</font>\n",
    "<b><font size='3ptx'>First, we have to define keywords to describe each classification category of interest.</font></b>\n",
    "\n",
    "This process requires some degree of domain knowledge to define keywords that describe classification categories and are semantically similar to each other within the classification categories. e.g.:\n",
    "![keyword examples](images/1.PNG)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_2'></a>\n",
    "### <font color='darkgreen'>2. Create Jointly Embedded Document and Word Vectors</font>\n",
    "> An embedding vector is a vector that allows us to represent a word or text document in multi-dimensional space. The idea behind embedding vectors is that similar words or text documents will have similar vectors. - <a href='https://towardsdatascience.com/how-to-perform-topic-modeling-with-top2vec-1ae9bb4e89dc'>Amol Mavuduru</a>\n",
    "<br/>\n",
    "\n",
    "Therefore, after creating jointly embedded vectors, documents are located close to other similar documents and close to the most distinguishing words.\n",
    "![doc vectors](images/2.PNG)\n",
    "<br/>\n",
    "\n",
    "Once we have a set of word and document vectors, we can move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_3'></a>\n",
    "### <font color='darkgreen'>3. Find Document Vectors that are Similar to the Keyword Vectors of Each Classification Category</font>\n",
    "<b><font size='3ptx'>Now we can compute cosine similarities between documents and the manually defined keywords of each category. Documents that are similar to category keywords are assigned to a set of candidate documents of the respective category.</font></b>\n",
    "![doc category space](images/3.PNG)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_4'></a>\n",
    "### <font color='darkgreen'>4. Clean Outlier Documents for Each Classification Category</font>\n",
    "<b><font size='3ptx'>The algorithm uses <a href='https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843'>LOF</a> to clean outlier documents from each set of candidate documents that may be related to some of the descriptive keywords but do not properly match the intended classification category.</font></b>\n",
    "![clean outlier](images/4.PNG)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_5'></a>\n",
    "### <font color='darkgreen'>5. Compute the Centroid of the Outlier Cleaned Document Vectors as Label Vector for Each Classification Category</font>\n",
    "<b><font size='3ptx'>To get embedding representations of classification categories, we compute label vectors. Later, the similarity of documents to label vectors will be used to classify text documents. </font></b>\n",
    "\n",
    "Each label vector consists of the <b><a href='https://en.wikipedia.org/wiki/Centroid'>centroid</a></b> of the outlier cleaned document vectors for a category. The algorithm computes document rather than keyword centroids since experiments showed that <b>it is more difficult to classify documents based on similarities to keywords only, even if they share the same vector space.</b>\n",
    "![centroid of doc](images/5.PNG)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect1_6'></a>\n",
    "### <font color='darkgreen'>6. Text Document Classification</font>\n",
    "<b><font size='3ptx'>The algorithm computes label vector (<font color='brown'>document vector similarities</font>) for each label vector and document vector in the dataset. Finally, text documents are classified as category with the highest label vector (<font color='brown'>document vector similarities</font>).</font></b>\n",
    "![doc classifiers](images/6.PNG)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2'></a>\n",
    "## <font color='darkblue'>Lbl2Vec Tutorial</font>\n",
    "* <b><font size='3ptx'><a href='#sect2_1'>Installing Lbl2Vec</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2_2'>Reading the Data</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2_3'>Preprocessing the Data</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2_4'>Training Lbl2Vec</a></font></b>\n",
    "* <b><font size='3ptx'><a href='#sect2_5'>Classification of Text Documents</a></font></b>\n",
    "<br/><br/>\n",
    "\n",
    "<b><font size='3ptx'>In this tutorial we will use <a href='https://github.com/sebischair/Lbl2Vec'>Lbl2Vec</a> to classify text documents from the <a href='http://qwone.com/~jason/20Newsgroups/'>20 Newsgroups dataset</a>. It is a collection of approximately 20,000 text documents, partitioned evenly across 20 different newsgroups categoties.</font></b>\n",
    "\n",
    "In this tutorial, we will focus on a subset of the 20 Newsgroups dataset consisting of the categories “rec.motorcycles”, “rec.sport.baseball”, “rec.sport.hockey” and “sci.crypt”. Furthermore, we will use already predefined keywords for each classification category. The predefined keywords can be downloaded <a href='https://github.com/TimSchopf/MediumBlog/blob/main/data/20newsgroups_keywords.csv'>here</a>. You can also access more <b><a href='https://github.com/sebischair/Lbl2Vec/tree/main/examples'>Lbl2Vec examples on GitHub</a></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_1'></a>\n",
    "### <font color='darkgreen'>Installing Lbl2Vec</font>\n",
    "We can install Lbl2Vec using pip with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lbl2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_2'></a>\n",
    "### <font color='darkgreen'>Reading the Data</font>\n",
    "We store the downloaded “<a href='https://github.com/TimSchopf/MediumBlog/blob/main/data/20newsgroups_keywords.csv'>20newsgroups_keywords.csv</a>” file in the same directory as our Python script. Then we read the CSV with pandas and fetch the 20 Newsgroups dataset from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>Bike Dod Ride Bmw Riding Bikes Motorcycle Ride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>Baseball Game Team Year Players Games Hit Brav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>Hockey Game Team Nhl Play Season Games Espn Pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>Key Clipper Encryption Chip Keys Privacy Escro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index          class_name  \\\n",
       "0            8     rec.motorcycles   \n",
       "1            9  rec.sport.baseball   \n",
       "2           10    rec.sport.hockey   \n",
       "3           11           sci.crypt   \n",
       "\n",
       "                                            keywords  \n",
       "0  Bike Dod Ride Bmw Riding Bikes Motorcycle Ride...  \n",
       "1  Baseball Game Team Year Players Games Hit Brav...  \n",
       "2  Hockey Game Team Nhl Play Season Games Espn Pl...  \n",
       "3  Key Clipper Encryption Chip Keys Privacy Escro...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# load data\n",
    "train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# parse data to pandas DataFrames\n",
    "newsgroup_test = pd.DataFrame({'article':test.data, 'class_index':test.target})\n",
    "newsgroup_train = pd.DataFrame({'article':train.data, 'class_index':train.target})\n",
    "\n",
    "# load labels with keywords\n",
    "labels = pd.read_csv('datas/20newsgroups_keywords.csv',sep=';')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_3'></a>\n",
    "### <font color='darkgreen'>Preprocessing the Data</font>\n",
    "<b><font size='3ptx'>To train a Lbl2Vec model, we need to preprocess the data</font></b>. First, we process the keywords to be used as input for Lbl2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>[bike, dod, ride, bmw, riding, bikes, motorcyc...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>[baseball, game, team, year, players, games, h...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>[hockey, game, team, nhl, play, season, games,...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>[key, clipper, encryption, chip, keys, privacy...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index          class_name  \\\n",
       "0            8     rec.motorcycles   \n",
       "1            9  rec.sport.baseball   \n",
       "2           10    rec.sport.hockey   \n",
       "3           11           sci.crypt   \n",
       "\n",
       "                                            keywords  number_of_keywords  \n",
       "0  [bike, dod, ride, bmw, riding, bikes, motorcyc...                  28  \n",
       "1  [baseball, game, team, year, players, games, h...                  31  \n",
       "2  [hockey, game, team, nhl, play, season, games,...                  35  \n",
       "3  [key, clipper, encryption, chip, keys, privacy...                  40  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(\n",
    "    lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "# lets check our keywords\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the keywords describe each classification category and the number of keywords varies.\n",
    "\n",
    "Furthermore, we also need to preprocess the news articles. Therefore, we word tokenize each document and add <b><a href='https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument'>gensim.models.doc2vec.TaggedDocument</a></b> tags. <a href='https://github.com/sebischair/Lbl2Vec'><b>Lbl2Vec</b></a> needs the tokenized and tagged documents as training input format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>class_index</th>\n",
       "      <th>data_set_type</th>\n",
       "      <th>tagged_docs</th>\n",
       "      <th>doc_key</th>\n",
       "      <th>class_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: cubbie@garnet.berkeley.edu (            ...</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>([from, cubbie, garnet, berkeley, edu, subject...</td>\n",
       "      <td>0</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>[baseball, game, team, year, players, games, h...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: crypt-comments@math.ncsu.edu\\nSubject: C...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>([from, crypt, comments, math, ncsu, edu, subj...</td>\n",
       "      <td>2</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>[key, clipper, encryption, chip, keys, privacy...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: george@minster.york.ac.uk\\nSubject: Non-...</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>([from, george, minster, york, ac, uk, subject...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>[key, clipper, encryption, chip, keys, privacy...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: williac@govonca.gov.on.ca (Chris William...</td>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "      <td>([from, williac, govonca, gov, on, ca, chris, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>[hockey, game, team, nhl, play, season, games,...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: ayari@judikael.loria.fr (Ayari Iskander)...</td>\n",
       "      <td>10</td>\n",
       "      <td>train</td>\n",
       "      <td>([from, ayari, judikael, loria, fr, ayari, isk...</td>\n",
       "      <td>15</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>[hockey, game, team, nhl, play, season, games,...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  class_index  \\\n",
       "0  From: cubbie@garnet.berkeley.edu (            ...            9   \n",
       "1  From: crypt-comments@math.ncsu.edu\\nSubject: C...           11   \n",
       "2  From: george@minster.york.ac.uk\\nSubject: Non-...           11   \n",
       "3  From: williac@govonca.gov.on.ca (Chris William...           10   \n",
       "4  From: ayari@judikael.loria.fr (Ayari Iskander)...           10   \n",
       "\n",
       "  data_set_type                                        tagged_docs doc_key  \\\n",
       "0         train  ([from, cubbie, garnet, berkeley, edu, subject...       0   \n",
       "1         train  ([from, crypt, comments, math, ncsu, edu, subj...       2   \n",
       "2         train  ([from, george, minster, york, ac, uk, subject...      11   \n",
       "3         train  ([from, williac, govonca, gov, on, ca, chris, ...      12   \n",
       "4         train  ([from, ayari, judikael, loria, fr, ayari, isk...      15   \n",
       "\n",
       "           class_name                                           keywords  \\\n",
       "0  rec.sport.baseball  [baseball, game, team, year, players, games, h...   \n",
       "1           sci.crypt  [key, clipper, encryption, chip, keys, privacy...   \n",
       "2           sci.crypt  [key, clipper, encryption, chip, keys, privacy...   \n",
       "3    rec.sport.hockey  [hockey, game, team, nhl, play, season, games,...   \n",
       "4    rec.sport.hockey  [hockey, game, team, nhl, play, season, games,...   \n",
       "\n",
       "   number_of_keywords  \n",
       "0                  31  \n",
       "1                  40  \n",
       "2                  40  \n",
       "3                  35  \n",
       "4                  35  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    '''Tokenize the given doc as string\n",
    "    \n",
    "    - Method strip_tags removes meta tags from the text.\n",
    "    - Method simple preprocess converts a document into a list of lowercase tokens, \n",
    "        ignoring tokens that are too short or too long\n",
    "    - Method simple preprocess also removes numerical values as well as punktuation characters\n",
    "    \n",
    "    Args:\n",
    "        doc: document text string\n",
    "    '''\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "# Add data set type column\n",
    "newsgroup_train['data_set_type'] = 'train'\n",
    "newsgroup_test['data_set_type'] = 'test'\n",
    "\n",
    "# Concat train and test data\n",
    "newsgroup_full_corpus = pd.concat(\n",
    "    [newsgroup_train, newsgroup_test]).reset_index(drop=True)\n",
    "\n",
    "# Reduce dataset to only articles that belong to classes where we defined our keywords\n",
    "newsgroup_full_corpus = newsgroup_full_corpus[\n",
    "    newsgroup_full_corpus['class_index'].isin(list(labels['class_index']))]\n",
    "\n",
    "# Tokenize and tag documents for Lbl2Vec training\n",
    "newsgroup_full_corpus['tagged_docs'] = newsgroup_full_corpus.apply(\n",
    "    lambda row: TaggedDocument(tokenize(row['article']), [str(row.name)]), axis=1)\n",
    "\n",
    "# Add doc_key column\n",
    "newsgroup_full_corpus['doc_key'] = newsgroup_full_corpus.index.astype(str)\n",
    "\n",
    "# Add class_name column\n",
    "newsgroup_full_corpus = newsgroup_full_corpus.merge(\n",
    "    labels, left_on='class_index', right_on='class_index', how='left')\n",
    "\n",
    "newsgroup_full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the article texts and their classification categories in the dataframe. The “`tagged_docs`” column consists of the preprocessed documents that are needed as Lbl2Vec input. The classification categories in the “`class_name`” column are used for evaluation only but not for Lbl2Vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_4'></a>\n",
    "### <font color='darkgreen'>Training Lbl2Vec</font>\n",
    "After preparing the data, we now can train a Lbl2Vec model on the train dataset. We initialize the model with the following parameters:\n",
    "* <b><font color='violet'>keywords_list</font></b> : iterable list of lists with descriptive keywords for each category.\n",
    "* <b><font color='violet'>tagged_documents</font></b> : iterable list of <b><a href='https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument'>gensim.models.doc2vec.TaggedDocument</a></b> elements. Each element consists of one document.\n",
    "* <b><font color='violet'>label_names</font></b> : iterable list of custom names for each label. Label names and keywords of the same topic must have the same index.\n",
    "* <b><font color='violet'>similarity_threshold</font></b> : only documents with a higher similarity to the respective description keywords than this treshold are used to calculate the label embedding.\n",
    "* <b><font color='violet'>min_num_docs</font></b> : minimum number of documents that are used to calculate the label embedding.\n",
    "* <b><font color='violet'>epochs</font></b> : number of iterations over the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:18:42,942 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2021-12-26 14:18:48,679 - Lbl2Vec - INFO - Train label embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from lbl2vec import Lbl2Vec\n",
    "\n",
    "# init model with parameters\n",
    "Lbl2Vec_model = Lbl2Vec(\n",
    "    keywords_list=list(labels.keywords),\n",
    "    tagged_documents=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type'] == 'train'],\n",
    "    label_names=list(labels.class_name),\n",
    "    similarity_threshold=0.43,\n",
    "    min_num_docs=100, epochs=10)\n",
    "\n",
    "# train model\n",
    "Lbl2Vec_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect2_5'></a>\n",
    "### <font color='darkgreen'>Classification of Text Documents</font>\n",
    "After the model is trained, we can predict the categories of documents used to train the Lbl2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:20:42,900 - Lbl2Vec - INFO - Get document embeddings from model\n",
      "2021-12-26 14:20:42,904 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8744769874476988\n",
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# predict similarity scores\n",
    "model_docs_lbl_similarities = Lbl2Vec_model.predict_model_docs()\n",
    "\n",
    "# merge DataFrames to compare the predicted and true category labels\n",
    "evaluation_train = model_docs_lbl_similarities.merge(\n",
    "    newsgroup_full_corpus[newsgroup_full_corpus['data_set_type'] == 'train'],\n",
    "    left_on='doc_key', right_on='doc_key')\n",
    "y_true_train = evaluation_train['class_name']\n",
    "y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_train, y_pred_train, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Our model can predict the correct document categories with a respectable F1 Score of 0.88. This is achieved without even seeing the document labels during training</b>.\n",
    "\n",
    "Moreover, we can also predict the classification categories of documents that were not used to train the Lbl2Vec model and are therefore completely unknown to it. To this end, we predict the categories of documents from the previously unused test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:22:45,984 - Lbl2Vec - INFO - Calculate document embeddings\n",
      "2021-12-26 14:22:47,606 - Lbl2Vec - INFO - Calculate document<->label similarities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8572327044025158\n"
     ]
    }
   ],
   "source": [
    "# predict similarity scores of new test documents (they were not used during Lbl2Vec training)\n",
    "new_docs_lbl_similarities = Lbl2Vec_model.predict_new_docs(\n",
    "    tagged_docs=newsgroup_full_corpus['tagged_docs'][newsgroup_full_corpus['data_set_type']=='test'])\n",
    "\n",
    "# merge DataFrames to compare the predicted and true topic labels\n",
    "evaluation_test = new_docs_lbl_similarities.merge(\n",
    "    newsgroup_full_corpus[newsgroup_full_corpus['data_set_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "y_true_test = evaluation_test['class_name']\n",
    "y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "print('F1 score:',f1_score(y_true_test, y_pred_test, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trained Lbl2Vec model can even predict the classification categories of new documents with a F1 Score of 0.86. As mentioned before, this is achieved with a completely unsupervised approach where no label information was used during training.\n",
    "\n",
    "For more details about the features available in Lbl2Vec, please check out the <a href='https://github.com/sebischair/Lbl2Vec'>Lbl2Vec GitHub repository</a>. I hope you found this tutorial to be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sect3'></a>\n",
    "## <font color='darkblue'>Summary</font>\n",
    "<b><font size='3ptx'><a href='https://pypi.org/project/lbl2vec/'>Lbl2Vec</a> is a recently developed approach that can be used for unsupervised text document classification.</font></b> Unlike other state-of-the-art approaches it needs no label information during training and therefore offers the opportunity to run low-cost text classification for unlabeled datasets. The open-source Lbl2Vec library is also very easy to use and allows developers to train models in just a few lines of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
